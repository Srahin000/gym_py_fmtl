{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded: 12500 samples, 48 features\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import flwr as fl\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "results_dir = 'experiment_results'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('datasets/local_cache/dataset_12500_samples_65_features.csv')\n",
    "\n",
    "# Drop features with high label leakage\n",
    "cols_to_drop = [\n",
    " 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt',\n",
    " 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_cnt', 'ece_flag_cnt',\n",
    " 'fwd_header_length', 'bwd_header_length',\n",
    " 'active_mean', 'active_std', 'active_max', 'active_min',\n",
    " 'idle_mean', 'idle_std', 'idle_max', 'idle_min',\n",
    " 'subflow_fwd_bytes'\n",
    "]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "print(f\" Data loaded: {len(df)} samples, {len(df.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "print(\" Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Training: local_epochs=1, lr=0.001\n",
      "  Clients: 600 total (3 clusters × 200 clients)\n",
      "  Participation: 100.0%\n",
      "\n",
      "  Data Distribution (Two-Level):\n",
      "    Level 1 (Clusters): equal split\n",
      "    Level 2 (Clients): dirichlet split (α=0.4)\n",
      "    Cluster α: 0.4 (used when cluster_split='dirichlet')\n"
     ]
    }
   ],
   "source": [
    "CFG = {\n",
    "    # Training parameters\n",
    "    'local_epochs': 1,\n",
    "    'lr': 1e-3,\n",
    "    'loss_weights': {'traffic': 1, 'duration': 1, 'bandwidth': 1},\n",
    "    'test_size': 0.2,\n",
    "    \n",
    "    # Client configuration\n",
    "    'n_clients_flat': 600,\n",
    "    'n_clusters': 3,\n",
    "    'clients_per_cluster': 200,\n",
    "    'client_frac': 1.0,  # 100% client participation\n",
    "    \n",
    "    # Hierarchical FL\n",
    "    'global_aggregator_cluster': 1,  # Cluster 1 performs global aggregation\n",
    "    \n",
    "    # Data distribution (TWO-LEVEL SPLIT)\n",
    "    'cluster_split': 'equal',      # How to split data among clusters ('equal' or 'dirichlet')\n",
    "    'client_split': 'dirichlet',   # How to split data among clients within clusters (always 'dirichlet')\n",
    "    'alpha_client': 0.4,           # Dirichlet α for client-level distribution\n",
    "    'alpha_cluster': 0.4,          # Dirichlet α for cluster-level distribution (when cluster_split='dirichlet')\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Training: local_epochs={CFG['local_epochs']}, lr={CFG['lr']}\")\n",
    "print(f\"  Clients: {CFG['n_clients_flat']} total ({CFG['n_clusters']} clusters × {CFG['clients_per_cluster']} clients)\")\n",
    "print(f\"  Participation: {CFG['client_frac']*100}%\")\n",
    "print(f\"\\n  Data Distribution (Two-Level):\")\n",
    "print(f\"    Level 1 (Clusters): {CFG['cluster_split']} split\")\n",
    "print(f\"    Level 2 (Clients): {CFG['client_split']} split (α={CFG['alpha_client']})\")\n",
    "print(f\"    Cluster α: {CFG['alpha_cluster']} (used when cluster_split='dirichlet')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection for Each Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Traffic features: 39\n",
      " Duration features: 39\n",
      " Bandwidth features: 39\n"
     ]
    }
   ],
   "source": [
    "# Define features to exclude for each task (prevent label leakage)\n",
    "exclude_traffic = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port', # identity → leakage\n",
    " 'protocol', # not useful for QUIC-only\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "exclude_duration = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port',\n",
    " 'protocol',\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "exclude_bandwidth = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port',\n",
    " 'protocol',\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "Xcols_traffic = [col for col in df.columns if col not in exclude_traffic]\n",
    "Xcols_duration = [col for col in df.columns if col not in exclude_duration]\n",
    "Xcols_bandwidth = [col for col in df.columns if col not in exclude_bandwidth]\n",
    "\n",
    "print(f\" Traffic features: {len(Xcols_traffic)}\")\n",
    "print(f\" Duration features: {len(Xcols_duration)}\")\n",
    "print(f\" Bandwidth features: {len(Xcols_bandwidth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train samples: 10000\n",
      " Test samples: 2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "n = len(df)\n",
    "indices = np.arange(n)\n",
    "train_idx, test_idx = train_test_split(\n",
    " indices, \n",
    " test_size=CFG['test_size'], \n",
    " random_state=seed, \n",
    " shuffle=True\n",
    ")\n",
    "\n",
    "train_df = df.iloc[train_idx].copy()\n",
    "test_df = df.iloc[test_idx].copy()\n",
    "\n",
    "print(f\" Train samples: {len(train_df)}\")\n",
    "print(f\" Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Winsorization (Outlier Handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "all_features = list(set(Xcols_traffic + Xcols_duration + Xcols_bandwidth))\n",
    "\n",
    "# Calculate winsorization bounds from training data\n",
    "winsor_bounds = {}\n",
    "for col in all_features:\n",
    "    if col in train_df.columns:\n",
    "     lower = train_df[col].quantile(0.01)\n",
    "     upper = train_df[col].quantile(0.99)\n",
    "     winsor_bounds[col] = (lower, upper)\n",
    "\n",
    "# Apply winsorization\n",
    "for col, (lower, upper) in winsor_bounds.items():\n",
    " lower_limit = (train_df[col] < lower).mean()\n",
    " upper_limit = (train_df[col] > upper).mean()\n",
    " \n",
    " for df_temp in [train_df, test_df]:\n",
    "     df_temp[col] = winsorize(df_temp[col], limits=(lower_limit, upper_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Variable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantile thresholds computed for duration and bandwidth\n"
     ]
    }
   ],
   "source": [
    "# Create quantile-based labels for duration and bandwidth (5 classes each)\n",
    "y_dur_raw_train = train_df['flow_duration'].values\n",
    "y_bw_raw_train = train_df['bandwidth_bps'].values\n",
    "\n",
    "# Log-transform\n",
    "bw_log = np.log1p(y_bw_raw_train)\n",
    "dur_log = np.log1p(y_dur_raw_train)\n",
    "\n",
    "# Compute 5-bin quantiles (20%, 40%, 60%, 80%)\n",
    "bw_quantiles = np.quantile(bw_log, [0.20, 0.40, 0.60, 0.80])\n",
    "dur_quantiles = np.quantile(dur_log, [0.20, 0.40, 0.60, 0.80])\n",
    "\n",
    "def create_quantile_labels(raw_values, quantiles):\n",
    " \"\"\"Create 5-class labels (0-4) using quantile thresholds\"\"\"\n",
    " v = np.log1p(raw_values)\n",
    " labels = np.digitize(v, quantiles, right=False) # returns 0..4\n",
    " return labels\n",
    "\n",
    "print(\" Quantile thresholds computed for duration and bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Label Encoding and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Labels created and features standardized\n",
      " Traffic classes: 5\n",
      " Duration classes: 5\n",
      " Bandwidth classes: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Create labels for all tasks\n",
    "y_dur_train = create_quantile_labels(train_df['flow_duration'].values, dur_quantiles)\n",
    "y_dur_test = create_quantile_labels(test_df['flow_duration'].values, dur_quantiles)\n",
    "\n",
    "y_bw_train = create_quantile_labels(train_df['bandwidth_bps'].values, bw_quantiles)\n",
    "y_bw_test = create_quantile_labels(test_df['bandwidth_bps'].values, bw_quantiles)\n",
    "\n",
    "# Traffic classification (label encoding)\n",
    "le_traf = LabelEncoder()\n",
    "y_traf_train = le_traf.fit_transform(train_df['label'])\n",
    "y_traf_test = le_traf.transform(test_df['label'])\n",
    "\n",
    "# Standardize features\n",
    "feature_scaler = StandardScaler()\n",
    "train_df[all_features] = feature_scaler.fit_transform(train_df[all_features])\n",
    "test_df[all_features] = feature_scaler.transform(test_df[all_features])\n",
    "\n",
    "print(\" Labels created and features standardized\")\n",
    "print(f\" Traffic classes: {len(np.unique(y_traf_train))}\")\n",
    "print(f\" Duration classes: {len(np.unique(y_dur_train))}\")\n",
    "print(f\" Bandwidth classes: {len(np.unique(y_bw_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature matrices extracted\n",
      " Traffic: (10000, 39)\n",
      " Duration: (10000, 39)\n",
      " Bandwidth: (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "# Extract feature matrices for each task\n",
    "X_traffic_train = train_df[Xcols_traffic].values\n",
    "X_duration_train = train_df[Xcols_duration].values\n",
    "X_bandwidth_train = train_df[Xcols_bandwidth].values\n",
    "\n",
    "X_traffic_test = test_df[Xcols_traffic].values\n",
    "X_duration_test = test_df[Xcols_duration].values\n",
    "X_bandwidth_test = test_df[Xcols_bandwidth].values\n",
    "\n",
    "print(\" Feature matrices extracted\")\n",
    "print(f\" Traffic: {X_traffic_train.shape}\")\n",
    "print(f\" Duration: {X_duration_train.shape}\")\n",
    "print(f\" Bandwidth: {X_bandwidth_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Padding (Uniform Dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All features padded to dimension: 39\n",
      "  Traffic: (10000, 39)\n",
      "  Duration: (10000, 39)\n",
      "  Bandwidth: (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "# Pad all feature matrices to the same dimension\n",
    "max_dim = max(X_traffic_train.shape[1], X_duration_train.shape[1], X_bandwidth_train.shape[1])\n",
    "\n",
    "def pad_features(X, target_size):\n",
    "    \"\"\"Pad features with zeros to reach target size\"\"\"\n",
    "    if X.shape[1] < target_size:\n",
    "        padding = np.zeros((X.shape[0], target_size - X.shape[1]))\n",
    "        return np.concatenate([X, padding], axis=1)\n",
    "    return X\n",
    "\n",
    "X_traffic_train = pad_features(X_traffic_train, max_dim)\n",
    "X_duration_train = pad_features(X_duration_train, max_dim)\n",
    "X_bandwidth_train = pad_features(X_bandwidth_train, max_dim)\n",
    "\n",
    "X_traffic_test = pad_features(X_traffic_test, max_dim)\n",
    "X_duration_test = pad_features(X_duration_test, max_dim)\n",
    "X_bandwidth_test = pad_features(X_bandwidth_test, max_dim)\n",
    "\n",
    "print(f\"✓ All features padded to dimension: {max_dim}\")\n",
    "print(f\"  Traffic: {X_traffic_train.shape}\")\n",
    "print(f\"  Duration: {X_duration_train.shape}\")\n",
    "print(f\"  Bandwidth: {X_bandwidth_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mutual Information Analysis (Feature Leakage Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing mutual information between features and labels...\n",
      "\n",
      "Duration - Found 30 features with MI > 0.2:\n",
      "  • bwd_packets_per_s: 0.9275\n",
      "  • flow_packets_per_s: 0.9051\n",
      "  • fwd_packets_per_s: 0.8397\n",
      "  • flow_iat_mean: 0.7458\n",
      "  • flow_iat_max: 0.7057\n",
      "  • flow_iat_std: 0.6616\n",
      "  • fwd_iat_total: 0.5753\n",
      "  • fwd_iat_std: 0.5706\n",
      "  • fwd_iat_max: 0.5513\n",
      "  • fwd_iat_mean: 0.5258\n",
      "  ... and 20 more\n",
      "\n",
      "Bandwidth - Found 30 features with MI > 0.2:\n",
      "  • bwd_packets_per_s: 1.2198\n",
      "  • flow_packets_per_s: 1.1862\n",
      "  • fwd_packets_per_s: 1.0154\n",
      "  • flow_iat_mean: 0.7372\n",
      "  • flow_iat_max: 0.6862\n",
      "  • flow_iat_std: 0.6665\n",
      "  • fwd_iat_std: 0.5595\n",
      "  • fwd_iat_mean: 0.5562\n",
      "  • fwd_iat_max: 0.5272\n",
      "  • fwd_iat_total: 0.5123\n",
      "  ... and 20 more\n",
      "\n",
      "Traffic - Found 35 features with MI > 0.2:\n",
      "  • bwd_pkt_len_min: 1.1980\n",
      "  • bwd_seg_size_min: 1.1928\n",
      "  • fwd_seg_size_min: 1.0199\n",
      "  • fwd_pkt_len_min: 1.0194\n",
      "  • bwd_pkt_len_max: 0.9857\n",
      "  • fwd_pkt_len_max: 0.7743\n",
      "  • flow_rate_entropy: 0.6934\n",
      "  • bwd_pkt_len_mean: 0.6692\n",
      "  • total_len_bwd_packets: 0.6347\n",
      "  • avg_bwd_segment_size: 0.5876\n",
      "  ... and 25 more\n",
      "\n",
      "✓ Mutual information analysis complete\n",
      "  Note: High MI features may indicate correlation with labels\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import sys\n",
    "\n",
    "def find_high_mi_features(X_cols, y_train, train_df, task_name, seed, threshold=0.2):\n",
    "    \"\"\"Find features with high mutual information (potential label leakage)\"\"\"\n",
    "    X_train = train_df[X_cols].values\n",
    "    \n",
    "    try:\n",
    "        mi_scores = mutual_info_classif(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            discrete_features=False,\n",
    "            random_state=seed\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating MI for {task_name}: {e}\", file=sys.stderr)\n",
    "        return []\n",
    "    \n",
    "    mi_results = dict(zip(X_cols, mi_scores))\n",
    "    \n",
    "    problematic = []\n",
    "    for feat, mi in mi_results.items():\n",
    "        if mi > threshold:\n",
    "            problematic.append((feat, mi))\n",
    "    \n",
    "    if problematic:\n",
    "        print(f\"\\n{task_name} - Found {len(problematic)} features with MI > {threshold}:\")\n",
    "        problematic.sort(key=lambda x: x[1], reverse=True)\n",
    "        for feat, mi in problematic[:10]:  # Show top 10\n",
    "            print(f\"  • {feat}: {mi:.4f}\")\n",
    "        if len(problematic) > 10:\n",
    "            print(f\"  ... and {len(problematic) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n{task_name} - No features found with MI > {threshold}\")\n",
    "    \n",
    "    return problematic\n",
    "\n",
    "print(\"Analyzing mutual information between features and labels...\")\n",
    "\n",
    "problematic_dur = find_high_mi_features(\n",
    "    Xcols_duration, y_dur_train, train_df, 'Duration', seed\n",
    ")\n",
    "\n",
    "problematic_bw = find_high_mi_features(\n",
    "    Xcols_bandwidth, y_bw_train, train_df, 'Bandwidth', seed\n",
    ")\n",
    "\n",
    "problematic_tf = find_high_mi_features(\n",
    "    Xcols_traffic, y_traf_train, train_df, 'Traffic', seed\n",
    ")\n",
    "\n",
    "all_diagnostics = {\n",
    "    'duration': problematic_dur,\n",
    "    'bandwidth': problematic_bw,\n",
    "    'traffic': problematic_tf\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Mutual information analysis complete\")\n",
    "print(\"  Note: High MI features may indicate correlation with labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Client Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 600 clients\n",
      "  Cluster split: equal\n",
      "  Client split: dirichlet\n",
      "  Sample sizes: min=50, max=92, avg=50.2\n",
      "\n",
      "  Cluster distribution:\n",
      "    Cluster 0: 200 clients, 10016 samples\n",
      "    Cluster 1: 200 clients, 10067 samples\n",
      "    Cluster 2: 200 clients, 10061 samples\n",
      "\n",
      "  Sample client label distributions:\n",
      "    Client 0 (Cluster 0): {0: 16, 1: 9, 2: 9, 3: 7, 4: 9}\n",
      "    Client 1 (Cluster 0): {0: 16, 1: 11, 2: 8, 3: 9, 4: 6}\n",
      "    Client 2 (Cluster 0): {0: 8, 1: 12, 2: 5, 3: 13, 4: 12}\n"
     ]
    }
   ],
   "source": [
    "def build_client_partitions(cluster_split='equal', client_split='dirichlet', verbose=True):\n",
    "    \"\"\"\n",
    "    Build client partitions with TWO-LEVEL data distribution:\n",
    "    - Level 1: Distribute data among CLUSTERS (equal or dirichlet)\n",
    "    - Level 2: Distribute each cluster's data among CLIENTS (dirichlet)\n",
    "    \n",
    "    Args:\n",
    "        cluster_split: 'equal' or 'dirichlet' - how to split data among clusters\n",
    "        client_split: 'dirichlet' - how to split data among clients within clusters\n",
    "        verbose: Print statistics\n",
    "    \n",
    "    Returns:\n",
    "        client_indices_flat: List of client data indices\n",
    "        client_index_to_cluster: Dict mapping client idx to cluster id\n",
    "    \"\"\"\n",
    "    n_clients = CFG['n_clients_flat']\n",
    "    n_clusters = CFG['n_clusters']\n",
    "    clients_per_cluster = CFG['clients_per_cluster']\n",
    "    alpha_client = CFG['alpha_client']\n",
    "    alpha_cluster = CFG['alpha_cluster']\n",
    "    min_size = 50\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    train_indices = np.arange(len(y_traf_train))\n",
    "    labels = np.unique(y_traf_train)\n",
    "    \n",
    "    # Cluster_level_split\n",
    "    \n",
    "    if cluster_split == 'equal':\n",
    "        # Equal split: each cluster gets 1/n_clusters of data\n",
    "        samples_per_cluster = len(train_indices) // n_clusters\n",
    "        cluster_indices = []\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            start_idx = cluster_id * samples_per_cluster\n",
    "            end_idx = start_idx + samples_per_cluster if cluster_id < n_clusters - 1 else len(train_indices)\n",
    "            cluster_indices.append(train_indices[start_idx:end_idx])\n",
    "    \n",
    "    elif cluster_split == 'dirichlet':\n",
    "        # Dirichlet split: non-IID distribution among clusters\n",
    "        cluster_bins = [[] for _ in range(n_clusters)]\n",
    "        label_indices = {}\n",
    "        \n",
    "        for lbl in labels:\n",
    "            label_indices[lbl] = train_indices[y_traf_train == lbl]\n",
    "        \n",
    "        for lbl in labels:\n",
    "            idxs = label_indices[lbl]\n",
    "            rng.shuffle(idxs)\n",
    "            proportions = rng.dirichlet([alpha_cluster] * n_clusters)\n",
    "            cuts = (np.cumsum(proportions) * len(idxs)).astype(int)\n",
    "            parts = np.split(idxs, cuts[:-1])\n",
    "            \n",
    "            for cluster_id, part in enumerate(parts):\n",
    "                cluster_bins[cluster_id].extend(part.tolist())\n",
    "        \n",
    "        cluster_indices = [np.array(sorted(set(cluster_bins[i])), dtype=int) for i in range(n_clusters)]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cluster_split: {cluster_split}\")\n",
    "\n",
    "    # Client_level_split\n",
    "    \n",
    "    client_indices_flat = []\n",
    "    client_index_to_cluster = {}\n",
    "    \n",
    "    for cluster_id, cluster_data_indices in enumerate(cluster_indices):\n",
    "        # Get labels for this cluster's data\n",
    "        cluster_labels = y_traf_train[cluster_data_indices]\n",
    "        unique_cluster_labels = np.unique(cluster_labels)\n",
    "        \n",
    "        # Build client bins for this cluster using Dirichlet\n",
    "        client_bins = [[] for _ in range(clients_per_cluster)]\n",
    "        \n",
    "        for lbl in unique_cluster_labels:\n",
    "            # Get indices within cluster that have this label\n",
    "            lbl_mask = cluster_labels == lbl\n",
    "            lbl_indices = cluster_data_indices[lbl_mask]\n",
    "            \n",
    "            if len(lbl_indices) > 0:\n",
    "                rng.shuffle(lbl_indices)\n",
    "                proportions = rng.dirichlet([alpha_client] * clients_per_cluster)\n",
    "                cuts = (np.cumsum(proportions) * len(lbl_indices)).astype(int)\n",
    "                parts = np.split(lbl_indices, cuts[:-1])\n",
    "                \n",
    "                for local_client_id, part in enumerate(parts):\n",
    "                    client_bins[local_client_id].extend(part.tolist())\n",
    "        \n",
    "        # Create clients for this cluster\n",
    "        for local_client_id in range(clients_per_cluster):\n",
    "            client_data = np.array(sorted(set(client_bins[local_client_id])), dtype=int)\n",
    "            \n",
    "            # Ensure minimum size\n",
    "            if len(client_data) < min_size:\n",
    "                need = min_size - len(client_data)\n",
    "                # Sample from cluster's data\n",
    "                available = list(set(cluster_data_indices) - set(client_data))\n",
    "                if len(available) >= need:\n",
    "                    extra = rng.choice(available, size=need, replace=False)\n",
    "                else:\n",
    "                    extra = rng.choice(cluster_data_indices, size=need, replace=True)\n",
    "                client_data = np.concatenate([client_data, extra])\n",
    "                client_data = np.unique(client_data).astype(int)\n",
    "            \n",
    "            global_client_id = cluster_id * clients_per_cluster + local_client_id\n",
    "            client_indices_flat.append(client_data.astype(int))\n",
    "            client_index_to_cluster[global_client_id] = cluster_id\n",
    "    \n",
    "    # Statistics\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✓ Created {len(client_indices_flat)} clients\")\n",
    "        print(f\"  Cluster split: {cluster_split}\")\n",
    "        print(f\"  Client split: {client_split}\")\n",
    "        print(f\"  Sample sizes: min={min([len(c) for c in client_indices_flat])}, \"\n",
    "              f\"max={max([len(c) for c in client_indices_flat])}, \"\n",
    "              f\"avg={np.mean([len(c) for c in client_indices_flat]):.1f}\")\n",
    "        \n",
    "        print(\"\\n  Cluster distribution:\")\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_clients = [i for i in range(n_clients) if client_index_to_cluster[i] == cluster_id]\n",
    "            cluster_samples = sum(len(client_indices_flat[i]) for i in cluster_clients)\n",
    "            print(f\"    Cluster {cluster_id}: {len(cluster_clients)} clients, {cluster_samples} samples\")\n",
    "        \n",
    "        print(\"\\n  Sample client label distributions:\")\n",
    "        for i in range(min(3, len(client_indices_flat))):\n",
    "            indices = client_indices_flat[i]\n",
    "            labels_count = {}\n",
    "            for lbl in labels:\n",
    "                count = np.sum(y_traf_train[indices] == lbl)\n",
    "                if count > 0:\n",
    "                    labels_count[int(lbl)] = int(count)\n",
    "            print(f\"    Client {i} (Cluster {client_index_to_cluster[i]}): {labels_count}\")\n",
    "    \n",
    "    return client_indices_flat, client_index_to_cluster\n",
    "# Build clients with specified split type\n",
    "client_indices_flat, client_index_to_cluster = build_client_partitions(\n",
    "    cluster_split=CFG['cluster_split'],  # ✓ NEW: equal or dirichlet for clusters\n",
    "    client_split=CFG['client_split'],    # ✓ NEW: dirichlet for clients\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client data structures created\n",
      " Total clients: 600\n",
      " Clusters: 3\n",
      "\n",
      "Client 0 data shapes:\n",
      " Traffic:   X=(50, 39),   y=(50,)\n",
      " Duration:  X=(50, 39),  y=(50,)\n",
      " Bandwidth: X=(50, 39), y=(50,)\n"
     ]
    }
   ],
   "source": [
    "class ClientData:\n",
    "    \"\"\"Container for client data and metadata\"\"\"\n",
    "    def __init__(self, data_dict, cluster_id):\n",
    "        self.ds = data_dict\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "\n",
    "# Create client objects\n",
    "clients = []\n",
    "\n",
    "for i, indices in enumerate(client_indices_flat):\n",
    "\n",
    "    # ensure numpy integer index array\n",
    "    indices = np.asarray(indices, dtype=np.int32)\n",
    "\n",
    "    # Slice features\n",
    "    X_traffic_client   = X_traffic_train[indices].astype(np.float32).copy()\n",
    "    X_duration_client  = X_duration_train[indices].astype(np.float32).copy()\n",
    "    X_bandwidth_client = X_bandwidth_train[indices].astype(np.float32).copy()\n",
    "\n",
    "    # Slice labels\n",
    "    y_traffic_client   = y_traf_train[indices].astype(np.int32).copy()\n",
    "    y_duration_client  = y_dur_train[indices].astype(np.int32).copy()\n",
    "    y_bandwidth_client = y_bw_train[indices].astype(np.int32).copy()\n",
    "\n",
    "    # Package\n",
    "    client_data_dict = {\n",
    "        'traffic':   (X_traffic_client,   y_traffic_client),\n",
    "        'duration':  (X_duration_client,  y_duration_client),\n",
    "        'bandwidth': (X_bandwidth_client, y_bandwidth_client)\n",
    "    }\n",
    "\n",
    "    # Cluster ID lookup\n",
    "    cluster_id = client_index_to_cluster[i]\n",
    "\n",
    "    # Create client object\n",
    "    clients.append(ClientData(client_data_dict, cluster_id))\n",
    "\n",
    "\n",
    "# Diagnostics\n",
    "print(\"\\nClient data structures created\")\n",
    "print(f\" Total clients: {len(clients)}\")\n",
    "print(f\" Clusters: {CFG['n_clusters']}\")\n",
    "\n",
    "print(\"\\nClient 0 data shapes:\")\n",
    "print(f\" Traffic:   X={clients[0].ds['traffic'][0].shape},   y={clients[0].ds['traffic'][1].shape}\")\n",
    "print(f\" Duration:  X={clients[0].ds['duration'][0].shape},  y={clients[0].ds['duration'][1].shape}\")\n",
    "print(f\" Bandwidth: X={clients[0].ds['bandwidth'][0].shape}, y={clients[0].ds['bandwidth'][1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test data prepared\n",
      " Traffic: (2500, 39)\n",
      " Duration: (2500, 39)\n",
      " Bandwidth: (2500, 39)\n"
     ]
    }
   ],
   "source": [
    "test_data = {\n",
    " 'traffic': (X_traffic_test.astype(np.float32), y_traf_test.astype(int)),\n",
    " 'duration': (X_duration_test.astype(np.float32), y_dur_test.astype(int)),\n",
    " 'bandwidth': (X_bandwidth_test.astype(np.float32), y_bw_test.astype(int))\n",
    "}\n",
    "\n",
    "print(\"\\n Test data prepared\")\n",
    "print(f\" Traffic: {test_data['traffic'][0].shape}\")\n",
    "print(f\" Duration: {test_data['duration'][0].shape}\")\n",
    "print(f\" Bandwidth: {test_data['bandwidth'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed test data saved to: trained_models/preprocessed_test_data.pkl\n",
      "  Samples: 2500\n",
      "  Input dim: 39\n",
      "  Classes: {'traffic': 5, 'duration': 5, 'bandwidth': 5}\n",
      "  Traffic labels: ['discord', 'facebook-web', 'google-services', 'instagram', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed test data for PyBullet simulation inference\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create test data dict with all preprocessed arrays\n",
    "preprocessed_test_data = {\n",
    "    'X_traffic': X_traffic_test.astype(np.float32),\n",
    "    'X_duration': X_duration_test.astype(np.float32),\n",
    "    'X_bandwidth': X_bandwidth_test.astype(np.float32),\n",
    "    'y_traffic': y_traf_test.astype(np.int32),\n",
    "    'y_duration': y_dur_test.astype(np.int32),\n",
    "    'y_bandwidth': y_bw_test.astype(np.int32),\n",
    "    'n_samples': len(y_traf_test),\n",
    "    'input_dim': X_traffic_test.shape[1],\n",
    "    'n_classes': {\n",
    "        'traffic': len(np.unique(y_traf_test)),\n",
    "        'duration': len(np.unique(y_dur_test)),\n",
    "        'bandwidth': len(np.unique(y_bw_test))\n",
    "    },\n",
    "    'traffic_label_encoder_classes': le_traf.classes_.tolist()  # Save label mapping\n",
    "}\n",
    "\n",
    "# Save to trained_models directory\n",
    "save_path = 'trained_models/preprocessed_test_data.pkl'\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(preprocessed_test_data, f)\n",
    "\n",
    "print(f\"✓ Preprocessed test data saved to: {save_path}\")\n",
    "print(f\"  Samples: {preprocessed_test_data['n_samples']}\")\n",
    "print(f\"  Input dim: {preprocessed_test_data['input_dim']}\")\n",
    "print(f\"  Classes: {preprocessed_test_data['n_classes']}\")\n",
    "print(f\"  Traffic labels: {preprocessed_test_data['traffic_label_encoder_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Data Distribution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DISTRIBUTION SUMMARY\n",
      "\n",
      "Unique Classes:\n",
      " Traffic classes: 5\n",
      " Duration classes: 5\n",
      " Bandwidth classes: 5\n",
      "\n",
      "Duration (Train) Distribution:\n",
      " Very Short (0): 2000\n",
      " Short (1): 2000\n",
      " Medium (2): 2000\n",
      " Long (3): 2000\n",
      " Very Long (4): 2000\n",
      "\n",
      "Bandwidth (Train) Distribution:\n",
      " Very Low (0): 2000\n",
      " Low (1): 2000\n",
      " Medium (2): 2000\n",
      " High (3): 2000\n",
      " Very High (4): 2000\n",
      "\n",
      "Traffic (Train) Distribution:\n",
      " Class 0: 1999\n",
      " Class 1: 1991\n",
      " Class 2: 2006\n",
      " Class 3: 1984\n",
      " Class 4: 2020\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(labels, name, mapping=None):\n",
    "    \"\"\"Print class distribution with optional name mapping.\"\"\"\n",
    "    print(f\"\\n{name} Distribution:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    for u, c in zip(unique, counts):\n",
    "        if mapping:\n",
    "            label_name = mapping.get(u, f\"Class {u}\")\n",
    "            print(f\" {label_name} ({u}): {c}\")\n",
    "        else:\n",
    "            print(f\" Class {u}: {c}\")\n",
    "\n",
    "\n",
    "duration_map = {\n",
    "    0: \"Very Short\",\n",
    "    1: \"Short\",\n",
    "    2: \"Medium\",\n",
    "    3: \"Long\",\n",
    "    4: \"Very Long\"\n",
    "}\n",
    "\n",
    "bandwidth_map = {\n",
    "    0: \"Very Low\",\n",
    "    1: \"Low\",\n",
    "    2: \"Medium\",\n",
    "    3: \"High\",\n",
    "    4: \"Very High\"\n",
    "}\n",
    "\n",
    "print(\"DATA DISTRIBUTION SUMMARY\")\n",
    "\n",
    "print(\"\\nUnique Classes:\")\n",
    "print(f\" Traffic classes: {len(np.unique(y_traf_train))}\")\n",
    "print(f\" Duration classes: {len(np.unique(y_dur_train))}\")\n",
    "print(f\" Bandwidth classes: {len(np.unique(y_bw_train))}\")\n",
    "\n",
    "print_distribution(y_dur_train, \"Duration (Train)\", duration_map)\n",
    "print_distribution(y_bw_train, \"Bandwidth (Train)\", bandwidth_map)\n",
    "print_distribution(y_traf_train, \"Traffic (Train)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Architecture (FedMTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedMTLModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Federated Multi-Task Learning Model\n",
    "\n",
    "    Architecture:\n",
    "    - Shared layers: 2 dense layers (256 → 128) with dropout\n",
    "    - Task-specific layers: 1 dense layer per task\n",
    "    - Task heads: 3 classification heads (traffic, duration, bandwidth)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dims, n_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tasks = ['traffic', 'duration', 'bandwidth']\n",
    "\n",
    "        # Shared layers (learned across all tasks)\n",
    "        self.shared_dense1 = keras.layers.Dense(256, activation='relu', name='shared_dense1')\n",
    "        self.shared_drop1  = keras.layers.Dropout(dropout)\n",
    "        self.shared_dense2 = keras.layers.Dense(128, activation='relu', name='shared_dense2')\n",
    "        self.shared_drop2  = keras.layers.Dropout(dropout)\n",
    "\n",
    "        # Task-specific layers\n",
    "        self.task_dense = {\n",
    "            'traffic':   keras.layers.Dense(64, activation='relu', name='task_traffic_dense'),\n",
    "            'duration':  keras.layers.Dense(32, activation='relu', name='task_duration_dense'),\n",
    "            'bandwidth': keras.layers.Dense(64, activation='relu', name='task_bandwidth_dense'),\n",
    "        }\n",
    "\n",
    "        # Task heads (output logits)\n",
    "        self.task_heads = {\n",
    "            'traffic':   keras.layers.Dense(n_classes['traffic'],   name='traffic_output'),\n",
    "            'duration':  keras.layers.Dense(n_classes['duration'],  name='duration_output'),\n",
    "            'bandwidth': keras.layers.Dense(n_classes['bandwidth'], name='bandwidth_output'),\n",
    "        }\n",
    "\n",
    "    def call(self, x, task, training=False):\n",
    "        \"\"\"Forward pass for a specific task\"\"\"\n",
    "        # Shared layers\n",
    "        x = self.shared_dense1(x)\n",
    "        x = self.shared_drop1(x, training=training)\n",
    "        x = self.shared_dense2(x)\n",
    "        x = self.shared_drop2(x, training=training)\n",
    "\n",
    "        # Task-specific branch\n",
    "        x = self.task_dense[task](x)\n",
    "\n",
    "        # Final classification head\n",
    "        return self.task_heads[task](x)\n",
    "\n",
    "    def build_all(self, input_dim):\n",
    "        \"\"\"Build all task heads with a dummy forward pass\"\"\"\n",
    "        tf.random.set_seed(seed)\n",
    "        dummy = tf.random.normal((1, input_dim))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            _ = self.call(dummy, task=task, training=False)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "# Register in Keras custom objects\n",
    "tf.keras.utils.get_custom_objects().update({'FedMTLModel': FedMTLModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Flower Client Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MTLFlowerClient implementation complete\n",
      "Features:\n",
      " - Multi-task local training\n",
      " - Weighted loss aggregation over active tasks\n",
      " - Task-specific evaluation\n",
      " - Parameter change tracking\n"
     ]
    }
   ],
   "source": [
    "class MTLFlowerClient(fl.client.NumPyClient):\n",
    "    \"\"\"\n",
    "    Flower client for Multi-Task Learning\n",
    "\n",
    "    Handles:\n",
    "    - Local training on multiple tasks\n",
    "    - Parameter synchronization with server\n",
    "    - Task-specific evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, client_data, cfg, cluster_id):\n",
    "        self.model = model\n",
    "        self.client_data = client_data \n",
    "        self.cfg = cfg\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.cfg['lr'])\n",
    "\n",
    "        # Loss functions (all classification)\n",
    "        self.loss_fns = {\n",
    "            'traffic': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'duration': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'bandwidth': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        }\n",
    "\n",
    "        # Task-specific loss weights\n",
    "        self.loss_weights = cfg['loss_weights']\n",
    "\n",
    "    # -------- Utility --------\n",
    "    def _ensure_model_built(self):\n",
    "        \"\"\"Make sure the Keras model is built before use.\"\"\"\n",
    "        if self.model.built:\n",
    "            return\n",
    "\n",
    "        # Try to build from local data\n",
    "        x = None\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task in self.client_data:\n",
    "                X_task, _ = self.client_data[task]\n",
    "                if len(X_task) > 0:\n",
    "                    x = tf.convert_to_tensor(X_task[:1], dtype=tf.float32)\n",
    "                    break\n",
    "\n",
    "        # If this client has absolutely no data, fall back to cfg['max_dim'] if available\n",
    "        if x is None:\n",
    "            if 'max_dim' in self.cfg:\n",
    "                input_dim = self.cfg['max_dim']\n",
    "            else:\n",
    "                # Try to infer from any task across this client\n",
    "                all_dims = [\n",
    "                    v[0].shape[1] for v in self.client_data.values()\n",
    "                    if v[0].shape[0] > 0\n",
    "                ]\n",
    "                input_dim = all_dims[0] if all_dims else 1\n",
    "            x = tf.random.normal((1, input_dim))\n",
    "\n",
    "        for t in ['traffic', 'duration', 'bandwidth']:\n",
    "            _ = self.model(x, task=t, training=False)\n",
    "        self.model.built = True\n",
    "\n",
    "    # -------- Flower API --------\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current model weights\"\"\"\n",
    "        self._ensure_model_built()\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Local training on client data\"\"\"\n",
    "        self._ensure_model_built()\n",
    "\n",
    "        # Set global weights\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "        # Local training loop\n",
    "        for epoch in range(self.cfg['local_epochs']):\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = 0.0\n",
    "                used_tasks = []\n",
    "\n",
    "                # Loop through all available tasks\n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    if task not in self.client_data:\n",
    "                        continue\n",
    "\n",
    "                    X_task, y_task = self.client_data[task]\n",
    "                    if len(X_task) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Convert data to tensors\n",
    "                    X_task_tf = tf.convert_to_tensor(X_task, dtype=tf.float32)\n",
    "                    y_task_tf = tf.convert_to_tensor(y_task, dtype=tf.int32)\n",
    "\n",
    "                    # Forward pass\n",
    "                    logits = self.model(X_task_tf, task=task, training=True)\n",
    "\n",
    "                    # Compute loss and apply task weight\n",
    "                    task_loss = self.loss_fns[task](y_task_tf, logits)\n",
    "                    weighted_loss = task_loss * self.loss_weights[task]\n",
    "\n",
    "                    total_loss += weighted_loss\n",
    "                    used_tasks.append(task)\n",
    "\n",
    "                if len(used_tasks) > 0:\n",
    "                    # Normalize by sum of weights of tasks that are actually present\n",
    "                    norm = sum(self.loss_weights[t] for t in used_tasks)\n",
    "                    total_loss = total_loss / norm\n",
    "\n",
    "                    # Apply gradients\n",
    "                    grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "                    if grads is not None and any(g is not None for g in grads):\n",
    "                        self.optimizer.apply_gradients(\n",
    "                            zip(grads, self.model.trainable_weights)\n",
    "                        )\n",
    "                else:\n",
    "                    total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "        # Return updated weights and metadata\n",
    "        num_examples = sum(len(data[1]) for data in self.client_data.values())\n",
    "        avg_loss = float(total_loss.numpy()) if isinstance(total_loss, tf.Tensor) else float(total_loss)\n",
    "\n",
    "        return self.model.get_weights(), num_examples, {\n",
    "            \"loss\": avg_loss,\n",
    "            \"num_tasks\": len(self.client_data),\n",
    "            \"cluster_id\": self.cluster_id,\n",
    "            \"num_examples\": num_examples\n",
    "        }\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate model on client data\"\"\"\n",
    "        self._ensure_model_built()\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        task_accuracies = {}\n",
    "        used_tasks = []\n",
    "\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task not in self.client_data:\n",
    "                continue\n",
    "\n",
    "            X_task, y_task = self.client_data[task]\n",
    "            if len(X_task) == 0:\n",
    "                continue\n",
    "\n",
    "            X_task_tf = tf.convert_to_tensor(X_task, dtype=tf.float32)\n",
    "            y_task_tf = tf.convert_to_tensor(y_task, dtype=tf.int32)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(X_task_tf, task=task, training=False)\n",
    "\n",
    "            # Compute loss and apply weights\n",
    "            task_loss = self.loss_fns[task](y_task_tf, logits)\n",
    "            weighted_loss = task_loss * self.loss_weights[task]\n",
    "            total_loss += weighted_loss\n",
    "\n",
    "            # Classification evaluation\n",
    "            predictions = tf.argmax(logits, axis=1)\n",
    "            accuracy = tf.reduce_mean(\n",
    "                tf.cast(\n",
    "                    tf.equal(predictions, tf.cast(y_task_tf, tf.int64)),\n",
    "                    tf.float32,\n",
    "                )\n",
    "            )\n",
    "            task_accuracies[f\"{task}_accuracy\"] = float(accuracy)\n",
    "            task_accuracies[f\"{task}_loss\"] = float(task_loss)\n",
    "\n",
    "            total_samples += len(y_task)\n",
    "            used_tasks.append(task)\n",
    "\n",
    "        if len(used_tasks) > 0:\n",
    "            norm = sum(self.loss_weights[t] for t in used_tasks)\n",
    "            avg_loss = float(total_loss / norm)\n",
    "            overall_accuracy = np.mean([\n",
    "                task_accuracies[f\"{task}_accuracy\"]\n",
    "                for task in used_tasks\n",
    "            ])\n",
    "        else:\n",
    "            avg_loss = 0.0\n",
    "            overall_accuracy = 0.0\n",
    "\n",
    "        task_accuracies[\"accuracy\"] = overall_accuracy\n",
    "\n",
    "        return float(avg_loss), int(total_samples), task_accuracies\n",
    "\n",
    "\n",
    "print(\"\\nMTLFlowerClient implementation complete\")\n",
    "print(\"Features:\")\n",
    "print(\" - Multi-task local training\")\n",
    "print(\" - Weighted loss aggregation over active tasks\")\n",
    "print(\" - Task-specific evaluation\")\n",
    "print(\" - Parameter change tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CONFIGURATION SUMMARY\n",
      "\n",
      "Input dimensions:\n",
      " traffic: 39\n",
      " duration: 39\n",
      " bandwidth: 39\n",
      "\n",
      "Number of classes:\n",
      " traffic: 5\n",
      " duration: 5\n",
      " bandwidth: 5\n",
      "\n",
      "Training configuration:\n",
      " Local epochs: 1\n",
      " Learning rate: 0.001\n",
      " Loss weights: {'traffic': 1, 'duration': 1, 'bandwidth': 1}\n",
      " Client participation: 100.0%\n",
      "\n",
      "Federation structure:\n",
      " Total clients: 600\n",
      " Number of clusters: 3\n",
      " Clients per cluster: 200\n",
      " Global aggregator: Cluster 1\n",
      " Split type: equal\n"
     ]
    }
   ],
   "source": [
    "in_dims = {\n",
    " 'traffic': max_dim,\n",
    " 'duration': max_dim,\n",
    " 'bandwidth': max_dim \n",
    "}\n",
    "\n",
    "n_classes = {\n",
    " 'traffic': len(np.unique(y_traf_train)),\n",
    " 'duration': len(np.unique(y_dur_train)),\n",
    " 'bandwidth': len(np.unique(y_bw_train))\n",
    "}\n",
    "\n",
    "print(\"MODEL CONFIGURATION SUMMARY\")\n",
    "print(f\"\\nInput dimensions:\")\n",
    "for task, dim in in_dims.items():\n",
    " print(f\" {task}: {dim}\")\n",
    "print(f\"\\nNumber of classes:\")\n",
    "for task, n in n_classes.items():\n",
    " print(f\" {task}: {n}\")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\" Local epochs: {CFG['local_epochs']}\")\n",
    "print(f\" Learning rate: {CFG['lr']}\")\n",
    "print(f\" Loss weights: {CFG['loss_weights']}\")\n",
    "print(f\" Client participation: {CFG['client_frac']*100}%\")\n",
    "\n",
    "print(f\"\\nFederation structure:\")\n",
    "print(f\" Total clients: {CFG['n_clients_flat']}\")\n",
    "print(f\" Number of clusters: {CFG['n_clusters']}\")\n",
    "print(f\" Clients per cluster: {CFG['clients_per_cluster']}\")\n",
    "print(f\" Global aggregator: Cluster {CFG['global_aggregator_cluster']}\")\n",
    "print(f\" Split type: {CFG['cluster_split']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ray shutdown complete\n"
     ]
    }
   ],
   "source": [
    "# Shutdown Ray to clear all workers and memory\n",
    "if ray.is_initialized():\n",
    " ray.shutdown()\n",
    " print(\" Ray shutdown complete\")\n",
    "else:\n",
    " print(\" Ray not running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH COMPROMISATION EXPERIMENTS\n",
    "\n",
    "## Test Plan:\n",
    "1. **Baseline (100 rounds)**: Normal training to convergence\n",
    "2. **CH Compromise After Convergence**: Train 100 rounds → Compromise CH → Continue 25 rounds (total 125)\n",
    "3. **Transient CH Compromise**: Compromise CH during training (125 rounds total)\n",
    "\n",
    "All tests use the same hierarchical architecture with 3 clusters and CH1 as global aggregator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27c. Training-Only Strategies (Save Models, No Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KPI-ENABLED TRAINING STRATEGIES\n",
    "# ============================================================================\n",
    "# These strategies integrate comprehensive KPI tracking during training\n",
    "\n",
    "class TrainingOnlyStrategyWithKPIs(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Training strategy that saves models AND tracks comprehensive KPIs\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir='trained_models', kpi_tracker=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        self.saved_models = []\n",
    "        self.kpi_tracker = kpi_tracker\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Start experiment timer\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.start_experiment()\n",
    "        \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        # Start round timing\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.start_round()\n",
    "            \n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Measure computational load during aggregation\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.measure_computational_load()\n",
    "        \n",
    "        # Standard FedAvg aggregation\n",
    "        aggregated_params, metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        # Save model params after every round\n",
    "        model_weights = fl.common.parameters_to_ndarrays(aggregated_params)\n",
    "        save_path = os.path.join(self.save_dir, f'model_round_{server_round}.pkl')\n",
    "        \n",
    "        # Prepare save data with KPIs\n",
    "        save_data = {\n",
    "            'round': server_round,\n",
    "            'weights': model_weights,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        # Add KPI snapshot if tracker exists\n",
    "        if self.kpi_tracker and self.kpi_tracker.kpis['round_durations']:\n",
    "            save_data['kpi_snapshot'] = {\n",
    "                'round_duration': self.kpi_tracker.kpis['round_durations'][-1] if self.kpi_tracker.kpis['round_durations'] else 0,\n",
    "                'cumulative_time': self.kpi_tracker.kpis['cumulative_time'][-1] if self.kpi_tracker.kpis['cumulative_time'] else 0,\n",
    "                'cpu_percent': self.kpi_tracker.kpis['computational_load']['cpu_percent'][-1] if self.kpi_tracker.kpis['computational_load']['cpu_percent'] else 0,\n",
    "                'memory_mb': self.kpi_tracker.kpis['computational_load']['memory_rss_mb'][-1] if self.kpi_tracker.kpis['computational_load']['memory_rss_mb'] else 0,\n",
    "            }\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        self.saved_models.append(save_path)\n",
    "        \n",
    "        # Print training progress\n",
    "        if metrics:\n",
    "            avg_loss = metrics.get('loss', 0.0)\n",
    "            print(f\"[Round {server_round:3d}] Training Loss: {avg_loss:.4f} | Model saved\")\n",
    "        elif server_round % 20 == 0:\n",
    "            print(f\"[Round {server_round}] Model saved: {save_path}\")\n",
    "        \n",
    "        return aggregated_params, metrics\n",
    "    \n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        \"\"\"Aggregate evaluation results and track KPIs\"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Per-task accuracy aggregation\n",
    "        task_totals = {\n",
    "            'traffic_accuracy': 0.0,\n",
    "            'duration_accuracy': 0.0,\n",
    "            'bandwidth_accuracy': 0.0\n",
    "        }\n",
    "        \n",
    "        for _, eval_res in results:\n",
    "            num_examples = eval_res.num_examples\n",
    "            total_loss += eval_res.loss * num_examples\n",
    "            if 'accuracy' in eval_res.metrics:\n",
    "                total_accuracy += eval_res.metrics['accuracy'] * num_examples\n",
    "            \n",
    "            # Aggregate per-task accuracies\n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                task_key = f'{task}_accuracy'\n",
    "                if task_key in eval_res.metrics:\n",
    "                    task_totals[task_key] += eval_res.metrics[task_key] * num_examples\n",
    "            \n",
    "            total_samples += num_examples\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "        avg_accuracy = total_accuracy / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Calculate per-task averages\n",
    "        aggregated_metrics = {'accuracy': avg_accuracy, 'loss': avg_loss}\n",
    "        for task_key in task_totals:\n",
    "            aggregated_metrics[task_key] = task_totals[task_key] / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Track KPIs\n",
    "        if self.kpi_tracker:\n",
    "            accuracies = {\n",
    "                'global': avg_accuracy,\n",
    "                'traffic': aggregated_metrics['traffic_accuracy'],\n",
    "                'duration': aggregated_metrics['duration_accuracy'],\n",
    "                'bandwidth': aggregated_metrics['bandwidth_accuracy'],\n",
    "            }\n",
    "            self.kpi_tracker.end_round(server_round, accuracies, phase='normal')\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        if server_round % 10 == 0 or server_round == 1:\n",
    "            print(f\"[Round {server_round:3d}] Eval - Traffic: {aggregated_metrics['traffic_accuracy']:.4f}, \"\n",
    "                  f\"Duration: {aggregated_metrics['duration_accuracy']:.4f}, \"\n",
    "                  f\"Bandwidth: {aggregated_metrics['bandwidth_accuracy']:.4f}\")\n",
    "        \n",
    "        return avg_loss, aggregated_metrics\n",
    "\n",
    "\n",
    "class HierarchicalTrainingOnlyStrategyWithKPIs(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Hierarchical training-only strategy with comprehensive KPI tracking\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir='trained_models', kpi_tracker=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        self.saved_models = []\n",
    "        self.global_aggregator_cluster = CFG['global_aggregator_cluster']\n",
    "        self.kpi_tracker = kpi_tracker\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Start experiment timer\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.start_experiment()\n",
    "        \n",
    "    def _ndarrays_weighted_average(self, param_list):\n",
    "        if not param_list:\n",
    "            return None\n",
    "        total_weight = float(sum(w for _, w in param_list))\n",
    "        if total_weight <= 0:\n",
    "            total_weight = 1.0\n",
    "        summed = [np.zeros_like(arr, dtype=arr.dtype) for arr in param_list[0][0]]\n",
    "        for arrays, w in param_list:\n",
    "            for i, arr in enumerate(arrays):\n",
    "                summed[i] = summed[i] + (arr * (w / total_weight))\n",
    "        return summed\n",
    "    \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        # Start round timing\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.start_round()\n",
    "            \n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Measure computational load during aggregation\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.measure_computational_load()\n",
    "        \n",
    "        # Tier 1: Aggregate within clusters\n",
    "        cluster_to_pairs = {}\n",
    "        cluster_client_counts = defaultdict(int)\n",
    "        \n",
    "        for client_proxy, fit_res in results:\n",
    "            nds = fl.common.parameters_to_ndarrays(fit_res.parameters)\n",
    "            weight = getattr(fit_res, 'num_examples', None)\n",
    "            if weight is None:\n",
    "                weight = int(fit_res.metrics.get('num_examples', 1)) if hasattr(fit_res, 'metrics') else 1\n",
    "            cluster_id = int(fit_res.metrics.get('cluster_id', 0)) if hasattr(fit_res, 'metrics') else 0\n",
    "            cluster_to_pairs.setdefault(cluster_id, []).append((nds, weight))\n",
    "            cluster_client_counts[cluster_id] += 1\n",
    "        \n",
    "        cluster_params = {}\n",
    "        cluster_weights = {}\n",
    "        \n",
    "        for cid, pairs in cluster_to_pairs.items():\n",
    "            if pairs:\n",
    "                cluster_params[cid] = self._ndarrays_weighted_average(pairs)\n",
    "                cluster_weights[cid] = float(sum(w for _, w in pairs))\n",
    "        \n",
    "        # Tier 2: Global aggregation at CH1\n",
    "        global_agg_cluster = self.global_aggregator_cluster\n",
    "        \n",
    "        if global_agg_cluster in cluster_params:\n",
    "            global_pairs = []\n",
    "            for cid in [0, 2]:\n",
    "                if cid in cluster_params:\n",
    "                    global_pairs.append((cluster_params[cid], cluster_weights[cid]))\n",
    "            \n",
    "            if global_agg_cluster in cluster_params:\n",
    "                global_pairs.append((cluster_params[global_agg_cluster], cluster_weights[global_agg_cluster]))\n",
    "            \n",
    "            if global_pairs:\n",
    "                global_params = self._ndarrays_weighted_average(global_pairs)\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(global_params)\n",
    "            else:\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(cluster_params[global_agg_cluster])\n",
    "        else:\n",
    "            all_pairs = [(cluster_params[cid], cluster_weights[cid]) for cid in cluster_params.keys()]\n",
    "            if all_pairs:\n",
    "                global_params = self._ndarrays_weighted_average(all_pairs)\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(global_params)\n",
    "            else:\n",
    "                return None, {}\n",
    "        \n",
    "        # Save model params after every round\n",
    "        model_weights = fl.common.parameters_to_ndarrays(aggregated_params)\n",
    "        save_path = os.path.join(self.save_dir, f'model_round_{server_round}.pkl')\n",
    "        \n",
    "        # Prepare comprehensive save data\n",
    "        save_data = {\n",
    "            'round': server_round,\n",
    "            'weights': model_weights,\n",
    "            'cluster_params': {cid: params for cid, params in cluster_params.items()},\n",
    "            'cluster_client_counts': dict(cluster_client_counts),\n",
    "            'metrics': {\n",
    "                'participating_clusters': len(cluster_params),\n",
    "                'cluster_weights': cluster_weights\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add KPI snapshot if tracker exists\n",
    "        if self.kpi_tracker and self.kpi_tracker.kpis['round_durations']:\n",
    "            save_data['kpi_snapshot'] = {\n",
    "                'round_duration': self.kpi_tracker.kpis['round_durations'][-1] if self.kpi_tracker.kpis['round_durations'] else 0,\n",
    "                'cumulative_time': self.kpi_tracker.kpis['cumulative_time'][-1] if self.kpi_tracker.kpis['cumulative_time'] else 0,\n",
    "                'cpu_percent': self.kpi_tracker.kpis['computational_load']['cpu_percent'][-1] if self.kpi_tracker.kpis['computational_load']['cpu_percent'] else 0,\n",
    "                'memory_mb': self.kpi_tracker.kpis['computational_load']['memory_rss_mb'][-1] if self.kpi_tracker.kpis['computational_load']['memory_rss_mb'] else 0,\n",
    "                'participating_clients_per_cluster': dict(cluster_client_counts)\n",
    "            }\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "        \n",
    "        self.saved_models.append(save_path)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"[Round {server_round:3d}] Clusters: {len(cluster_params)} | Model saved\")\n",
    "        \n",
    "        return aggregated_params, {}\n",
    "    \n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        \"\"\"Aggregate evaluation results and track KPIs\"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Per-task accuracy aggregation\n",
    "        task_totals = {\n",
    "            'traffic_accuracy': 0.0,\n",
    "            'duration_accuracy': 0.0,\n",
    "            'bandwidth_accuracy': 0.0\n",
    "        }\n",
    "        \n",
    "        # Per-cluster tracking\n",
    "        cluster_metrics = defaultdict(lambda: {'samples': 0, 'accuracy': 0.0})\n",
    "        \n",
    "        for _, eval_res in results:\n",
    "            num_examples = eval_res.num_examples\n",
    "            total_loss += eval_res.loss * num_examples\n",
    "            if 'accuracy' in eval_res.metrics:\n",
    "                total_accuracy += eval_res.metrics['accuracy'] * num_examples\n",
    "                \n",
    "                # Track per-cluster if available\n",
    "                if 'cluster_id' in eval_res.metrics:\n",
    "                    cid = eval_res.metrics['cluster_id']\n",
    "                    cluster_metrics[cid]['samples'] += num_examples\n",
    "                    cluster_metrics[cid]['accuracy'] += eval_res.metrics['accuracy'] * num_examples\n",
    "            \n",
    "            # Aggregate per-task accuracies\n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                task_key = f'{task}_accuracy'\n",
    "                if task_key in eval_res.metrics:\n",
    "                    task_totals[task_key] += eval_res.metrics[task_key] * num_examples\n",
    "            \n",
    "            total_samples += num_examples\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "        avg_accuracy = total_accuracy / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Calculate per-task averages\n",
    "        aggregated_metrics = {'accuracy': avg_accuracy, 'loss': avg_loss}\n",
    "        for task_key in task_totals:\n",
    "            aggregated_metrics[task_key] = task_totals[task_key] / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Calculate per-cluster accuracies\n",
    "        for cid, data in cluster_metrics.items():\n",
    "            if data['samples'] > 0:\n",
    "                aggregated_metrics[f'cluster_{cid}_accuracy'] = data['accuracy'] / data['samples']\n",
    "        \n",
    "        # Track KPIs\n",
    "        if self.kpi_tracker:\n",
    "            accuracies = {\n",
    "                'global': avg_accuracy,\n",
    "                'traffic': aggregated_metrics['traffic_accuracy'],\n",
    "                'duration': aggregated_metrics['duration_accuracy'],\n",
    "                'bandwidth': aggregated_metrics['bandwidth_accuracy'],\n",
    "            }\n",
    "            \n",
    "            # Add per-cluster accuracies\n",
    "            for cid in range(self.kpi_tracker.n_clusters):\n",
    "                key = f'cluster_{cid}_accuracy'\n",
    "                if key in aggregated_metrics:\n",
    "                    accuracies[f'cluster_{cid}'] = aggregated_metrics[key]\n",
    "            \n",
    "            self.kpi_tracker.end_round(server_round, accuracies, phase='normal')\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        if server_round % 10 == 0 or server_round == 1:\n",
    "            print(f\"[Round {server_round:3d}] Eval - Traffic: {aggregated_metrics['traffic_accuracy']:.4f}, \"\n",
    "                  f\"Duration: {aggregated_metrics['duration_accuracy']:.4f}, \"\n",
    "                  f\"Bandwidth: {aggregated_metrics['bandwidth_accuracy']:.4f}\")\n",
    "        \n",
    "        return avg_loss, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "import psutil\n",
    "from scipy.stats import pearsonr\n",
    "from collections import defaultdict\n",
    "\n",
    "class ComprehensiveKPITracker:\n",
    "    \"\"\"\n",
    "    Comprehensive KPI Tracker for Scalable FMTL Experiments\n",
    "    \n",
    "    Tracks all metrics from TIER 1 and TIER 2 categories:\n",
    "    - Learning Performance\n",
    "    - Model Architecture & Resources\n",
    "    - Communication Efficiency\n",
    "    - Attack Impact & Recovery\n",
    "    - Cluster Health & Participation\n",
    "    - CH Selection & Load\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg, model, n_clusters=3, clients_per_cluster=200):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.n_clusters = n_clusters\n",
    "        self.clients_per_cluster = clients_per_cluster\n",
    "        self.total_clients = n_clusters * clients_per_cluster\n",
    "        \n",
    "        # Initialize KPI storage\n",
    "        self.kpis = {\n",
    "            # ========== TIER 1: Learning Performance ==========\n",
    "            'global_accuracy': [],           # Per-round global accuracy\n",
    "            'per_cluster_accuracy': defaultdict(list),  # {cluster_id: [accuracies]}\n",
    "            'per_task_accuracy': defaultdict(list),     # {task: [accuracies]}\n",
    "            'convergence_round': None,       # Round when converged\n",
    "            'convergence_time_seconds': None,  # 🆕 Wall-clock time to convergence\n",
    "            'round_durations': [],           # 🆕 Duration of each round in seconds\n",
    "            'cumulative_time': [],           # 🆕 Cumulative wall-clock time\n",
    "            \n",
    "            # ========== TIER 1: Model Architecture & Resources ==========\n",
    "            'model_parameter_size_bytes': 0,\n",
    "            'model_parameter_size_kb': 0.0,\n",
    "            'model_architecture_overhead_bytes': 0,  # 🆕 sys.getsizeof + pickle\n",
    "            'inference_latency_ms': 0.0,     # 🆕 Average inference time\n",
    "            'inference_latency_std_ms': 0.0, # 🆕 Std dev of inference time\n",
    "            'computational_load': {          # 🆕 Per-UAV computational metrics\n",
    "                'cpu_percent': [],\n",
    "                'memory_rss_mb': [],\n",
    "            },\n",
    "            \n",
    "            # ========== TIER 1: Communication Efficiency ==========\n",
    "            'communication_cost_per_round': [],  # Bytes per round\n",
    "            'total_communication_bytes': 0,\n",
    "            'communication_breakdown': {     # 🆕 By phase\n",
    "                'normal': 0,\n",
    "                'attack': 0,\n",
    "                'recovery': 0,\n",
    "            },\n",
    "            'extra_cost_due_to_attack': 0,   # 🆕 Attack + recovery - baseline equivalent\n",
    "            'per_cluster_communication': defaultdict(list),  # 🆕 {cluster_id: [bytes]}\n",
    "            'bytes_per_federation_round': 0.0,  # 🆕 Average bytes per round\n",
    "            \n",
    "            # ========== TIER 2: Attack Impact & Recovery ==========\n",
    "            'detection_time_rounds': 1,      # Rounds between attack and detection\n",
    "            'recovery_time_breakdown': {     # Phase durations in rounds\n",
    "                'detection': 1,\n",
    "                'isolation': 7,\n",
    "                'reintegration': 7,\n",
    "            },\n",
    "            'recovery_time_seconds': 0.0,    # 🆕 Real seconds for recovery\n",
    "            'accuracy_degradation_during_attack': {  # 🆕 Pre-attack - attack round\n",
    "                'global': 0.0,\n",
    "                'traffic': 0.0,\n",
    "                'duration': 0.0,\n",
    "                'bandwidth': 0.0,\n",
    "            },\n",
    "            'time_to_restore_accuracy_rounds': 0,  # 🆕 First round >= 99% pre-attack\n",
    "            'model_divergence_during_isolation': [],  # 🆕 L2 norm vs global weights\n",
    "            'task_specific_attack_impact': {  # 🆕 Per-task drop percentages\n",
    "                'traffic': 0.0,\n",
    "                'duration': 0.0,\n",
    "                'bandwidth': 0.0,\n",
    "            },\n",
    "            'per_task_recovery_curves': defaultdict(list),  # 🆕 {task: [accuracies]}\n",
    "            \n",
    "            # ========== TIER 2: Cluster Health & Participation ==========\n",
    "            'participation_rate_per_cluster': defaultdict(list),  # {cluster_id: [rates]}\n",
    "            'cluster_0_isolation_impact': {  # Impact on C1, C2 during C0 isolation\n",
    "                'c1_accuracy_during_isolation': [],\n",
    "                'c2_accuracy_during_isolation': [],\n",
    "            },\n",
    "            'gradual_reintegration_effect': {  # 🆕 Accuracy at 30%, 70%, 100%\n",
    "                '30_percent': {'round': None, 'accuracy': 0.0},\n",
    "                '70_percent': {'round': None, 'accuracy': 0.0},\n",
    "                '100_percent': {'round': None, 'accuracy': 0.0},\n",
    "            },\n",
    "            'participation_accuracy_correlation': 0.0,  # 🆕 Pearson correlation\n",
    "            \n",
    "            # ========== TIER 2: CH Selection & Load ==========\n",
    "            'ch_load_members_per_ch': {},    # {ch_id: num_members}\n",
    "            'ch_duty_cycle': {},             # {ch_id: duty_cycle_estimate}\n",
    "            'ch_selection_frequency': 0,     # 🆕 Number of re-elections in 125 rounds\n",
    "            'ch_reelection_time_seconds': [],  # 🆕 Time for each re-election\n",
    "            'new_ch0_characteristics': {     # 🆕 Properties of newly elected CH0\n",
    "                'energy_residual': 0.0,\n",
    "                'rssi_avg': 0.0,\n",
    "            },\n",
    "            'context_aware_selection_score': 0.0,  # 🆕 alpha*E + beta*RSSI\n",
    "        }\n",
    "        \n",
    "        # Timing state\n",
    "        self._round_start_time = None\n",
    "        self._experiment_start_time = None\n",
    "        self._attack_start_round = None\n",
    "        self._recovery_start_round = None\n",
    "        self._recovery_end_round = None\n",
    "        \n",
    "        # Compute initial model metrics\n",
    "        self._compute_model_metrics()\n",
    "        \n",
    "    def _compute_model_metrics(self):\n",
    "        \"\"\"Compute model parameter size and architecture overhead\"\"\"\n",
    "        # Ensure model is built\n",
    "        if not self.model.built:\n",
    "            self.model.build_all(self.cfg.get('max_dim', 39))\n",
    "        \n",
    "        # Model parameter size\n",
    "        weights = self.model.get_weights()\n",
    "        param_size = sum(w.nbytes for w in weights)\n",
    "        self.kpis['model_parameter_size_bytes'] = param_size\n",
    "        self.kpis['model_parameter_size_kb'] = param_size / 1024\n",
    "        \n",
    "        # Architecture overhead (sys.getsizeof + pickle serialization)\n",
    "        try:\n",
    "            model_sys_size = sys.getsizeof(self.model)\n",
    "            pickle_size = len(pickle.dumps(weights))\n",
    "            self.kpis['model_architecture_overhead_bytes'] = model_sys_size + pickle_size\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not compute architecture overhead: {e}\")\n",
    "            self.kpis['model_architecture_overhead_bytes'] = param_size\n",
    "    \n",
    "    def start_experiment(self):\n",
    "        \"\"\"Mark the start of the experiment\"\"\"\n",
    "        self._experiment_start_time = time.time()\n",
    "        \n",
    "    def start_round(self):\n",
    "        \"\"\"Mark the start of a training round\"\"\"\n",
    "        self._round_start_time = time.time()\n",
    "        \n",
    "    def end_round(self, round_num, accuracies, phase='normal', participating_clients=None):\n",
    "        \"\"\"\n",
    "        Record metrics at the end of a training round\n",
    "        \n",
    "        Args:\n",
    "            round_num: Current round number\n",
    "            accuracies: Dict with 'global', 'traffic', 'duration', 'bandwidth', \n",
    "                       and optionally per-cluster accuracies\n",
    "            phase: 'normal', 'attack', or 'recovery'\n",
    "            participating_clients: Dict {cluster_id: num_participating}\n",
    "        \"\"\"\n",
    "        # Round duration\n",
    "        if self._round_start_time is not None:\n",
    "            duration = time.time() - self._round_start_time\n",
    "            self.kpis['round_durations'].append(duration)\n",
    "            \n",
    "            # Cumulative time\n",
    "            if self.kpis['cumulative_time']:\n",
    "                self.kpis['cumulative_time'].append(\n",
    "                    self.kpis['cumulative_time'][-1] + duration\n",
    "                )\n",
    "            else:\n",
    "                self.kpis['cumulative_time'].append(duration)\n",
    "        \n",
    "        # Accuracies\n",
    "        self.kpis['global_accuracy'].append(accuracies.get('global', 0.0))\n",
    "        \n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task in accuracies:\n",
    "                self.kpis['per_task_accuracy'][task].append(accuracies[task])\n",
    "        \n",
    "        # Per-cluster accuracies\n",
    "        for cluster_id in range(self.n_clusters):\n",
    "            key = f'cluster_{cluster_id}'\n",
    "            if key in accuracies:\n",
    "                self.kpis['per_cluster_accuracy'][cluster_id].append(accuracies[key])\n",
    "        \n",
    "        # Communication cost for this round\n",
    "        model_size = self.kpis['model_parameter_size_bytes']\n",
    "        num_clients = participating_clients or self.total_clients\n",
    "        if isinstance(num_clients, dict):\n",
    "            num_clients = sum(num_clients.values())\n",
    "        \n",
    "        # Formula: W = 2 * N * ω (upload + download)\n",
    "        round_comm_cost = 2 * num_clients * model_size\n",
    "        self.kpis['communication_cost_per_round'].append(round_comm_cost)\n",
    "        self.kpis['total_communication_bytes'] += round_comm_cost\n",
    "        \n",
    "        # Track by phase\n",
    "        self.kpis['communication_breakdown'][phase] += round_comm_cost\n",
    "        \n",
    "        # Per-cluster communication\n",
    "        if participating_clients and isinstance(participating_clients, dict):\n",
    "            for cluster_id, count in participating_clients.items():\n",
    "                cluster_comm = 2 * count * model_size\n",
    "                self.kpis['per_cluster_communication'][cluster_id].append(cluster_comm)\n",
    "        \n",
    "        # Participation rate\n",
    "        if participating_clients and isinstance(participating_clients, dict):\n",
    "            for cluster_id, count in participating_clients.items():\n",
    "                rate = count / self.clients_per_cluster\n",
    "                self.kpis['participation_rate_per_cluster'][cluster_id].append(rate)\n",
    "        \n",
    "        # Check convergence (variance < 0.01 over last 5 rounds)\n",
    "        if self.kpis['convergence_round'] is None and len(self.kpis['global_accuracy']) >= 10:\n",
    "            recent_acc = self.kpis['global_accuracy'][-10:]\n",
    "            if np.var(recent_acc) < 0.01:\n",
    "                self.kpis['convergence_round'] = round_num\n",
    "                if self.kpis['cumulative_time']:\n",
    "                    self.kpis['convergence_time_seconds'] = self.kpis['cumulative_time'][-1]\n",
    "    \n",
    "    def record_attack_start(self, round_num):\n",
    "        \"\"\"Record when attack starts\"\"\"\n",
    "        self._attack_start_round = round_num\n",
    "        \n",
    "        # Store pre-attack accuracy for degradation calculation\n",
    "        if self.kpis['global_accuracy']:\n",
    "            idx = min(round_num - 1, len(self.kpis['global_accuracy']) - 1)\n",
    "            self._pre_attack_global_acc = self.kpis['global_accuracy'][idx]\n",
    "            self._pre_attack_task_acc = {\n",
    "                task: self.kpis['per_task_accuracy'][task][idx] \n",
    "                if idx < len(self.kpis['per_task_accuracy'][task]) else 0.0\n",
    "                for task in ['traffic', 'duration', 'bandwidth']\n",
    "            }\n",
    "    \n",
    "    def record_attack_detected(self, round_num):\n",
    "        \"\"\"Record when attack is detected\"\"\"\n",
    "        if self._attack_start_round:\n",
    "            self.kpis['detection_time_rounds'] = round_num - self._attack_start_round\n",
    "        self._recovery_start_round = round_num\n",
    "    \n",
    "    def record_recovery_complete(self, round_num):\n",
    "        \"\"\"Record when recovery is complete\"\"\"\n",
    "        self._recovery_end_round = round_num\n",
    "        \n",
    "        # Calculate recovery time in seconds\n",
    "        if self._recovery_start_round and self.kpis['cumulative_time']:\n",
    "            start_time = self.kpis['cumulative_time'][self._recovery_start_round - 1] \\\n",
    "                        if self._recovery_start_round <= len(self.kpis['cumulative_time']) else 0\n",
    "            end_time = self.kpis['cumulative_time'][round_num - 1] \\\n",
    "                      if round_num <= len(self.kpis['cumulative_time']) else self.kpis['cumulative_time'][-1]\n",
    "            self.kpis['recovery_time_seconds'] = end_time - start_time\n",
    "    \n",
    "    def record_accuracy_degradation(self, attack_round_accuracy):\n",
    "        \"\"\"Record accuracy degradation during attack\"\"\"\n",
    "        if hasattr(self, '_pre_attack_global_acc'):\n",
    "            self.kpis['accuracy_degradation_during_attack']['global'] = \\\n",
    "                self._pre_attack_global_acc - attack_round_accuracy.get('global', 0)\n",
    "            \n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                if task in attack_round_accuracy and task in self._pre_attack_task_acc:\n",
    "                    self.kpis['accuracy_degradation_during_attack'][task] = \\\n",
    "                        self._pre_attack_task_acc[task] - attack_round_accuracy[task]\n",
    "                    # Task-specific impact (percentage)\n",
    "                    if self._pre_attack_task_acc[task] > 0:\n",
    "                        self.kpis['task_specific_attack_impact'][task] = \\\n",
    "                            (self._pre_attack_task_acc[task] - attack_round_accuracy[task]) / \\\n",
    "                            self._pre_attack_task_acc[task] * 100\n",
    "    \n",
    "    def record_model_divergence(self, cluster_weights, global_weights):\n",
    "        \"\"\"Record model divergence during isolation (L2 norm)\"\"\"\n",
    "        c0_flat = np.concatenate([w.flatten() for w in cluster_weights])\n",
    "        global_flat = np.concatenate([w.flatten() for w in global_weights])\n",
    "        divergence = np.linalg.norm(c0_flat - global_flat)\n",
    "        self.kpis['model_divergence_during_isolation'].append(divergence)\n",
    "    \n",
    "    def record_gradual_reintegration(self, round_num, participation_percent, accuracy):\n",
    "        \"\"\"Record accuracy during gradual re-integration phases\"\"\"\n",
    "        key_map = {30: '30_percent', 70: '70_percent', 100: '100_percent'}\n",
    "        if participation_percent in key_map:\n",
    "            key = key_map[participation_percent]\n",
    "            self.kpis['gradual_reintegration_effect'][key] = {\n",
    "                'round': round_num,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "    \n",
    "    def record_ch_reelection(self, election_time_seconds, new_ch_energy=None, new_ch_rssi=None):\n",
    "        \"\"\"Record CH re-election event\"\"\"\n",
    "        self.kpis['ch_selection_frequency'] += 1\n",
    "        self.kpis['ch_reelection_time_seconds'].append(election_time_seconds)\n",
    "        \n",
    "        if new_ch_energy is not None:\n",
    "            self.kpis['new_ch0_characteristics']['energy_residual'] = new_ch_energy\n",
    "        if new_ch_rssi is not None:\n",
    "            self.kpis['new_ch0_characteristics']['rssi_avg'] = new_ch_rssi\n",
    "            \n",
    "        # Context-aware score (example: alpha=0.5, beta=0.5)\n",
    "        if new_ch_energy is not None and new_ch_rssi is not None:\n",
    "            alpha, beta = 0.5, 0.5\n",
    "            self.kpis['context_aware_selection_score'] = alpha * new_ch_energy + beta * new_ch_rssi\n",
    "    \n",
    "    def measure_inference_latency(self, test_samples, n_iterations=100):\n",
    "        \"\"\"Measure average inference latency over multiple samples\"\"\"\n",
    "        X_test = test_samples[:n_iterations] if len(test_samples) > n_iterations else test_samples\n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(min(n_iterations, len(X_test))):\n",
    "            sample = X_test[i:i+1]\n",
    "            start = time.perf_counter()\n",
    "            _ = self.model(sample, task='traffic', training=False)\n",
    "            latencies.append((time.perf_counter() - start) * 1000)  # Convert to ms\n",
    "        \n",
    "        self.kpis['inference_latency_ms'] = np.mean(latencies)\n",
    "        self.kpis['inference_latency_std_ms'] = np.std(latencies)\n",
    "    \n",
    "    def measure_computational_load(self):\n",
    "        \"\"\"Measure current CPU and memory usage\"\"\"\n",
    "        process = psutil.Process()\n",
    "        self.kpis['computational_load']['cpu_percent'].append(psutil.cpu_percent())\n",
    "        self.kpis['computational_load']['memory_rss_mb'].append(\n",
    "            process.memory_info().rss / (1024 * 1024)\n",
    "        )\n",
    "    \n",
    "    def compute_final_metrics(self):\n",
    "        \"\"\"Compute derived metrics at the end of experiment\"\"\"\n",
    "        # Bytes per federation round (average)\n",
    "        if self.kpis['communication_cost_per_round']:\n",
    "            self.kpis['bytes_per_federation_round'] = np.mean(\n",
    "                self.kpis['communication_cost_per_round']\n",
    "            )\n",
    "        \n",
    "        # Extra cost due to attack\n",
    "        baseline_per_round = self.kpis['bytes_per_federation_round']\n",
    "        attack_recovery_rounds = 15  # 7 isolation + 8 reintegration typical\n",
    "        baseline_equivalent = baseline_per_round * attack_recovery_rounds\n",
    "        attack_cost = self.kpis['communication_breakdown']['attack']\n",
    "        recovery_cost = self.kpis['communication_breakdown']['recovery']\n",
    "        self.kpis['extra_cost_due_to_attack'] = attack_cost + recovery_cost - baseline_equivalent\n",
    "        \n",
    "        # Time to restore accuracy\n",
    "        if hasattr(self, '_pre_attack_global_acc') and self.kpis['global_accuracy']:\n",
    "            threshold = self._pre_attack_global_acc * 0.99\n",
    "            for i, acc in enumerate(self.kpis['global_accuracy']):\n",
    "                if self._attack_start_round and i >= self._attack_start_round and acc >= threshold:\n",
    "                    self.kpis['time_to_restore_accuracy_rounds'] = i - self._attack_start_round + 1\n",
    "                    break\n",
    "        \n",
    "        # Participation-accuracy correlation\n",
    "        if self.kpis['participation_rate_per_cluster'] and self.kpis['global_accuracy']:\n",
    "            # Average participation rate across clusters\n",
    "            avg_participation = []\n",
    "            for i in range(len(self.kpis['global_accuracy'])):\n",
    "                rates = [\n",
    "                    self.kpis['participation_rate_per_cluster'][cid][i]\n",
    "                    for cid in range(self.n_clusters)\n",
    "                    if i < len(self.kpis['participation_rate_per_cluster'][cid])\n",
    "                ]\n",
    "                if rates:\n",
    "                    avg_participation.append(np.mean(rates))\n",
    "            \n",
    "            if len(avg_participation) > 2 and len(self.kpis['global_accuracy']) > 2:\n",
    "                min_len = min(len(avg_participation), len(self.kpis['global_accuracy']))\n",
    "                try:\n",
    "                    corr, _ = pearsonr(\n",
    "                        avg_participation[:min_len],\n",
    "                        self.kpis['global_accuracy'][:min_len]\n",
    "                    )\n",
    "                    self.kpis['participation_accuracy_correlation'] = corr\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # CH load (assuming equal distribution)\n",
    "        for ch_id in range(self.n_clusters):\n",
    "            self.kpis['ch_load_members_per_ch'][ch_id] = self.clients_per_cluster\n",
    "        \n",
    "        # CH duty cycle estimate (simplified)\n",
    "        energy_per_msg = 0.001  # Joules (example)\n",
    "        total_energy = 1.0  # Joules (example battery)\n",
    "        for ch_id in range(self.n_clusters):\n",
    "            msgs_as_ch = len(self.kpis['round_durations']) * 2  # 2 msgs per round (agg + broadcast)\n",
    "            duty_cycle = (energy_per_msg * msgs_as_ch) / total_energy\n",
    "            self.kpis['ch_duty_cycle'][ch_id] = min(duty_cycle, 1.0)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a formatted summary of all KPIs\"\"\"\n",
    "        self.compute_final_metrics()\n",
    "        \n",
    "        summary = {\n",
    "            # Format sizes for readability\n",
    "            'model_size_formatted': f\"{self.kpis['model_parameter_size_kb']:.2f} KB\",\n",
    "            'architecture_overhead_formatted': f\"{self.kpis['model_architecture_overhead_bytes'] / 1024:.2f} KB\",\n",
    "            'total_communication_formatted': self._format_bytes(self.kpis['total_communication_bytes']),\n",
    "            'bytes_per_round_formatted': self._format_bytes(self.kpis['bytes_per_federation_round']),\n",
    "            \n",
    "            # All raw KPIs\n",
    "            **self.kpis\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def _format_bytes(self, bytes_val):\n",
    "        \"\"\"Format bytes to human readable string\"\"\"\n",
    "        if bytes_val >= 1e9:\n",
    "            return f\"{bytes_val / 1e9:.2f} GB\"\n",
    "        elif bytes_val >= 1e6:\n",
    "            return f\"{bytes_val / 1e6:.2f} MB\"\n",
    "        elif bytes_val >= 1e3:\n",
    "            return f\"{bytes_val / 1e3:.2f} KB\"\n",
    "        else:\n",
    "            return f\"{bytes_val:.0f} B\"\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a comprehensive KPI summary\"\"\"\n",
    "        summary = self.get_summary()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"COMPREHENSIVE KPI SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\n📊 TIER 1: LEARNING PERFORMANCE\")\n",
    "        print(\"-\" * 40)\n",
    "        if summary['global_accuracy']:\n",
    "            print(f\"  Final Global Accuracy: {summary['global_accuracy'][-1]:.4f}\")\n",
    "        print(f\"  Convergence Round: {summary['convergence_round']}\")\n",
    "        print(f\"  Convergence Time: {summary['convergence_time_seconds']:.2f}s\" if summary['convergence_time_seconds'] else \"  Convergence Time: N/A\")\n",
    "        if summary['round_durations']:\n",
    "            print(f\"  Avg Round Duration: {np.mean(summary['round_durations']):.3f}s\")\n",
    "        \n",
    "        print(\"\\n  Per-Task Final Accuracy:\")\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if summary['per_task_accuracy'][task]:\n",
    "                print(f\"    {task.capitalize()}: {summary['per_task_accuracy'][task][-1]:.4f}\")\n",
    "        \n",
    "        print(\"\\n  Per-Cluster Final Accuracy:\")\n",
    "        for cid in range(self.n_clusters):\n",
    "            if summary['per_cluster_accuracy'][cid]:\n",
    "                print(f\"    Cluster {cid}: {summary['per_cluster_accuracy'][cid][-1]:.4f}\")\n",
    "        \n",
    "        print(\"\\n🏗️ TIER 1: MODEL ARCHITECTURE & RESOURCES\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Model Parameter Size: {summary['model_size_formatted']}\")\n",
    "        print(f\"  Architecture Overhead: {summary['architecture_overhead_formatted']}\")\n",
    "        print(f\"  Inference Latency: {summary['inference_latency_ms']:.3f} ± {summary['inference_latency_std_ms']:.3f} ms\")\n",
    "        if summary['computational_load']['cpu_percent']:\n",
    "            print(f\"  Avg CPU Load: {np.mean(summary['computational_load']['cpu_percent']):.1f}%\")\n",
    "            print(f\"  Avg Memory (RSS): {np.mean(summary['computational_load']['memory_rss_mb']):.1f} MB\")\n",
    "        \n",
    "        print(\"\\n📡 TIER 1: COMMUNICATION EFFICIENCY\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Total Communication: {summary['total_communication_formatted']}\")\n",
    "        print(f\"  Avg Bytes/Round: {summary['bytes_per_round_formatted']}\")\n",
    "        print(f\"  Communication Breakdown:\")\n",
    "        print(f\"    Normal: {self._format_bytes(summary['communication_breakdown']['normal'])}\")\n",
    "        print(f\"    Attack: {self._format_bytes(summary['communication_breakdown']['attack'])}\")\n",
    "        print(f\"    Recovery: {self._format_bytes(summary['communication_breakdown']['recovery'])}\")\n",
    "        print(f\"  Extra Cost Due to Attack: {self._format_bytes(summary['extra_cost_due_to_attack'])}\")\n",
    "        \n",
    "        print(\"\\n⚔️ TIER 2: ATTACK IMPACT & RECOVERY\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Detection Time: {summary['detection_time_rounds']} rounds\")\n",
    "        print(f\"  Recovery Time Breakdown:\")\n",
    "        print(f\"    Detection: {summary['recovery_time_breakdown']['detection']} rounds\")\n",
    "        print(f\"    Isolation: {summary['recovery_time_breakdown']['isolation']} rounds\")\n",
    "        print(f\"    Reintegration: {summary['recovery_time_breakdown']['reintegration']} rounds\")\n",
    "        print(f\"  Recovery Time (Wall-clock): {summary['recovery_time_seconds']:.2f}s\")\n",
    "        print(f\"  Accuracy Degradation During Attack:\")\n",
    "        for key, val in summary['accuracy_degradation_during_attack'].items():\n",
    "            print(f\"    {key.capitalize()}: {val:.4f}\")\n",
    "        print(f\"  Time to Restore Accuracy: {summary['time_to_restore_accuracy_rounds']} rounds\")\n",
    "        print(f\"  Task-Specific Attack Impact (% drop):\")\n",
    "        for task, impact in summary['task_specific_attack_impact'].items():\n",
    "            print(f\"    {task.capitalize()}: {impact:.2f}%\")\n",
    "        \n",
    "        print(\"\\n🏥 TIER 2: CLUSTER HEALTH & PARTICIPATION\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Gradual Re-integration Effect:\")\n",
    "        for pct, data in summary['gradual_reintegration_effect'].items():\n",
    "            if data['round']:\n",
    "                print(f\"    {pct}: Round {data['round']}, Accuracy {data['accuracy']:.4f}\")\n",
    "        print(f\"  Participation-Accuracy Correlation: {summary['participation_accuracy_correlation']:.4f}\")\n",
    "        \n",
    "        print(\"\\n👑 TIER 2: CH SELECTION & LOAD\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  CH Load (Members/CH):\")\n",
    "        for ch_id, load in summary['ch_load_members_per_ch'].items():\n",
    "            print(f\"    CH{ch_id}: {load} members\")\n",
    "        print(f\"  CH Selection Frequency: {summary['ch_selection_frequency']} re-elections\")\n",
    "        if summary['ch_reelection_time_seconds']:\n",
    "            print(f\"  Avg CH Re-election Time: {np.mean(summary['ch_reelection_time_seconds']):.4f}s\")\n",
    "        print(f\"  New CH0 Characteristics:\")\n",
    "        print(f\"    Energy Residual: {summary['new_ch0_characteristics']['energy_residual']:.4f}\")\n",
    "        print(f\"    RSSI Avg: {summary['new_ch0_characteristics']['rssi_avg']:.4f}\")\n",
    "        print(f\"  Context-Aware Selection Score: {summary['context_aware_selection_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data structures defined (ClientDataHierarchical, UAVMetrics)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA STRUCTURES FOR HIERARCHICAL SETUP\n",
    "# ============================================================================\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ClientDataHierarchical:\n",
    "    \"\"\"Holds client's data for all tasks with cluster assignment\"\"\"\n",
    "    ds: Dict[str, Tuple[np.ndarray, np.ndarray]]  # task -> (X, y)\n",
    "    cluster_id: int\n",
    "    client_id: int\n",
    "\n",
    "@dataclass\n",
    "class UAVMetrics:\n",
    "    \"\"\"Metrics for UAV/client used in CH selection\"\"\"\n",
    "    client_id: int\n",
    "    cluster_id: int\n",
    "    energy_residual: float\n",
    "    rssi_avg: float\n",
    "    num_examples: int\n",
    "    param_change: float\n",
    "\n",
    "print(\" Data structures defined (ClientDataHierarchical, UAVMetrics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ClusterAwareClient defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLUSTER-AWARE CLIENT WITH CONTEXT METRICS\n",
    "# ============================================================================\n",
    "\n",
    "class ClusterAwareClient(fl.client.NumPyClient):\n",
    "    \"\"\"Client with UAV context metrics (energy, RSSI) for CH selection\"\"\"\n",
    "    def __init__(self, model, client_data, cfg, cluster_id, client_id):\n",
    "        self.model = model\n",
    "        self.client_data = client_data\n",
    "        self.cfg = cfg\n",
    "        self.cluster_id = cluster_id\n",
    "        self.client_id = client_id\n",
    "        \n",
    "        # Simulated context metrics for CH selection\n",
    "        np.random.seed(seed + client_id)\n",
    "        self.energy_residual = np.random.uniform(0.5, 1.0)\n",
    "        self.rssi_avg = np.random.uniform(0.6, 1.0)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=cfg['lr'])\n",
    "        self.loss_fns = {\n",
    "            'traffic': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'duration': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'bandwidth': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        }\n",
    "        self.loss_weights = cfg['loss_weights']\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        initial_params = [np.copy(p) for p in parameters]\n",
    "        \n",
    "        # Local training\n",
    "        for epoch in range(self.cfg['local_epochs']):\n",
    "            with tf.GradientTape() as tape:\n",
    "                epoch_loss = 0.0\n",
    "                epoch_tasks = 0\n",
    "                \n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    if task not in self.client_data or len(self.client_data[task][0]) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    X, y = self.client_data[task]\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    y_tensor = tf.convert_to_tensor(y, dtype=tf.int64)\n",
    "                    \n",
    "                    logits = self.model(X_tensor, task=task, training=True)\n",
    "                    loss = self.loss_fns[task](y_tensor, logits)\n",
    "                    weighted_loss = loss * self.loss_weights.get(task, 1.0)\n",
    "                    \n",
    "                    epoch_loss += weighted_loss\n",
    "                    epoch_tasks += 1\n",
    "                \n",
    "                if epoch_tasks > 0:\n",
    "                    avg_loss = epoch_loss / epoch_tasks\n",
    "                    gradients = tape.gradient(avg_loss, self.model.trainable_weights)\n",
    "                    self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n",
    "        \n",
    "        # Compute parameter change\n",
    "        final_params = self.model.get_weights()\n",
    "        param_change = np.mean([np.linalg.norm(f - i) for f, i in zip(final_params, initial_params)])\n",
    "        \n",
    "        # Compute accuracy\n",
    "        total_acc = 0.0\n",
    "        num_tasks = 0\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task not in self.client_data or len(self.client_data[task][0]) == 0:\n",
    "                continue\n",
    "            X, y = self.client_data[task]\n",
    "            X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "            logits = self.model(X_tensor, task=task, training=False)\n",
    "            preds = tf.argmax(logits, axis=1).numpy()\n",
    "            acc = float(np.mean(preds == y))\n",
    "            total_acc += acc\n",
    "            num_tasks += 1\n",
    "        \n",
    "        num_examples = sum(len(self.client_data[t][1]) for t in ['traffic', 'duration', 'bandwidth'] \n",
    "                         if t in self.client_data)\n",
    "        \n",
    "        return final_params, num_examples, {\n",
    "            \"accuracy\": total_acc / max(num_tasks, 1),\n",
    "            \"cluster_id\": self.cluster_id,\n",
    "            \"client_id\": self.client_id,\n",
    "            \"energy_residual\": self.energy_residual,\n",
    "            \"rssi_avg\": self.rssi_avg,\n",
    "            \"param_change\": float(param_change),\n",
    "            \"num_examples\": num_examples\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        total_acc = 0.0\n",
    "        num_tasks = 0\n",
    "        \n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task not in self.client_data or len(self.client_data[task][0]) == 0:\n",
    "                continue\n",
    "            X, y = self.client_data[task]\n",
    "            X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "            logits = self.model(X_tensor, task=task, training=False)\n",
    "            preds = tf.argmax(logits, axis=1).numpy()\n",
    "            acc = float(np.mean(preds == y))\n",
    "            total_acc += acc\n",
    "            num_tasks += 1\n",
    "        \n",
    "        num_examples = sum(len(self.client_data[t][1]) for t in ['traffic', 'duration', 'bandwidth'] \n",
    "                         if t in self.client_data)\n",
    "        \n",
    "        return 0.0, num_examples, {\"accuracy\": total_acc / max(num_tasks, 1)}\n",
    "\n",
    "print(\" ClusterAwareClient defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Client partitioning function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLIENT PARTITIONING FOR HIERARCHICAL SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def build_client_partitions_hierarchical(verbose=True):\n",
    "    \"\"\"Build client partitions for hierarchical setup with equal split\"\"\"\n",
    "    n_clients = CFG['n_clients_flat']\n",
    "    n_clusters = CFG['n_clusters']\n",
    "    clients_per_cluster = CFG['clients_per_cluster']\n",
    "    \n",
    "    # Equal split: divide samples equally among clusters\n",
    "    samples_per_cluster = len(y_traf_train) // n_clusters\n",
    "    cluster_indices = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        start_idx = cluster_id * samples_per_cluster\n",
    "        end_idx = start_idx + samples_per_cluster if cluster_id < n_clusters - 1 else len(y_traf_train)\n",
    "        cluster_indices.append(np.arange(start_idx, end_idx))\n",
    "    \n",
    "    # Divide each cluster's data among its clients\n",
    "    client_indices_flat = []\n",
    "    client_index_to_cluster = {}\n",
    "    \n",
    "    for cluster_id, cluster_idxs in enumerate(cluster_indices):\n",
    "        np.random.shuffle(cluster_idxs)\n",
    "        samples_per_client = len(cluster_idxs) // clients_per_cluster\n",
    "        \n",
    "        for local_client_id in range(clients_per_cluster):\n",
    "            start = local_client_id * samples_per_client\n",
    "            end = start + samples_per_client if local_client_id < clients_per_cluster - 1 else len(cluster_idxs)\n",
    "            client_idxs = cluster_idxs[start:end]\n",
    "            client_indices_flat.append(client_idxs)\n",
    "            \n",
    "            global_client_id = cluster_id * clients_per_cluster + local_client_id\n",
    "            client_index_to_cluster[global_client_id] = cluster_id\n",
    "    \n",
    "    # Create ClientDataHierarchical objects\n",
    "    clients = []\n",
    "    for client_id, indices in enumerate(client_indices_flat):\n",
    "        cluster_id = client_index_to_cluster[client_id]\n",
    "        \n",
    "        client_ds = {\n",
    "            'traffic': (X_traffic_train[indices].astype(np.float32), y_traf_train[indices]),\n",
    "            'duration': (X_duration_train[indices].astype(np.float32), y_dur_train[indices]),\n",
    "            'bandwidth': (X_bandwidth_train[indices].astype(np.float32), y_bw_train[indices])\n",
    "        }\n",
    "        \n",
    "        clients.append(ClientDataHierarchical(ds=client_ds, cluster_id=cluster_id, client_id=client_id))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Client partitioning complete (equal split):\")\n",
    "        print(f\"   Total clients: {len(clients)}\")\n",
    "        print(f\"   Clusters: {n_clusters}\")\n",
    "        print(f\"   Clients per cluster: {clients_per_cluster}\")\n",
    "        \n",
    "        sizes = [sum(len(c.ds[t][1]) for t in ['traffic', 'duration', 'bandwidth']) // 3 for c in clients]\n",
    "        print(f\"   Sample sizes: min={min(sizes)}, max={max(sizes)}, avg={np.mean(sizes):.1f}\")\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_clients = [c for c in clients if c.cluster_id == cluster_id]\n",
    "            cluster_samples = sum(len(c.ds['traffic'][1]) for c in cluster_clients)\n",
    "            print(f\"   Cluster {cluster_id}: {len(cluster_clients)} clients, {cluster_samples} samples\")\n",
    "    \n",
    "    return clients, client_index_to_cluster\n",
    "\n",
    "print(\" Client partitioning function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PER-CLUSTER TEST DATA PARTITIONS (Equal and Dirichlet)\n",
    "# ============================================================================\n",
    "\n",
    "def create_per_cluster_test_data_equal(test_data, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Create per-cluster test data with EQUAL split.\n",
    "    \n",
    "    Args:\n",
    "        test_data: Dict with 'traffic', 'duration', 'bandwidth' tasks\n",
    "                   Each task has (X_test, y_test)\n",
    "        n_clusters: Number of clusters\n",
    "    \n",
    "    Returns:\n",
    "        Dict[cluster_id] -> {'traffic': (X, y), 'duration': (X, y), 'bandwidth': (X, y)}\n",
    "    \"\"\"\n",
    "    cluster_test_data = {}\n",
    "    \n",
    "    for task in ['traffic', 'duration', 'bandwidth']:\n",
    "        if task not in test_data:\n",
    "            continue\n",
    "        \n",
    "        X_test, y_test = test_data[task]\n",
    "        n_samples = len(X_test)\n",
    "        samples_per_cluster = n_samples // n_clusters\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            if cluster_id not in cluster_test_data:\n",
    "                cluster_test_data[cluster_id] = {}\n",
    "            \n",
    "            start_idx = cluster_id * samples_per_cluster\n",
    "            end_idx = start_idx + samples_per_cluster if cluster_id < n_clusters - 1 else n_samples\n",
    "            \n",
    "            cluster_test_data[cluster_id][task] = (\n",
    "                X_test[start_idx:end_idx],\n",
    "                y_test[start_idx:end_idx]\n",
    "            )\n",
    "    \n",
    "    print(f\" Created EQUAL split per-cluster test data:\")\n",
    "    for cid in cluster_test_data:\n",
    "        print(f\"   Cluster {cid}: {len(cluster_test_data[cid]['traffic'][1])} samples per task\")\n",
    "    \n",
    "    return cluster_test_data\n",
    "\n",
    "\n",
    "def create_per_cluster_test_data_dirichlet(test_data, n_clusters=3, alpha=0.4, seed=42):\n",
    "    \"\"\"\n",
    "    Create per-cluster test data with DIRICHLET (non-IID) split.\n",
    "    \n",
    "    Args:\n",
    "        test_data: Dict with 'traffic', 'duration', 'bandwidth' tasks\n",
    "        n_clusters: Number of clusters\n",
    "        alpha: Dirichlet concentration parameter (lower = more non-IID)\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Dict[cluster_id] -> {'traffic': (X, y), 'duration': (X, y), 'bandwidth': (X, y)}\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    cluster_test_data = {}\n",
    "    \n",
    "    for task in ['traffic', 'duration', 'bandwidth']:\n",
    "        if task not in test_data:\n",
    "            continue\n",
    "        \n",
    "        X_test, y_test = test_data[task]\n",
    "        n_samples = len(X_test)\n",
    "        \n",
    "        # Get class labels and generate Dirichlet distribution\n",
    "        unique_labels = np.unique(y_test)\n",
    "        n_classes = len(unique_labels)\n",
    "        \n",
    "        # For each class, sample Dirichlet distribution for cluster allocation\n",
    "        cluster_indices = [[] for _ in range(n_clusters)]\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            label_indices = np.where(y_test == label)[0]\n",
    "            n_label_samples = len(label_indices)\n",
    "            \n",
    "            # Sample from Dirichlet\n",
    "            proportions = np.random.dirichlet([alpha] * n_clusters)\n",
    "            proportions = (proportions * n_label_samples).astype(int)\n",
    "            \n",
    "            # Adjust to ensure all samples are assigned\n",
    "            proportions[-1] = n_label_samples - proportions[:-1].sum()\n",
    "            \n",
    "            # Assign indices to clusters\n",
    "            start = 0\n",
    "            for cluster_id in range(n_clusters):\n",
    "                end = start + proportions[cluster_id]\n",
    "                cluster_indices[cluster_id].extend(label_indices[start:end])\n",
    "                start = end\n",
    "        \n",
    "        # Store data for each cluster\n",
    "        for cluster_id in range(n_clusters):\n",
    "            if cluster_id not in cluster_test_data:\n",
    "                cluster_test_data[cluster_id] = {}\n",
    "            \n",
    "            indices = cluster_indices[cluster_id]\n",
    "            cluster_test_data[cluster_id][task] = (\n",
    "                X_test[indices],\n",
    "                y_test[indices]\n",
    "            )\n",
    "    \n",
    "    print(f\"Created DIRICHLET split per-cluster test data (alpha={alpha}):\")\n",
    "    for cid in cluster_test_data:\n",
    "        print(f\"   Cluster {cid}: {len(cluster_test_data[cid]['traffic'][1])} samples per task\")\n",
    "        # Show class distribution\n",
    "        labels, counts = np.unique(cluster_test_data[cid]['traffic'][1], return_counts=True)\n",
    "        print(f\"      Class distribution: {dict(zip(labels, counts))}\")\n",
    "    \n",
    "    return cluster_test_data\n",
    "\n",
    "\n",
    "# Example usage (will be called in convergence/transient scenarios)\n",
    "# cluster_test_equal = create_per_cluster_test_data_equal(test_data)\n",
    "# cluster_test_dirichlet = create_per_cluster_test_data_dirichlet(test_data, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IntegratedHierarchicalCHStrategy defined (FIXED - Cluster 0 frozen during D&R-E)\n",
      "   Features: Hierarchical aggregation + CH compromise + Integrated testing + Frozen params\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INTEGRATED HIERARCHICAL STRATEGY WITH CH COMPROMISE + TESTING (FIXED)\n",
    "# ============================================================================\n",
    "\n",
    "class IntegratedHierarchicalCHStrategy(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Hierarchical FedAvg with:\n",
    "    - CH Compromise Recovery (detection, isolation, gradual re-entry)\n",
    "    - Integrated Testing (tests model EVERY round on test data)\n",
    "    - Context-Aware CH Selection\n",
    "    - Per-Cluster Testing (equal AND dirichlet splits)\n",
    "    - 🔥 FIXED: Cluster 0 parameters frozen during D&R-E phase\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        test_data_equal=None,\n",
    "        test_data_dirichlet=None,\n",
    "        model_class=None,\n",
    "        in_dims=None,\n",
    "        n_classes=None,\n",
    "        max_dim=None,\n",
    "        compromise_round=None,\n",
    "        compromised_cluster=0,\n",
    "        global_aggregator_cluster=1,\n",
    "        client_list=None,\n",
    "        detection_rounds=7,\n",
    "        continuity_rounds=3,\n",
    "        alpha_energy=0.6,\n",
    "        beta_rssi=0.4,\n",
    "        kpi_tracker=None,\n",
    "        save_dir=None,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.test_data_equal = test_data_equal\n",
    "        self.test_data_dirichlet = test_data_dirichlet\n",
    "        self.model_class = model_class\n",
    "        self.in_dims = in_dims\n",
    "        self.n_classes = n_classes\n",
    "        self.max_dim = max_dim\n",
    "        self.compromise_round = compromise_round\n",
    "        self.compromised_cluster = compromised_cluster\n",
    "        self.global_aggregator_cluster = global_aggregator_cluster\n",
    "        self.ch_compromised = compromise_round is not None\n",
    "        self.compromise_detected_round = None\n",
    "        self.recovery_phase = None\n",
    "        self.cluster_test_accuracies_by_round = {'equal': {}, 'dirichlet': {}}\n",
    "        self.cluster_heads = {}\n",
    "        self.cluster_uav_metrics = {}\n",
    "        self.client_list = client_list or []\n",
    "        self.detection_rounds = detection_rounds\n",
    "        self.continuity_rounds = continuity_rounds\n",
    "        self.alpha_energy = alpha_energy\n",
    "        self.beta_rssi = beta_rssi\n",
    "        self.recovery_log = []\n",
    "        self.kpi_tracker = kpi_tracker\n",
    "        self.save_dir = save_dir\n",
    "        self._frozen_cluster_params = None  # 🔥 Store frozen params for Cluster 0\n",
    "        \n",
    "        if self.save_dir:\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "    \n",
    "    def _get_participation_fraction(self, rounds_since_detection):\n",
    "        \"\"\"Calculate participation fraction during continuity phase\"\"\"\n",
    "        if rounds_since_detection < 0 or rounds_since_detection < self.detection_rounds:\n",
    "            return 0.0\n",
    "        \n",
    "        continuity_start = self.detection_rounds\n",
    "        continuity_progress = rounds_since_detection - continuity_start\n",
    "        \n",
    "        if continuity_progress < 0:\n",
    "            return 0.0\n",
    "        elif continuity_progress == 0:\n",
    "            return 0.3\n",
    "        elif continuity_progress == 1:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 1.0\n",
    "    \n",
    "    def _context_aware_ch_selection(self, uav_list):\n",
    "        \"\"\"Select CH based on energy and RSSI\"\"\"\n",
    "        if not uav_list:\n",
    "            return None\n",
    "        \n",
    "        best_score = -1\n",
    "        best_uav = None\n",
    "        \n",
    "        for uav in uav_list:\n",
    "            energy = uav.get('energy_residual', 0.75)\n",
    "            rssi = uav.get('rssi_avg', 0.8)\n",
    "            score = self.alpha_energy * energy + self.beta_rssi * rssi\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_uav = uav.get('client_id')\n",
    "        \n",
    "        return best_uav\n",
    "    \n",
    "    def _ndarrays_weighted_average(self, pairs):\n",
    "        \"\"\"Weighted average of ndarrays\"\"\"\n",
    "        total_weight = sum(w for _, w in pairs)\n",
    "        if total_weight == 0:\n",
    "            return pairs[0][0]\n",
    "        \n",
    "        summed = [np.zeros_like(arr) for arr in pairs[0][0]]\n",
    "        for ndarrays, w in pairs:\n",
    "            for i, arr in enumerate(ndarrays):\n",
    "                summed[i] = summed[i] + (arr * (w / total_weight))\n",
    "        return summed\n",
    "    \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        # Start round timing\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.start_round()\n",
    "            \n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Measure computational load during aggregation\n",
    "        if self.kpi_tracker:\n",
    "            self.kpi_tracker.measure_computational_load()\n",
    "        \n",
    "        # Step 1: CH Compromise Logic\n",
    "        participating_clusters = set()\n",
    "        \n",
    "        if self.ch_compromised and server_round == self.compromise_round:\n",
    "            print(f\"\\n🚨 COMPROMISE DETECTED: Round {server_round}, Cluster {self.compromised_cluster}\")\n",
    "            self.compromise_detected_round = server_round + 1\n",
    "            self.recovery_phase = 'detection'\n",
    "            self.recovery_log.append({\n",
    "                'round': server_round,\n",
    "                'event': 'COMPROMISE_DETECTED',\n",
    "                'cluster': self.compromised_cluster\n",
    "            })\n",
    "            \n",
    "            if self.kpi_tracker:\n",
    "                self.kpi_tracker.record_attack_detected(server_round)\n",
    "        \n",
    "        # Step 2: CH Re-election & Recovery\n",
    "        if self.ch_compromised and self.compromise_detected_round is not None:\n",
    "            rounds_since_detection = server_round - self.compromise_detected_round\n",
    "            \n",
    "            # 🔍 DEBUG: Print phase logic\n",
    "            if server_round >= self.compromise_round and server_round <= self.compromise_round + 5:\n",
    "                print(f\" [Round {server_round}] Phase Logic: rounds_since={rounds_since_detection}, \"\n",
    "                      f\"detection_rounds={self.detection_rounds}, continuity={self.continuity_rounds}, \"\n",
    "                      f\"current_phase={self.recovery_phase}\")\n",
    "            \n",
    "            # Detection phase: rounds_since_detection < detection_rounds\n",
    "            if rounds_since_detection < self.detection_rounds:\n",
    "                # Stay in detection phase (cluster isolated)\n",
    "                pass\n",
    "            elif rounds_since_detection == self.detection_rounds:\n",
    "                # Transition to continuity phase (gradual re-entry)\n",
    "                if self.recovery_phase != 'continuity':\n",
    "                    self.recovery_phase = 'continuity'\n",
    "                    # CH re-election\n",
    "                    if self.compromised_cluster in self.cluster_uav_metrics:\n",
    "                        cluster_uavs = self.cluster_uav_metrics[self.compromised_cluster]\n",
    "                        new_ch = self._context_aware_ch_selection(cluster_uavs)\n",
    "                        old_ch = self.cluster_heads.get(self.compromised_cluster, None)\n",
    "                        self.cluster_heads[self.compromised_cluster] = new_ch\n",
    "                        \n",
    "                        print(f\"CH RE-ELECTION: Cluster {self.compromised_cluster}, New CH: {new_ch}\")\n",
    "                        self.recovery_log.append({\n",
    "                            'round': server_round,\n",
    "                            'event': 'CH_REELECTION',\n",
    "                            'cluster': self.compromised_cluster,\n",
    "                            'new_ch': new_ch\n",
    "                        })\n",
    "            elif rounds_since_detection >= self.detection_rounds + self.continuity_rounds:\n",
    "                # After detection + continuity phases are complete\n",
    "                if self.recovery_phase != 'complete':\n",
    "                    self.recovery_phase = 'complete'\n",
    "                    print(f\"RECOVERY COMPLETE: Round {server_round}\")\n",
    "                    self.recovery_log.append({\n",
    "                        'round': server_round,\n",
    "                        'event': 'RECOVERY_COMPLETE',\n",
    "                        'cluster': self.compromised_cluster\n",
    "                    })\n",
    "        \n",
    "        # Step 3: Extract all triples first (before any filtering)\n",
    "        triples = []\n",
    "        for client_proxy, fit_res in results:\n",
    "            nds = fl.common.parameters_to_ndarrays(fit_res.parameters)\n",
    "            weight = getattr(fit_res, 'num_examples', None)\n",
    "            if weight is None:\n",
    "                weight = int(fit_res.metrics.get('num_examples', 1)) if hasattr(fit_res, 'metrics') else 1\n",
    "            cluster_id = int(fit_res.metrics.get('cluster_id', 0)) if hasattr(fit_res, 'metrics') else 0\n",
    "            \n",
    "            # Store UAV metrics for context-aware CH selection\n",
    "            if hasattr(fit_res, 'metrics') and fit_res.metrics:\n",
    "                energy = fit_res.metrics.get('energy_residual')\n",
    "                rssi = fit_res.metrics.get('rssi_avg')\n",
    "                if energy is not None and rssi is not None:\n",
    "                    if cluster_id not in self.cluster_uav_metrics:\n",
    "                        self.cluster_uav_metrics[cluster_id] = []\n",
    "                    self.cluster_uav_metrics[cluster_id].append({\n",
    "                        'client_id': fit_res.metrics.get('client_id'),\n",
    "                        'energy_residual': energy,\n",
    "                        'rssi_avg': rssi\n",
    "                    })\n",
    "            \n",
    "            triples.append((nds, weight, cluster_id, fit_res.metrics if hasattr(fit_res, 'metrics') else {}))\n",
    "        \n",
    "        # Step 4: Build temp params for ALL clusters BEFORE filtering for freeze check\n",
    "        temp_cluster_to_pairs = {}\n",
    "        for nds, w, cid, metrics in triples:\n",
    "            temp_cluster_to_pairs.setdefault(cid, []).append((nds, w, metrics))\n",
    "        \n",
    "        # Aggregate ALL clusters temporarily (including Cluster 0)\n",
    "        temp_cluster_params = {}\n",
    "        for cid, pairs in temp_cluster_to_pairs.items():\n",
    "            clean_pairs = [(nds, w) for nds, w, _ in pairs]\n",
    "            if clean_pairs:\n",
    "                temp_cluster_params[cid] = self._ndarrays_weighted_average(clean_pairs)\n",
    "        \n",
    "        # FREEZE LOGIC: Capture Cluster 0 params at compromise round BEFORE filtering\n",
    "        if (self.ch_compromised and \n",
    "            server_round == self.compromise_round and \n",
    "            self.compromised_cluster == 0 and\n",
    "            0 in temp_cluster_params and\n",
    "            self._frozen_cluster_params is None):\n",
    "            self._frozen_cluster_params = [arr.copy() for arr in temp_cluster_params[0]]\n",
    "            print(f\"  🧊 Froze Cluster 0 parameters at round {server_round} (last round before isolation)\")\n",
    "            \n",
    "            # 🔍 DEBUG: Print hash of frozen params for verification\n",
    "            frozen_hash = hash(str([w.flatten()[0] for w in self._frozen_cluster_params]))\n",
    "            print(f\"     Frozen params hash: {frozen_hash}\")\n",
    "        \n",
    "        # Step 5: NOW apply participation control filtering\n",
    "        cluster_to_pairs = {}\n",
    "        \n",
    "        for nds, w, cid, metrics in triples:\n",
    "            # FIX: Only apply participation control if compromise has been detected\n",
    "            if self.ch_compromised and self.compromise_detected_round is not None and cid == self.compromised_cluster:\n",
    "                rounds_since_detection = server_round - self.compromise_detected_round\n",
    "                participation_fraction = self._get_participation_fraction(rounds_since_detection)\n",
    "                \n",
    "                if participation_fraction == 0:\n",
    "                    continue  # Skip during D&R-E\n",
    "                elif participation_fraction < 1.0:\n",
    "                    # Gradual re-entry: select top clients by context score\n",
    "                    cluster_to_pairs.setdefault(cid, []).append((nds, w, metrics))\n",
    "                    continue\n",
    "            \n",
    "            cluster_to_pairs.setdefault(cid, []).append((nds, w, None))\n",
    "        \n",
    "        # Handle gradual participation for compromised cluster\n",
    "        if self.ch_compromised and self.compromise_detected_round is not None and self.compromised_cluster in cluster_to_pairs:\n",
    "            pairs = cluster_to_pairs[self.compromised_cluster]\n",
    "            rounds_since_detection = server_round - self.compromise_detected_round\n",
    "            participation_fraction = self._get_participation_fraction(rounds_since_detection)\n",
    "            \n",
    "            if participation_fraction < 1.0:\n",
    "                # Select top fraction by context score\n",
    "                client_scores = []\n",
    "                for nds, w, metrics in pairs:\n",
    "                    if metrics:\n",
    "                        energy = metrics.get('energy_residual', 0.75)\n",
    "                        rssi = metrics.get('rssi_avg', 0.8)\n",
    "                        score = self.alpha_energy * energy + self.beta_rssi * rssi\n",
    "                        client_scores.append((nds, w, score))\n",
    "                \n",
    "                client_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "                n_select = max(1, int(len(client_scores) * participation_fraction))\n",
    "                selected = client_scores[:n_select]\n",
    "                cluster_to_pairs[self.compromised_cluster] = [(nds, w, None) for nds, w, _ in selected]\n",
    "        \n",
    "        # Aggregate within clusters (after filtering)\n",
    "        cluster_params = {}\n",
    "        cluster_weights = {}\n",
    "        \n",
    "        for cid, pairs in cluster_to_pairs.items():\n",
    "            clean_pairs = [(nds, w) for nds, w, _ in pairs]\n",
    "            if clean_pairs:\n",
    "                cluster_params[cid] = self._ndarrays_weighted_average(clean_pairs)\n",
    "                cluster_weights[cid] = float(sum(w for _, w in clean_pairs))\n",
    "                participating_clusters.add(cid)\n",
    "        \n",
    "        if not cluster_params:\n",
    "            return None, {}\n",
    "        \n",
    "        # Step 6: Inter-cluster aggregation (global) - AT CLUSTER 1\n",
    "        if self.global_aggregator_cluster in cluster_params:\n",
    "            global_pairs = []\n",
    "            \n",
    "            # Add Cluster 0's model (if participating)\n",
    "            if 0 in cluster_params:\n",
    "                global_pairs.append((cluster_params[0], cluster_weights[0]))\n",
    "            \n",
    "            # Add Cluster 2's model (if participating)\n",
    "            if 2 in cluster_params:\n",
    "                global_pairs.append((cluster_params[2], cluster_weights[2]))\n",
    "            \n",
    "            # Add Cluster 1's own model (the global aggregator)\n",
    "            global_pairs.append((cluster_params[self.global_aggregator_cluster], \n",
    "                                cluster_weights[self.global_aggregator_cluster]))\n",
    "            \n",
    "            # Perform weighted average at Cluster 1\n",
    "            averaged = self._ndarrays_weighted_average(global_pairs)\n",
    "            aggregated_params = fl.common.ndarrays_to_parameters(averaged)\n",
    "            \n",
    "            if server_round % 10 == 0 or server_round <= 5:\n",
    "                print(f\"  🌍 [Round {server_round}] Global aggregation at Cluster {self.global_aggregator_cluster}\")\n",
    "                print(f\"     Participating clusters: {list(cluster_params.keys())}\")\n",
    "        else:\n",
    "            # Fallback\n",
    "            print(f\"  [Round {server_round}] Global aggregator (Cluster {self.global_aggregator_cluster}) not available\")\n",
    "            print(f\"     Fallback: Aggregating available clusters: {list(cluster_params.keys())}\")\n",
    "            global_pairs = [(cluster_params[cid], cluster_weights[cid]) for cid in cluster_params.keys()]\n",
    "            averaged = self._ndarrays_weighted_average(global_pairs)\n",
    "            aggregated_params = fl.common.ndarrays_to_parameters(averaged)\n",
    "        \n",
    "        # Step 7: INTEGRATED TESTING - Use frozen parameters for Cluster 0 during D&R-E\n",
    "        test_results = {'equal': {}, 'dirichlet': {}}\n",
    "        \n",
    "        # Test on EQUAL split per-cluster data\n",
    "        if self.test_data_equal is not None and server_round > 0:\n",
    "            for cid in range(3):  # Always test all 3 clusters\n",
    "                if cid not in self.test_data_equal:\n",
    "                    continue\n",
    "                \n",
    "                # CRITICAL FIX: Use frozen parameters for Cluster 0 during isolation (participation_fraction == 0)\n",
    "                use_frozen = False\n",
    "                if (cid == self.compromised_cluster and \n",
    "                    self.compromise_detected_round is not None and\n",
    "                    self._frozen_cluster_params is not None):\n",
    "                    # Check if cluster is isolated (participation_fraction == 0)\n",
    "                    rounds_since_detection = server_round - self.compromise_detected_round\n",
    "                    participation_fraction = self._get_participation_fraction(rounds_since_detection)\n",
    "                    if participation_fraction == 0.0:\n",
    "                        use_frozen = True\n",
    "                        test_params = self._frozen_cluster_params  # Use frozen params\n",
    "                        \n",
    "                        # 🔍 DEBUG: Verify frozen params haven't changed\n",
    "                        if rounds_since_detection == 0:\n",
    "                            frozen_hash = hash(str([w.flatten()[0] for w in self._frozen_cluster_params]))\n",
    "                            current_hash = hash(str([w.flatten()[0] for w in test_params]))\n",
    "                            live_hash = hash(str([w.flatten()[0] for w in averaged]))\n",
    "                            print(f\"  ❄️  Testing Cluster {cid} with FROZEN parameters during isolation\")\n",
    "                            print(f\"     Frozen hash: {frozen_hash}\")\n",
    "                            print(f\"     Current test hash: {current_hash}\")\n",
    "                            print(f\"     Live aggregated hash: {live_hash}\")\n",
    "                            print(f\"     Frozen vs Live: {'DIFFERENT' if frozen_hash != live_hash else 'SAME'}\")\n",
    "                        \n",
    "                        # Print every round during isolation\n",
    "                        if participation_fraction == 0.0:\n",
    "                            print(f\"  ❄️  [Round {server_round}] Using FROZEN params for Cluster {cid} testing (isolated)\")\n",
    "                \n",
    "                if not use_frozen:\n",
    "                    # Use live params from cluster_params or fallback to aggregated\n",
    "                    test_params = cluster_params.get(cid, averaged)\n",
    "                \n",
    "                temp_model = self.model_class(self.in_dims, self.n_classes, dropout=0.1)\n",
    "                temp_model.build_all(self.max_dim)\n",
    "                temp_model.set_weights(test_params)\n",
    "                \n",
    "                cluster_test_data = self.test_data_equal[cid]\n",
    "                metrics_equal = {}\n",
    "                \n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    if task in cluster_test_data:\n",
    "                        X_test, y_test = cluster_test_data[task]\n",
    "                        #  FIX: Use model(X, task=task) instead of getattr\n",
    "                        y_pred = temp_model(X_test, task=task, training=False)\n",
    "                        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                        acc = accuracy_score(y_test, y_pred_classes)\n",
    "                        metrics_equal[f'{task}_accuracy'] = float(acc)\n",
    "                \n",
    "                test_results['equal'][cid] = metrics_equal\n",
    "                self.cluster_test_accuracies_by_round['equal'].setdefault(cid, []).append(metrics_equal)\n",
    "        \n",
    "        # Test on DIRICHLET split per-cluster data\n",
    "        if self.test_data_dirichlet is not None and server_round > 0:\n",
    "            for cid in range(3):  # Always test all 3 clusters\n",
    "                if cid not in self.test_data_dirichlet:\n",
    "                    continue\n",
    "                \n",
    "                # CRITICAL FIX: Use frozen parameters for Cluster 0 during isolation (participation_fraction == 0)\n",
    "                use_frozen = False\n",
    "                if (cid == self.compromised_cluster and \n",
    "                    self.compromise_detected_round is not None and\n",
    "                    self._frozen_cluster_params is not None):\n",
    "                    # Check if cluster is isolated (participation_fraction == 0)\n",
    "                    rounds_since_detection = server_round - self.compromise_detected_round\n",
    "                    participation_fraction = self._get_participation_fraction(rounds_since_detection)\n",
    "                    if participation_fraction == 0.0:\n",
    "                        use_frozen = True\n",
    "                        test_params = self._frozen_cluster_params  # Use frozen params\n",
    "                \n",
    "                if not use_frozen:\n",
    "                    # Use live params from cluster_params or fallback to aggregated\n",
    "                    test_params = cluster_params.get(cid, averaged)\n",
    "                \n",
    "                temp_model = self.model_class(self.in_dims, self.n_classes, dropout=0.1)\n",
    "                temp_model.build_all(self.max_dim)\n",
    "                temp_model.set_weights(test_params)\n",
    "                \n",
    "                cluster_test_data = self.test_data_dirichlet[cid]\n",
    "                metrics_dirichlet = {}\n",
    "                \n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    if task in cluster_test_data:\n",
    "                        X_test, y_test = cluster_test_data[task]\n",
    "                        # FIX: Use model(X, task=task) instead of getattr\n",
    "                        y_pred = temp_model(X_test, task=task, training=False)\n",
    "                        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "                        acc = accuracy_score(y_test, y_pred_classes)\n",
    "                        metrics_dirichlet[f'{task}_accuracy'] = float(acc)\n",
    "                \n",
    "                test_results['dirichlet'][cid] = metrics_dirichlet\n",
    "                self.cluster_test_accuracies_by_round['dirichlet'].setdefault(cid, []).append(metrics_dirichlet)\n",
    "        \n",
    "        # Print test results\n",
    "        if server_round % 10 == 0 or server_round <= 5 or (self.ch_compromised and server_round >= self.compromise_round):\n",
    "            print(f\"\\n  📊 [Round {server_round}] Test Results:\")\n",
    "            for split_type in ['equal', 'dirichlet']:\n",
    "                if split_type in test_results and test_results[split_type]:\n",
    "                    print(f\"     {split_type.upper()} Split:\")\n",
    "                    for cid, metrics in test_results[split_type].items():\n",
    "                        traffic_acc = metrics.get('traffic_accuracy', 0)\n",
    "                        duration_acc = metrics.get('duration_accuracy', 0)\n",
    "                        bandwidth_acc = metrics.get('bandwidth_accuracy', 0)\n",
    "                        status = \"❄️ FROZEN\" if (cid == self.compromised_cluster and \n",
    "                                                  self.recovery_phase == 'detection' and\n",
    "                                                  self._frozen_cluster_params is not None) else \"\"\n",
    "                        print(f\"       Cluster {cid} {status}: Traffic={traffic_acc:.4f}, Duration={duration_acc:.4f}, Bandwidth={bandwidth_acc:.4f}\")\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        accs = []\n",
    "        for split_type in ['equal', 'dirichlet']:\n",
    "            for cid, metrics in test_results[split_type].items():\n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    acc_key = f'{task}_accuracy'\n",
    "                    if acc_key in metrics:\n",
    "                        accs.append(metrics[acc_key])\n",
    "        \n",
    "        avg_acc = float(np.mean(accs)) if accs else 0.0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if self.save_dir and server_round > 0:\n",
    "            save_path = os.path.join(self.save_dir, f'model_round_{server_round}.pkl')\n",
    "            checkpoint = {\n",
    "                'round': server_round,\n",
    "                'global_params': averaged,\n",
    "                'cluster_params': cluster_params,\n",
    "                'cluster_weights': cluster_weights,\n",
    "                'participating_clusters': list(participating_clusters),\n",
    "                'recovery_phase': self.recovery_phase,\n",
    "                'ch_compromised': self.ch_compromised,\n",
    "                'test_results': test_results,\n",
    "                'avg_accuracy': avg_acc\n",
    "            }\n",
    "            with open(save_path, 'wb') as f:\n",
    "                pickle.dump(checkpoint, f)\n",
    "            \n",
    "            if server_round % 10 == 0 or server_round <= 5:\n",
    "                print(f\"   💾 Checkpoint saved: {save_path}\")\n",
    "        \n",
    "        # End round timing\n",
    "        if self.kpi_tracker:\n",
    "            # Prepare accuracies dict for KPI tracker\n",
    "            kpi_accuracies = {\n",
    "                'global': avg_acc,\n",
    "            }\n",
    "            # Add per-task accuracies (average across equal and dirichlet)\n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                task_accs = []\n",
    "                for split_type in ['equal', 'dirichlet']:\n",
    "                    for cid, metrics in test_results.get(split_type, {}).items():\n",
    "                        if f'{task}_accuracy' in metrics:\n",
    "                            task_accs.append(metrics[f'{task}_accuracy'])\n",
    "                if task_accs:\n",
    "                    kpi_accuracies[task] = float(np.mean(task_accs))\n",
    "            \n",
    "            # Add per-cluster accuracies (average of equal split)\n",
    "            for cid in range(3):\n",
    "                if cid in test_results.get('equal', {}):\n",
    "                    cluster_metrics = test_results['equal'][cid]\n",
    "                    cluster_avg = np.mean([cluster_metrics.get(f'{task}_accuracy', 0) \n",
    "                                          for task in ['traffic', 'duration', 'bandwidth']])\n",
    "                    kpi_accuracies[f'cluster_{cid}'] = float(cluster_avg)\n",
    "            \n",
    "            # Determine phase\n",
    "            kpi_phase = 'normal'\n",
    "            if self.ch_compromised and self.compromise_detected_round is not None:\n",
    "                rounds_since = server_round - self.compromise_detected_round\n",
    "                if rounds_since >= 0 and rounds_since < self.detection_rounds:\n",
    "                    kpi_phase = 'attack'  # Detection/isolation phase\n",
    "                elif rounds_since >= self.detection_rounds:\n",
    "                    kpi_phase = 'recovery'  # Continuity/recovery phase\n",
    "            \n",
    "            # Count participating clients per cluster\n",
    "            participating_dict = {}\n",
    "            for _, _, cid, _ in triples:\n",
    "                participating_dict[cid] = participating_dict.get(cid, 0) + 1\n",
    "            \n",
    "            self.kpi_tracker.end_round(\n",
    "                round_num=server_round,\n",
    "                accuracies=kpi_accuracies,\n",
    "                phase=kpi_phase,\n",
    "                participating_clients=participating_dict\n",
    "            )\n",
    "        \n",
    "        return aggregated_params, {'accuracy': avg_acc}\n",
    "\n",
    "print(\" IntegratedHierarchicalCHStrategy defined (FIXED - Cluster 0 frozen during D&R-E)\")\n",
    "print(\"   Features: Hierarchical aggregation + CH compromise + Integrated testing + Frozen params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:21:37,158\tINFO worker.py:1771 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ray initialized for convergence scenario\n",
      "\n",
      "🔧 Building client partitions...\n",
      " Client partitioning complete (equal split):\n",
      "   Total clients: 600\n",
      "   Clusters: 3\n",
      "   Clients per cluster: 200\n",
      "   Sample sizes: min=16, max=150, avg=16.7\n",
      "   Cluster 0: 200 clients, 3333 samples\n",
      "   Cluster 1: 200 clients, 3333 samples\n",
      "   Cluster 2: 200 clients, 3334 samples\n",
      "\n",
      "🔧 Creating per-cluster test data partitions...\n",
      " Created EQUAL split per-cluster test data:\n",
      "   Cluster 0: 833 samples per task\n",
      "   Cluster 1: 833 samples per task\n",
      "   Cluster 2: 834 samples per task\n",
      "Created DIRICHLET split per-cluster test data (alpha=0.4):\n",
      "   Cluster 0: 737 samples per task\n",
      "      Class distribution: {np.int64(0): np.int64(71), np.int64(1): np.int64(1), np.int64(2): np.int64(459), np.int64(3): np.int64(204), np.int64(4): np.int64(2)}\n",
      "   Cluster 1: 1011 samples per task\n",
      "      Class distribution: {np.int64(0): np.int64(421), np.int64(1): np.int64(507), np.int64(2): np.int64(7), np.int64(3): np.int64(76)}\n",
      "   Cluster 2: 752 samples per task\n",
      "      Class distribution: {np.int64(0): np.int64(9), np.int64(1): np.int64(1), np.int64(2): np.int64(28), np.int64(3): np.int64(236), np.int64(4): np.int64(478)}\n",
      "\n",
      "Initializing KPI Tracker...\n",
      "   Measuring inference latency...\n",
      " KPI Tracker ready\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init(num_cpus=12, include_dashboard=False, ignore_reinit_error=True)\n",
    "    print(\" Ray initialized for convergence scenario\")\n",
    "\n",
    "# Build client partitions\n",
    "print(\"\\n🔧 Building client partitions...\")\n",
    "clients_convergence, _ = build_client_partitions_hierarchical(verbose=True)\n",
    "\n",
    "# Model dimensions\n",
    "max_dim = max(X_traffic_train.shape[1], X_duration_train.shape[1], X_bandwidth_train.shape[1])\n",
    "\n",
    "in_dims = {\n",
    "    'traffic': max_dim,\n",
    "    'duration': max_dim,\n",
    "    'bandwidth': max_dim\n",
    "}\n",
    "\n",
    "n_classes = {\n",
    "    'traffic': len(np.unique(y_traf_train)),\n",
    "    'duration': len(np.unique(y_dur_train)),\n",
    "    'bandwidth': len(np.unique(y_bw_train))\n",
    "}\n",
    "\n",
    "# Test data (global)\n",
    "test_data_global = {\n",
    "    'traffic': (X_traffic_test, y_traf_test),\n",
    "    'duration': (X_duration_test, y_dur_test),\n",
    "    'bandwidth': (X_bandwidth_test, y_bw_test)\n",
    "}\n",
    "\n",
    "# Create per-cluster test data partitions\n",
    "print(\"\\n🔧 Creating per-cluster test data partitions...\")\n",
    "cluster_test_equal_convergence = create_per_cluster_test_data_equal(test_data_global, n_clusters=3)\n",
    "cluster_test_dirichlet_convergence = create_per_cluster_test_data_dirichlet(test_data_global, n_clusters=3, alpha=0.4, seed=42)\n",
    "\n",
    "# Client function (Flower 2.0+ Context API)\n",
    "def client_fn_convergence(context: fl.common.Context) -> fl.client.Client:\n",
    "    \"\"\"Create Flower client with proper Context API\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    client_idx = hash(context.node_id) % len(clients_convergence)\n",
    "    client_obj = clients_convergence[client_idx]\n",
    "    \n",
    "    model = FedMTLModel(in_dims, n_classes, dropout=0.1)\n",
    "    model.build_all(max_dim)\n",
    "    \n",
    "    numpy_client = ClusterAwareClient(\n",
    "        model=model,\n",
    "        client_data=client_obj.ds,\n",
    "        cfg=CFG,\n",
    "        cluster_id=client_obj.cluster_id,\n",
    "        client_id=client_obj.client_id\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Convert NumPyClient to Client\n",
    "    return numpy_client.to_client()\n",
    "\n",
    "# Create global model\n",
    "global_model_convergence = FedMTLModel(in_dims, n_classes, dropout=0.1)\n",
    "global_model_convergence.build_all(max_dim)\n",
    "\n",
    "# Aggregation function\n",
    "def aggregate_metrics(metrics):\n",
    "    aggregated = {}\n",
    "    for num_examples, client_metrics in metrics:\n",
    "        for metric_name, metric_value in client_metrics.items():\n",
    "            if metric_name not in aggregated:\n",
    "                aggregated[metric_name] = []\n",
    "            aggregated[metric_name].append(metric_value)\n",
    "    for metric_name in aggregated:\n",
    "        aggregated[metric_name] = np.mean(aggregated[metric_name])\n",
    "    return aggregated\n",
    "\n",
    "#  Create KPI Tracker for Convergence\n",
    "print(\"\\nInitializing KPI Tracker...\")\n",
    "kpi_tracker_convergence = ComprehensiveKPITracker(\n",
    "    cfg=CFG,\n",
    "    model=global_model_convergence,\n",
    "    n_clusters=3,\n",
    "    clients_per_cluster=200\n",
    ")\n",
    "\n",
    "# Measure inference latency once\n",
    "print(\"   Measuring inference latency...\")\n",
    "kpi_tracker_convergence.measure_inference_latency(X_traffic_test[:100])\n",
    "print(f\" KPI Tracker ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=125, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CONVERGENCE SCENARIO (125 rounds)\n",
      " Training from scratch: rounds 1-125\n",
      " CH0 compromise at round 111\n",
      " D&R-E: rounds 112-118\n",
      " Continuity: rounds 119-121 (30%/70%/100%)\n",
      " Stabilization: rounds 122-125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 01:21:42,984\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'object_store_memory': 2147483648.0, 'node:127.0.0.1': 1.0, 'node:__internal_head__': 1.0, 'memory': 8296279245.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🌍 [Round 1] Global aggregation at Cluster 1\n",
      "     Participating clusters: [2, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  📊 [Round 1] Test Results:\n",
      "     EQUAL Split:\n",
      "       Cluster 0 : Traffic=0.3277, Duration=0.1164, Bandwidth=0.1609\n",
      "       Cluster 1 : Traffic=0.3085, Duration=0.1309, Bandwidth=0.1477\n",
      "       Cluster 2 : Traffic=0.2866, Duration=0.1523, Bandwidth=0.1739\n",
      "     DIRICHLET Split:\n",
      "       Cluster 0 : Traffic=0.7191, Duration=0.0579, Bandwidth=0.1050\n",
      "       Cluster 1 : Traffic=0.0752, Duration=0.0778, Bandwidth=0.1944\n",
      "       Cluster 2 : Traffic=0.2181, Duration=0.2702, Bandwidth=0.2963\n",
      "   💾 Checkpoint saved: trained_models/convergence_integrated/model_round_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n"
     ]
    }
   ],
   "source": [
    "#Multi_Cluster Training\n",
    "strategy_convergence = IntegratedHierarchicalCHStrategy(\n",
    "    test_data_equal=cluster_test_equal_convergence,\n",
    "    test_data_dirichlet=cluster_test_dirichlet_convergence,\n",
    "    model_class=FedMTLModel,\n",
    "    in_dims=in_dims,\n",
    "    n_classes=n_classes,\n",
    "    max_dim=max_dim,\n",
    "    compromise_round=111,  # Compromise at round 111\n",
    "    compromised_cluster=0,\n",
    "    global_aggregator_cluster=1,  #  Cluster 1 is the global aggregator\n",
    "    client_list=clients_convergence,\n",
    "    detection_rounds=7,\n",
    "    continuity_rounds=3,\n",
    "    alpha_energy=0.6,\n",
    "    beta_rssi=0.4,\n",
    "    kpi_tracker=kpi_tracker_convergence,  #  Pass KPI tracker\n",
    "    save_dir='trained_models/convergence_integrated',  #  Save checkpoints\n",
    "    fraction_fit=CFG['client_frac'],\n",
    "    fraction_evaluate=CFG['client_frac'],\n",
    "    min_fit_clients=10,\n",
    "    min_available_clients=len(clients_convergence),\n",
    "    min_evaluate_clients=10,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(global_model_convergence.get_weights()),\n",
    "    fit_metrics_aggregation_fn=aggregate_metrics,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_metrics,\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "\n",
    "print(f\" CONVERGENCE SCENARIO (125 rounds)\")\n",
    "\n",
    "print(f\" Training from scratch: rounds 1-125\")\n",
    "print(f\" CH0 compromise at round 111\")\n",
    "print(f\" D&R-E: rounds 112-118\")\n",
    "print(f\" Continuity: rounds 119-121 (30%/70%/100%)\")\n",
    "print(f\" Stabilization: rounds 122-125\")\n",
    "\n",
    "\n",
    "history_convergence = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_convergence,\n",
    "    num_clients=len(clients_convergence),\n",
    "    config=fl.server.ServerConfig(num_rounds=125),\n",
    "    strategy=strategy_convergence,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\" CONVERGENCE COMPLETE\")\n",
    "print(f\" Total rounds: 125\")\n",
    "print(f\" Results: strategy_convergence.cluster_test_accuracies_by_round\")\n",
    "\n",
    "# Recovery log\n",
    "if strategy_convergence.recovery_log:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"RECOVERY LOG\")\n",
    "    print(f\"{'='*40}\")\n",
    "    for event in strategy_convergence.recovery_log:\n",
    "        print(f\"Round {event['round']:3d}: {event['event']}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "\n",
    "# Print KPI Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE KPI SUMMARY - CONVERGENCE SCENARIO\")\n",
    "print(f\"{'='*80}\")\n",
    "kpi_tracker_convergence.print_summary()\n",
    "\n",
    "# Save KPI Data Locally\n",
    "import json\n",
    "kpi_save_dir = 'trained_models/convergence_integrated'\n",
    "os.makedirs(kpi_save_dir, exist_ok=True)\n",
    "\n",
    "# Save KPI summary as JSON\n",
    "kpi_summary = {\n",
    "    'experiment_type': 'convergence_125_rounds',\n",
    "    'compromise_at_round': 111,\n",
    "    'round_durations': kpi_tracker_convergence.kpis['round_durations'],\n",
    "    'cumulative_time': kpi_tracker_convergence.kpis['cumulative_time'],\n",
    "    'inference_latency_ms': kpi_tracker_convergence.kpis['inference_latency_ms'],\n",
    "    'inference_latency_std_ms': kpi_tracker_convergence.kpis['inference_latency_std_ms'],\n",
    "    'model_size_bytes': kpi_tracker_convergence.kpis['model_parameter_size_bytes'],\n",
    "    'computational_load': kpi_tracker_convergence.kpis['computational_load'],\n",
    "}\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'experiments': {}\n",
    "}\n",
    "\n",
    "\n",
    "kpi_json_path = os.path.join(kpi_save_dir, 'kpi_summary_convergence.json')\n",
    "with open(kpi_json_path, 'w') as f:\n",
    "    json.dump(kpi_summary, f, indent=2)\n",
    "print(f\"\\n💾 KPI summary saved to: {kpi_json_path}\")\n",
    "\n",
    "# Save full KPI tracker as pickle\n",
    "kpi_pickle_path = os.path.join(kpi_save_dir, 'kpi_tracker_convergence.pkl')\n",
    "with open(kpi_pickle_path, 'wb') as f:\n",
    "    pickle.dump(kpi_tracker_convergence, f)\n",
    "print(f\"💾 Full KPI tracker saved to: {kpi_pickle_path}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transient Scenario: 30 Rounds with CH Compromise\n",
    "\n",
    "**Timeline:**\n",
    "- Rounds 1-10: Normal training\n",
    "- Round 11: CH0 compromise\n",
    "- Rounds 12-18: D&R-E phase (cluster 0 offline)\n",
    "- Round 19: Continuity begins (30%)\n",
    "- Round 20: Increased participation (70%)\n",
    "- Round 21: Full restoration (100%)\n",
    "- Rounds 22-30: Re-stabilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSIENT: Integrated Training + Testing (30 rounds)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Shutdown and reinitialize Ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"🔄 Ray shutdown for fresh start\")\n",
    "\n",
    "ray.init(num_cpus=12, include_dashboard=False, ignore_reinit_error=True)\n",
    "print(\" Ray reinitialized for transient scenario\")\n",
    "\n",
    "# Build client partitions\n",
    "print(\"\\n🔧 Building client partitions for transient...\")\n",
    "clients_transient, _ = build_client_partitions_hierarchical(verbose=True)\n",
    "\n",
    "# Test data (global)\n",
    "test_data_global_transient = {\n",
    "    'traffic': (X_traffic_test, y_traf_test),\n",
    "    'duration': (X_duration_test, y_dur_test),\n",
    "    'bandwidth': (X_bandwidth_test, y_bw_test)\n",
    "}\n",
    "\n",
    "# Create per-cluster test data partitions\n",
    "print(\"\\n🔧 Creating per-cluster test data partitions for transient...\")\n",
    "cluster_test_equal_transient = create_per_cluster_test_data_equal(test_data_global_transient, n_clusters=3)\n",
    "cluster_test_dirichlet_transient = create_per_cluster_test_data_dirichlet(test_data_global_transient, n_clusters=3, alpha=0.4, seed=42)\n",
    "\n",
    "# Client function (Flower 2.0+ Context API)\n",
    "def client_fn_transient(context: fl.common.Context) -> fl.client.Client:\n",
    "    \"\"\"Create Flower client with proper Context API\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    client_idx = hash(context.node_id) % len(clients_transient)\n",
    "    client_obj = clients_transient[client_idx]\n",
    "    \n",
    "    model = FedMTLModel(in_dims, n_classes, dropout=0.1)\n",
    "    model.build_all(max_dim)\n",
    "    \n",
    "    numpy_client = ClusterAwareClient(\n",
    "        model=model,\n",
    "        client_data=client_obj.ds,\n",
    "        cfg=CFG,\n",
    "        cluster_id=client_obj.cluster_id,\n",
    "        client_id=client_obj.client_id\n",
    "    )\n",
    "    \n",
    "    # CRITICAL: Convert NumPyClient to Client\n",
    "    return numpy_client.to_client()\n",
    "\n",
    "# Create global model\n",
    "global_model_transient = FedMTLModel(in_dims, n_classes, dropout=0.1)\n",
    "global_model_transient.build_all(max_dim)\n",
    "\n",
    "#  Create KPI Tracker for Transient\n",
    "print(\"\\n📊 Initializing KPI Tracker...\")\n",
    "kpi_tracker_transient = ComprehensiveKPITracker(\n",
    "    cfg=CFG,\n",
    "    model=global_model_transient,\n",
    "    n_clusters=3,\n",
    "    clients_per_cluster=200\n",
    ")\n",
    "\n",
    "# Measure inference latency once\n",
    "print(\"   Measuring inference latency...\")\n",
    "kpi_tracker_transient.measure_inference_latency(X_traffic_test[:100])\n",
    "print(f\"    KPI Tracker ready\")\n",
    "\n",
    "# Create strategy\n",
    "strategy_transient = IntegratedHierarchicalCHStrategy(\n",
    "    test_data_equal=cluster_test_equal_transient,\n",
    "    test_data_dirichlet=cluster_test_dirichlet_transient,\n",
    "    model_class=FedMTLModel,\n",
    "    in_dims=in_dims,\n",
    "    n_classes=n_classes,\n",
    "    max_dim=max_dim,\n",
    "    compromise_round=11,  # Compromise at round 11\n",
    "    compromised_cluster=0,\n",
    "    global_aggregator_cluster=1,  #  Cluster 1 is the global aggregator\n",
    "    client_list=clients_transient,\n",
    "    detection_rounds=7,\n",
    "    continuity_rounds=3,\n",
    "    alpha_energy=0.6,\n",
    "    beta_rssi=0.4,\n",
    "    kpi_tracker=kpi_tracker_transient,  # Pass KPI tracker\n",
    "    save_dir='trained_models/transient_integrated',  #  Save checkpoints\n",
    "    fraction_fit=CFG['client_frac'],\n",
    "    fraction_evaluate=CFG['client_frac'],\n",
    "    min_fit_clients=10,\n",
    "    min_available_clients=len(clients_transient),\n",
    "    min_evaluate_clients=10,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(global_model_transient.get_weights()),\n",
    "    fit_metrics_aggregation_fn=aggregate_metrics,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_metrics,\n",
    ")\n",
    "\n",
    "# Run simulation\n",
    "print(f\" TRANSIENT SCENARIO (30 rounds)\")\n",
    "\n",
    "print(f\" Training from scratch: rounds 1-30\")\n",
    "print(f\" CH0 compromise at round 11\")\n",
    "print(f\"D&R-E: rounds 12-18\")\n",
    "print(f\"Continuity: rounds 19-21 (30%/70%/100%)\")\n",
    "print(f\"Stabilization: rounds 22-30\")\n",
    "\n",
    "\n",
    "history_transient = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_transient,\n",
    "    num_clients=len(clients_transient),\n",
    "    config=fl.server.ServerConfig(num_rounds=30),\n",
    "    strategy=strategy_transient,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\" TRANSIENT COMPLETE\")\n",
    "\n",
    "print(f\" Total rounds: 30\")\n",
    "print(f\" Results: strategy_transient.cluster_test_accuracies_by_round\")\n",
    "\n",
    "# Recovery log\n",
    "if strategy_transient.recovery_log:\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"RECOVERY LOG\")\n",
    "    print(f\"{'='*40}\")\n",
    "    for event in strategy_transient.recovery_log:\n",
    "        print(f\"Round {event['round']:3d}: {event['event']}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "\n",
    "# Print KPI Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE KPI SUMMARY - TRANSIENT SCENARIO\")\n",
    "print(f\"{'='*80}\")\n",
    "kpi_tracker_transient.print_summary()\n",
    "\n",
    "#  Save KPI Data Locally\n",
    "kpi_save_dir_transient = 'trained_models/transient_integrated'\n",
    "os.makedirs(kpi_save_dir_transient, exist_ok=True)\n",
    "\n",
    "# Save KPI summary as JSON\n",
    "kpi_summary_transient = {\n",
    "    'scenario': 'transient',\n",
    "    'total_rounds': 30,\n",
    "    'compromise_round': 11,\n",
    "    'round_durations': kpi_tracker_transient.kpis['round_durations'],\n",
    "    'cumulative_time': kpi_tracker_transient.kpis['cumulative_time'],\n",
    "    'inference_latency_ms': kpi_tracker_transient.kpis['inference_latency_ms'],\n",
    "    'inference_latency_std_ms': kpi_tracker_transient.kpis['inference_latency_std_ms'],\n",
    "    'model_size_bytes': kpi_tracker_transient.kpis['model_parameter_size_bytes'],\n",
    "    'computational_load': kpi_tracker_transient.kpis['computational_load'],\n",
    "    'recovery_log': strategy_transient.recovery_log,\n",
    "}\n",
    "\n",
    "kpi_json_path_transient = os.path.join(kpi_save_dir_transient, 'kpi_summary_transient.json')\n",
    "with open(kpi_json_path_transient, 'w') as f:\n",
    "    json.dump(kpi_summary_transient, f, indent=2)\n",
    "print(f\"\\nKPI summary saved to: {kpi_json_path_transient}\")\n",
    "\n",
    "# Save full KPI tracker as pickle\n",
    "kpi_pickle_path_transient = os.path.join(kpi_save_dir_transient, 'kpi_tracker_transient.pkl')\n",
    "with open(kpi_pickle_path_transient, 'wb') as f:\n",
    "    pickle.dump(kpi_tracker_transient, f)\n",
    "print(f\"Full KPI tracker saved to: {kpi_pickle_path_transient}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_save_dir_transient = 'trained_models/transient_integrated'\n",
    "os.makedirs(kpi_save_dir_transient, exist_ok=True)\n",
    "\n",
    "# Save KPI summary as JSON\n",
    "import json\n",
    "import pickle\n",
    "kpi_summary_transient = {\n",
    "    'scenario': 'transient',\n",
    "    'total_rounds': 30,\n",
    "    'compromise_round': 11,\n",
    "    'round_durations': kpi_tracker_transient.kpis['round_durations'],\n",
    "    'cumulative_time': kpi_tracker_transient.kpis['cumulative_time'],\n",
    "    'inference_latency_ms': kpi_tracker_transient.kpis['inference_latency_ms'],\n",
    "    'inference_latency_std_ms': kpi_tracker_transient.kpis['inference_latency_std_ms'],\n",
    "    'model_size_bytes': kpi_tracker_transient.kpis['model_parameter_size_bytes'],\n",
    "    'computational_load': kpi_tracker_transient.kpis['computational_load'],\n",
    "    'recovery_log': strategy_transient.recovery_log,\n",
    "}\n",
    "\n",
    "kpi_json_path_transient = os.path.join(kpi_save_dir_transient, 'kpi_summary_transient.json')\n",
    "with open(kpi_json_path_transient, 'w') as f:\n",
    "    json.dump(kpi_summary_transient, f, indent=2)\n",
    "print(f\"\\n KPI summary saved to: {kpi_json_path_transient}\")\n",
    "\n",
    "# Save full KPI tracker as pickle\n",
    "kpi_pickle_path_transient = os.path.join(kpi_save_dir_transient, 'kpi_tracker_transient.pkl')\n",
    "with open(kpi_pickle_path_transient, 'wb') as f:\n",
    "    pickle.dump(kpi_tracker_transient, f)\n",
    "print(f\" Full KPI tracker saved to: {kpi_pickle_path_transient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_integrated_experiment_results_fixed(\n",
    "    strategy_convergence,\n",
    "    strategy_transient,\n",
    "    history_convergence,\n",
    "    history_transient,\n",
    "    timestamp\n",
    "):\n",
    "    \"\"\"\n",
    "    Save all integrated experiment results (training + testing)\n",
    "    \n",
    "     FIXED: Now saves BOTH convergence and transient data properly\n",
    "    \"\"\"\n",
    "    \n",
    "    results_package = {\n",
    "        'timestamp': timestamp,\n",
    "        'cfg': CFG.copy(),\n",
    "        'model_metadata': {\n",
    "            'n_classes': n_classes,\n",
    "            'in_dims': in_dims,\n",
    "            'max_dim': max_dim,\n",
    "            'training_rounds_convergence': 125,\n",
    "            'training_rounds_transient': 30\n",
    "        },\n",
    "        'convergence_experiment': {},\n",
    "        'transient_experiment': {}\n",
    "    }\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CONVERGENCE EXPERIMENT (125 rounds)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nSaving convergence experiment results...\")\n",
    "    \n",
    "    if hasattr(strategy_convergence, 'cluster_test_accuracies_by_round'):\n",
    "        test_data = strategy_convergence.cluster_test_accuracies_by_round\n",
    "        \n",
    "        # Data format: {'equal': {cid: [metrics]}, 'dirichlet': {cid: [metrics]}}\n",
    "        per_cluster_equal = {}\n",
    "        per_cluster_dirichlet = {}\n",
    "        \n",
    "        if 'equal' in test_data:\n",
    "            for cid, metrics_list in test_data['equal'].items():\n",
    "                per_cluster_equal[cid] = []\n",
    "                for round_idx, metrics in enumerate(metrics_list, start=1):\n",
    "                    entry = {\n",
    "                        'round': round_idx,\n",
    "                        **metrics  # traffic_accuracy, duration_accuracy, bandwidth_accuracy\n",
    "                    }\n",
    "                    per_cluster_equal[cid].append(entry)\n",
    "        \n",
    "        if 'dirichlet' in test_data:\n",
    "            for cid, metrics_list in test_data['dirichlet'].items():\n",
    "                per_cluster_dirichlet[cid] = []\n",
    "                for round_idx, metrics in enumerate(metrics_list, start=1):\n",
    "                    entry = {\n",
    "                        'round': round_idx,\n",
    "                        **metrics\n",
    "                    }\n",
    "                    per_cluster_dirichlet[cid].append(entry)\n",
    "        \n",
    "        results_package['convergence_experiment']['per_cluster_equal'] = per_cluster_equal\n",
    "        results_package['convergence_experiment']['per_cluster_dirichlet'] = per_cluster_dirichlet\n",
    "        \n",
    "        print(f\"  ✅ Saved per-cluster results (equal): {len(per_cluster_equal)} clusters\")\n",
    "        print(f\"  ✅ Saved per-cluster results (dirichlet): {len(per_cluster_dirichlet)} clusters\")\n",
    "        if per_cluster_equal:\n",
    "            total_rounds = len(list(per_cluster_equal.values())[0])\n",
    "            print(f\"  ✅ Total rounds: {total_rounds}\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Warning: No convergence test data found in strategy_convergence\")\n",
    "    \n",
    "    # Recovery log\n",
    "    if hasattr(strategy_convergence, 'recovery_log'):\n",
    "        results_package['convergence_experiment']['recovery_log'] = strategy_convergence.recovery_log\n",
    "        print(f\"  ✅ Saved recovery log: {len(strategy_convergence.recovery_log)} events\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TRANSIENT EXPERIMENT (30 rounds)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\nSaving transient experiment results...\")\n",
    "    \n",
    "    if hasattr(strategy_transient, 'cluster_test_accuracies_by_round'):\n",
    "        test_data = strategy_transient.cluster_test_accuracies_by_round\n",
    "        \n",
    "        per_cluster_equal = {}\n",
    "        per_cluster_dirichlet = {}\n",
    "        \n",
    "        if 'equal' in test_data:\n",
    "            for cid, metrics_list in test_data['equal'].items():\n",
    "                per_cluster_equal[cid] = []\n",
    "                for round_idx, metrics in enumerate(metrics_list, start=1):\n",
    "                    entry = {\n",
    "                        'round': round_idx,\n",
    "                        **metrics\n",
    "                    }\n",
    "                    per_cluster_equal[cid].append(entry)\n",
    "        \n",
    "        if 'dirichlet' in test_data:\n",
    "            for cid, metrics_list in test_data['dirichlet'].items():\n",
    "                per_cluster_dirichlet[cid] = []\n",
    "                for round_idx, metrics in enumerate(metrics_list, start=1):\n",
    "                    entry = {\n",
    "                        'round': round_idx,\n",
    "                        **metrics\n",
    "                    }\n",
    "                    per_cluster_dirichlet[cid].append(entry)\n",
    "        \n",
    "        results_package['transient_experiment']['per_cluster_equal'] = per_cluster_equal\n",
    "        results_package['transient_experiment']['per_cluster_dirichlet'] = per_cluster_dirichlet\n",
    "        \n",
    "        print(f\"  ✅ Saved per-cluster results (equal): {len(per_cluster_equal)} clusters\")\n",
    "        print(f\"  ✅ Saved per-cluster results (dirichlet): {len(per_cluster_dirichlet)} clusters\")\n",
    "        if per_cluster_equal:\n",
    "            total_rounds = len(list(per_cluster_equal.values())[0])\n",
    "            print(f\"  ✅ Total rounds: {total_rounds}\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Warning: No transient test data found in strategy_transient\")\n",
    "    \n",
    "    # Recovery log\n",
    "    if hasattr(strategy_transient, 'recovery_log'):\n",
    "        results_package['transient_experiment']['recovery_log'] = strategy_transient.recovery_log\n",
    "        print(f\"  ✅ Saved recovery log: {len(strategy_transient.recovery_log)} events\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SAVE TO DISK\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Save pickle\n",
    "    results_file = os.path.join(results_dir, f'integrated_results_{timestamp}.pkl')\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(results_package, f)\n",
    "    \n",
    "    # Save JSON summary\n",
    "    summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'experiments': {\n",
    "            'convergence': {\n",
    "                'rounds': 125,\n",
    "                'compromise_round': 111,\n",
    "                'clusters': list(results_package['convergence_experiment']['per_cluster_equal'].keys()) if results_package['convergence_experiment'].get('per_cluster_equal') else []\n",
    "            },\n",
    "            'transient': {\n",
    "                'rounds': 30,\n",
    "                'compromise_round': 11,\n",
    "                'clusters': list(results_package['transient_experiment']['per_cluster_equal'].keys()) if results_package['transient_experiment'].get('per_cluster_equal') else []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = os.path.join(results_dir, f'integrated_summary_{timestamp}.json')\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    file_size = os.path.getsize(results_file) / (1024 * 1024)  # MB\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ ALL RESULTS SAVED SUCCESSFULLY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📁 Results file: {results_file}\")\n",
    "    print(f\"📄 Summary file: {summary_file}\")\n",
    "    print(f\"💾 File size: {file_size:.2f} MB\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return results_file\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RE-SAVE WITH FIXED FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "# Check if strategies exist\n",
    "if 'strategy_convergence' in globals() and 'strategy_transient' in globals():\n",
    "    print(\"\\n✅ Both strategies found - re-saving results...\")\n",
    "    \n",
    "    results_dir = 'experiment_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    saved_file = save_integrated_experiment_results_fixed(\n",
    "        strategy_convergence=strategy_convergence,\n",
    "        strategy_transient=strategy_transient,\n",
    "        history_convergence=history_convergence,\n",
    "        history_transient=history_transient,\n",
    "        timestamp=timestamp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ New results file created: {saved_file}\")\n",
    "    print(\"   You can now use this file for visualization!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠️  ERROR: Strategies not found in memory\")\n",
    "    print(\"   You need to:\")\n",
    "    print(\"   1. Run the convergence training cell (125 rounds)\")\n",
    "    print(\"   2. Run the transient training cell (30 rounds)\")\n",
    "    print(\"   3. Then run this save cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "results_dir = 'experiment_results'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "saved_file = save_integrated_experiment_results(\n",
    "    strategy_convergence=strategy_convergence,\n",
    "    strategy_transient=strategy_transient,\n",
    "    history_convergence=history_convergence,\n",
    "    history_transient=history_transient,\n",
    "    timestamp=timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(saved_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(\"Convergence clusters (equal):\", list(results['convergence_experiment']['per_cluster_equal'].keys()))\n",
    "print(\"Sample Cluster 0 data:\", results['convergence_experiment']['per_cluster_equal'][0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify\n",
    "import pickle\n",
    "with open(saved_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Check structure\n",
    "print(\"Convergence clusters:\", list(results['convergence_experiment']['per_cluster_equal'].keys()))\n",
    "print(\"Sample cluster 1 data:\", results['convergence_experiment']['per_cluster_equal'][1][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD SAVED RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING SAVED INTEGRATED EXPERIMENT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Update this with your actual saved file path\n",
    "results_file = 'experiment_results/integrated_results_20251205_091101.pkl'  # New timestamp\n",
    "\n",
    "with open(results_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded results from: {results_file}\")\n",
    "print(f\"   Timestamp: {results['timestamp']}\")\n",
    "print(f\"   Convergence rounds: {results['model_metadata']['training_rounds_convergence']}\")\n",
    "print(f\"   Transient rounds: {results['model_metadata']['training_rounds_transient']}\")\n",
    "\n",
    "# Extract data with error handling\n",
    "conv_equal = results['convergence_experiment'].get('per_cluster_equal', {})\n",
    "conv_dirichlet = results['convergence_experiment'].get('per_cluster_dirichlet', {})\n",
    "trans_equal = results['transient_experiment'].get('per_cluster_equal', {})\n",
    "trans_dirichlet = results['transient_experiment'].get('per_cluster_dirichlet', {})\n",
    "\n",
    "print(f\"\\nData structure check:\")\n",
    "print(f\"   Convergence equal: {type(conv_equal)} - {len(conv_equal)} clusters\")\n",
    "print(f\"   Convergence dirichlet: {type(conv_dirichlet)} - {len(conv_dirichlet)} clusters\")\n",
    "print(f\"   Transient equal: {type(trans_equal)} - {len(trans_equal)} clusters\")\n",
    "print(f\"   Transient dirichlet: {type(trans_dirichlet)} - {len(trans_dirichlet)} clusters\")\n",
    "\n",
    "# Check if convergence data is missing\n",
    "if not conv_equal or not conv_dirichlet:\n",
    "    print(\"\\nWARNING: Convergence data missing from saved file!\")\n",
    "    print(\"This is because the save function has convergence extraction commented out.\")\n",
    "    print(\"You need to:\")\n",
    "    print(\"  1. Re-run the save function with convergence data extraction enabled\")\n",
    "    print(\"  2. Or load convergence data directly from strategy_convergence object\")\n",
    "    \n",
    "    # Option: Try to load from strategy object if available\n",
    "    if 'strategy_convergence' in globals():\n",
    "        print(\"\\nAttempting to use strategy_convergence object...\")\n",
    "        conv_test_data = strategy_convergence.cluster_test_accuracies_by_round\n",
    "        if 'equal' in conv_test_data and 'dirichlet' in conv_test_data:\n",
    "            conv_equal = conv_test_data['equal']\n",
    "            conv_dirichlet = conv_test_data['dirichlet']\n",
    "            print(\"   Successfully extracted convergence data from strategy object\")\n",
    "        else:\n",
    "            print(\"   ERROR: Strategy object data structure unexpected\")\n",
    "            print(f\"   Keys found: {list(conv_test_data.keys())}\")\n",
    "    else:\n",
    "        print(\"\\nERROR: strategy_convergence object not available in globals()\")\n",
    "        print(\"Please re-run the convergence training or fix the save function.\")\n",
    "        # Exit early\n",
    "        import sys\n",
    "        sys.exit(1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR PLOTTING\n",
    "# ============================================================================\n",
    "\n",
    "def plot_cluster_performance(data, cluster_id, title, phases=None, figsize=(14, 8)):\n",
    "    \"\"\"Plot per-cluster performance with phase markers\"\"\"\n",
    "    if not data or cluster_id not in data:\n",
    "        print(f\"No data for cluster {cluster_id}\")\n",
    "        return\n",
    "    \n",
    "    cluster_data = data[cluster_id]\n",
    "    rounds = [item['round'] for item in cluster_data]\n",
    "    traffic = [item['traffic_accuracy'] for item in cluster_data]\n",
    "    duration = [item['duration_accuracy'] for item in cluster_data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in cluster_data]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    ax.plot(rounds, traffic, 'green', label='Traffic', linewidth=2, marker='o', markersize=3)\n",
    "    ax.plot(rounds, duration, 'blue', label='Duration', linewidth=2, marker='s', markersize=3)\n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2, marker='^', markersize=3)\n",
    "    \n",
    "    # Add phase markers\n",
    "    if phases:\n",
    "        for start, (end, color, label) in phases.items():\n",
    "            ax.axvspan(start, end, alpha=0.15, color=color, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=12)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(f\"  Rounds: {len(rounds)}\")\n",
    "    print(f\"  Final - Traffic: {traffic[-1]:.4f}, Duration: {duration[-1]:.4f}, Bandwidth: {bandwidth[-1]:.4f}\")\n",
    "\n",
    "def plot_overall_performance(data, title, phases=None, max_rounds=None, figsize=(16, 8)):\n",
    "    \"\"\"Plot overall performance averaging all clusters\"\"\"\n",
    "    if not data:\n",
    "        print(f\"No data available\")\n",
    "        return\n",
    "    \n",
    "    # Get all rounds\n",
    "    all_rounds = set()\n",
    "    for cluster_id in data.keys():\n",
    "        all_rounds.update([item['round'] for item in data[cluster_id]])\n",
    "    rounds = sorted(list(all_rounds))\n",
    "    \n",
    "    if max_rounds:\n",
    "        rounds = [r for r in rounds if r <= max_rounds]\n",
    "    \n",
    "    # Average across clusters\n",
    "    traffic_avg = []\n",
    "    duration_avg = []\n",
    "    bandwidth_avg = []\n",
    "    \n",
    "    for rnd in rounds:\n",
    "        traffic_vals = []\n",
    "        duration_vals = []\n",
    "        bandwidth_vals = []\n",
    "        \n",
    "        for cluster_id in data.keys():\n",
    "            cluster_data = [item for item in data[cluster_id] if item['round'] == rnd]\n",
    "            if cluster_data:\n",
    "                traffic_vals.append(cluster_data[0]['traffic_accuracy'])\n",
    "                duration_vals.append(cluster_data[0]['duration_accuracy'])\n",
    "                bandwidth_vals.append(cluster_data[0]['bandwidth_accuracy'])\n",
    "        \n",
    "        traffic_avg.append(np.mean(traffic_vals) if traffic_vals else 0)\n",
    "        duration_avg.append(np.mean(duration_vals) if duration_vals else 0)\n",
    "        bandwidth_avg.append(np.mean(bandwidth_vals) if bandwidth_vals else 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot accuracy curves\n",
    "    ax.plot(rounds, traffic_avg, 'green', label='Traffic', linewidth=2, marker='o', markersize=3)\n",
    "    ax.plot(rounds, duration_avg, 'blue', label='Duration', linewidth=2, marker='s', markersize=3)\n",
    "    ax.plot(rounds, bandwidth_avg, 'orange', label='Bandwidth', linewidth=2, marker='^', markersize=3)\n",
    "    \n",
    "    # Add phase markers\n",
    "    if phases:\n",
    "        for start, (end, color, label) in phases.items():\n",
    "            ax.axvspan(start, end, alpha=0.15, color=color, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=12)\n",
    "    ax.set_ylabel('Average Test Accuracy', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(f\"  Rounds: {len(rounds)}\")\n",
    "    print(f\"  Final - Traffic: {traffic_avg[-1]:.4f}, Duration: {duration_avg[-1]:.4f}, Bandwidth: {bandwidth_avg[-1]:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: PER-CLUSTER NORMAL TESTING (100 ROUNDS) - 6 GRAPHS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 1: PER-CLUSTER NORMAL TESTING (100 ROUNDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Equal split\n",
    "    plot_cluster_performance(\n",
    "        {cluster_id: [item for item in conv_equal[cluster_id] if item['round'] <= 100]},\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 1}: Cluster {cluster_id} - Normal Testing (Equal Split, 100 rounds)',\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Dirichlet split\n",
    "    plot_cluster_performance(\n",
    "        {cluster_id: [item for item in conv_dirichlet[cluster_id] if item['round'] <= 100]},\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 4}: Cluster {cluster_id} - Normal Testing (Dirichlet Split, 100 rounds)',\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: OVERALL MULTI-CLUSTER TESTING (100 ROUNDS) - 2 GRAPHS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: OVERALL MULTI-CLUSTER TESTING (100 ROUNDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter to 100 rounds\n",
    "conv_equal_100 = {cid: [item for item in conv_equal[cid] if item['round'] <= 100] for cid in conv_equal.keys()}\n",
    "conv_dirichlet_100 = {cid: [item for item in conv_dirichlet[cid] if item['round'] <= 100] for cid in conv_dirichlet.keys()}\n",
    "\n",
    "plot_overall_performance(\n",
    "    conv_equal_100,\n",
    "    'Graph 7: Overall Multi-Cluster Performance - Equal Split (100 rounds)',\n",
    "    max_rounds=100,\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "\n",
    "plot_overall_performance(\n",
    "    conv_dirichlet_100,\n",
    "    'Graph 8: Overall Multi-Cluster Performance - Dirichlet Split (100 rounds)',\n",
    "    max_rounds=100,\n",
    "    figsize=(16, 8)\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: PER-CLUSTER CONVERGENCE (125 ROUNDS) - 6 GRAPHS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: PER-CLUSTER CONVERGENCE (125 ROUNDS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Phase markers for convergence\n",
    "convergence_phases = {\n",
    "    111: (118, 'pink', 'D&R-E (111-118)'),\n",
    "    118: (121, 'yellow', 'Continuity (118-121)'),\n",
    "    121: (125, 'lightgreen', 'Stabilization (121-125)')\n",
    "}\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Equal split\n",
    "    plot_cluster_performance(\n",
    "        conv_equal,\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 9}: Cluster {cluster_id} - CH Compromise Convergence (Equal Split, 125 rounds)',\n",
    "        phases=convergence_phases,\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Dirichlet split\n",
    "    plot_cluster_performance(\n",
    "        conv_dirichlet,\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 12}: Cluster {cluster_id} - CH Compromise Convergence (Dirichlet Split, 125 rounds)',\n",
    "        phases=convergence_phases,\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: PER-CLUSTER TRANSIENT (30 ROUNDS) - 6 GRAPHS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"SECTION 4: PER-CLUSTER TRANSIENT (30 ROUNDS)\")\n",
    "\n",
    "# Phase markers for transient\n",
    "transient_phases = {\n",
    "    11: (18, 'pink', 'D&R-E (11-18)'),\n",
    "    18: (21, 'yellow', 'Continuity (18-21)'),\n",
    "    21: (30, 'lightgreen', 'Stabilization (21-30)')\n",
    "}\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Equal split\n",
    "    plot_cluster_performance(\n",
    "        trans_equal,\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 15}: Cluster {cluster_id} - CH Compromise Transient (Equal Split, 30 rounds)',\n",
    "        phases=transient_phases,\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "for cluster_id in [0, 1, 2]:\n",
    "    # Dirichlet split\n",
    "    plot_cluster_performance(\n",
    "        trans_dirichlet,\n",
    "        cluster_id,\n",
    "        f'Graph {cluster_id + 18}: Cluster {cluster_id} - CH Compromise Transient (Dirichlet Split, 30 rounds)',\n",
    "        phases=transient_phases,\n",
    "        figsize=(14, 7)\n",
    "    )\n",
    "\n",
    "print(\"ALL 20 GRAPHS GENERATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load existing results\n",
    "results_file = 'experiment_results/integrated_results_20251204_204130.pkl'\n",
    "with open(results_file, 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Extract transient data\n",
    "transient_data = results['transient_experiment']['per_cluster_equal']\n",
    "\n",
    "# Check structure\n",
    "print(\"Available rounds:\", len(transient_data))\n",
    "print(\"Sample entry:\", list(transient_data[0].keys()) if transient_data else \"Empty\")\n",
    "\n",
    "# Extract Cluster 0 and Cluster 1 data\n",
    "cluster_0_traffic = []\n",
    "cluster_1_traffic = []\n",
    "rounds_list = []\n",
    "\n",
    "for entry in transient_data:\n",
    "    round_num = entry['round']\n",
    "    rounds_list.append(round_num)\n",
    "    \n",
    "    # Cluster 0 data (from equal_split)\n",
    "    if 'equal_split' in entry and 0 in entry['equal_split']:\n",
    "        cluster_0_traffic.append(entry['equal_split'][0].get('traffic_accuracy', 0))\n",
    "    else:\n",
    "        cluster_0_traffic.append(0)\n",
    "    \n",
    "    # Cluster 1 data (from equal_split)\n",
    "    if 'equal_split' in entry and 1 in entry['equal_split']:\n",
    "        cluster_1_traffic.append(entry['equal_split'][1].get('traffic_accuracy', 0))\n",
    "    else:\n",
    "        cluster_1_traffic.append(0)\n",
    "\n",
    "# Plot Cluster 0 vs Cluster 1\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Cluster 0\n",
    "axes[0].plot(rounds_list, cluster_0_traffic, 'green', linewidth=2, marker='o', markersize=3)\n",
    "axes[0].axvspan(11, 18, alpha=0.15, color='pink', label='D&R-E (11-18)')\n",
    "axes[0].axvspan(18, 21, alpha=0.15, color='yellow', label='Continuity (18-21)')\n",
    "axes[0].axvspan(21, 30, alpha=0.10, color='lightgreen', label='Stabilization (21-30)')\n",
    "axes[0].set_xlabel('Rounds', fontsize=11)\n",
    "axes[0].set_ylabel('Traffic Accuracy', fontsize=11)\n",
    "axes[0].set_title('Graph 18: Cluster 0 - CH Compromise Transient (Dirichlet Split, 30 rounds)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 1.05)\n",
    "axes[0].set_xlim(0, 30)\n",
    "\n",
    "# Cluster 1\n",
    "axes[1].plot(rounds_list, cluster_1_traffic, 'green', linewidth=2, marker='o', markersize=3)\n",
    "axes[1].axvspan(11, 18, alpha=0.15, color='pink', label='D&R-E (11-18)')\n",
    "axes[1].axvspan(18, 21, alpha=0.15, color='yellow', label='Continuity (18-21)')\n",
    "axes[1].axvspan(21, 30, alpha=0.10, color='lightgreen', label='Stabilization (21-30)')\n",
    "axes[1].set_xlabel('Rounds', fontsize=11)\n",
    "axes[1].set_ylabel('Traffic Accuracy', fontsize=11)\n",
    "axes[1].set_title('Graph 19: Cluster 1 - CH Compromise Transient (Dirichlet Split, 30 rounds)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1.05)\n",
    "axes[1].set_xlim(0, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Cluster 0 (Compromised):\")\n",
    "print(f\"  Before D&R-E (Round 10): {cluster_0_traffic[9]:.4f}\")\n",
    "print(f\"  During D&R-E (Round 15): {cluster_0_traffic[14]:.4f}\")\n",
    "print(f\"  After recovery (Round 30): {cluster_0_traffic[-1]:.4f}\")\n",
    "print(f\"\\nCluster 1 (Healthy):\")\n",
    "print(f\"  Before D&R-E (Round 10): {cluster_1_traffic[9]:.4f}\")\n",
    "print(f\"  During D&R-E (Round 15): {cluster_1_traffic[14]:.4f}\")\n",
    "print(f\"  After recovery (Round 30): {cluster_1_traffic[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Checking saved models...\")\n",
    "print(\"\\n1. Convergence scenario:\")\n",
    "conv_dir = 'trained_models/convergence_integrated'\n",
    "if os.path.exists(conv_dir):\n",
    "    files = sorted([f for f in os.listdir(conv_dir) if f.endswith('.pkl')])\n",
    "    print(f\"    Found {len(files)} checkpoints\")\n",
    "    print(f\"   Rounds: {files[0]} to {files[-1]}\")\n",
    "else:\n",
    "    print(\"   ❌ Directory not found - need to run convergence training\")\n",
    "\n",
    "print(\"\\n2. Transient scenario:\")\n",
    "trans_dir = 'trained_models/transient_integrated'\n",
    "if os.path.exists(trans_dir):\n",
    "    files = sorted([f for f in os.listdir(trans_dir) if f.endswith('.pkl')])\n",
    "    print(f\"    Found {len(files)} checkpoints\")\n",
    "    print(f\"   Rounds: {files[0]} to {files[-1]}\")\n",
    "else:\n",
    "    print(\"   ❌ Directory not found - need to run transient training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if os.path.exists(conv_dir) and os.path.exists(trans_dir):\n",
    "    print(\" BOTH scenarios trained - you can save results directly\")\n",
    "else:\n",
    "    print(\" Missing training data - run the training cells first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cesnet",
   "language": "python",
   "name": "cesnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
