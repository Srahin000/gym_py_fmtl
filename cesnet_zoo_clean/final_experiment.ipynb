{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Cluster Federated MTL Setup\n",
    "This notebook contains all setup and configuration for single cluster federated multi-task learning.\n",
    "Ready for multi-cluster and CH compromisation experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded: 12500 samples, 48 features\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import flwr as fl\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "df = pd.read_csv('datasets/local_cache/dataset_12500_samples_65_features.csv')\n",
    "\n",
    "# Drop features with high label leakage\n",
    "cols_to_drop = [\n",
    " 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt',\n",
    " 'ack_flag_cnt', 'urg_flag_cnt', 'cwe_flag_cnt', 'ece_flag_cnt',\n",
    " 'fwd_header_length', 'bwd_header_length',\n",
    " 'active_mean', 'active_std', 'active_max', 'active_min',\n",
    " 'idle_mean', 'idle_std', 'idle_max', 'idle_min',\n",
    " 'subflow_fwd_bytes'\n",
    "]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "print(f\" Data loaded: {len(df)} samples, {len(df.columns)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random seeds set for reproducibility\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "print(\" Random seeds set for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Training: local_epochs=1, lr=0.001\n",
      "  Clients: 600 total (3 clusters × 200 clients)\n",
      "  Participation: 100.0%\n",
      "\n",
      "  Data Distribution (Two-Level):\n",
      "    Level 1 (Clusters): equal split\n",
      "    Level 2 (Clients): dirichlet split (α=0.4)\n",
      "    Cluster α: 0.4 (used when cluster_split='dirichlet')\n"
     ]
    }
   ],
   "source": [
    "CFG = {\n",
    "    # Training parameters\n",
    "    'local_epochs': 1,\n",
    "    'lr': 1e-3,\n",
    "    'loss_weights': {'traffic': 1, 'duration': 1, 'bandwidth': 1},\n",
    "    'test_size': 0.2,\n",
    "    \n",
    "    # Client configuration\n",
    "    'n_clients_flat': 600,\n",
    "    'n_clusters': 3,\n",
    "    'clients_per_cluster': 200,\n",
    "    'client_frac': 1.0,  # 100% client participation\n",
    "    \n",
    "    # Hierarchical FL\n",
    "    'global_aggregator_cluster': 1,  # Cluster 1 performs global aggregation\n",
    "    \n",
    "    # Data distribution (TWO-LEVEL SPLIT)\n",
    "    'cluster_split': 'equal',      # How to split data among clusters ('equal' or 'dirichlet')\n",
    "    'client_split': 'dirichlet',   # How to split data among clients within clusters (always 'dirichlet')\n",
    "    'alpha_client': 0.4,           # Dirichlet α for client-level distribution\n",
    "    'alpha_cluster': 0.4,          # Dirichlet α for cluster-level distribution (when cluster_split='dirichlet')\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Training: local_epochs={CFG['local_epochs']}, lr={CFG['lr']}\")\n",
    "print(f\"  Clients: {CFG['n_clients_flat']} total ({CFG['n_clusters']} clusters × {CFG['clients_per_cluster']} clients)\")\n",
    "print(f\"  Participation: {CFG['client_frac']*100}%\")\n",
    "print(f\"\\n  Data Distribution (Two-Level):\")\n",
    "print(f\"    Level 1 (Clusters): {CFG['cluster_split']} split\")\n",
    "print(f\"    Level 2 (Clients): {CFG['client_split']} split (α={CFG['alpha_client']})\")\n",
    "print(f\"    Cluster α: {CFG['alpha_cluster']} (used when cluster_split='dirichlet')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection for Each Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Traffic features: 39\n",
      " Duration features: 39\n",
      " Bandwidth features: 39\n"
     ]
    }
   ],
   "source": [
    "# Define features to exclude for each task (prevent label leakage)\n",
    "exclude_traffic = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port', # identity → leakage\n",
    " 'protocol', # not useful for QUIC-only\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "exclude_duration = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port',\n",
    " 'protocol',\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "exclude_bandwidth = [\n",
    " 'src_ip', 'dst_ip', 'src_port', 'dst_port',\n",
    " 'protocol',\n",
    " 'label', 'flow_duration', 'flow_bytes_per_s', 'bandwidth_bps'\n",
    "]\n",
    "\n",
    "Xcols_traffic = [col for col in df.columns if col not in exclude_traffic]\n",
    "Xcols_duration = [col for col in df.columns if col not in exclude_duration]\n",
    "Xcols_bandwidth = [col for col in df.columns if col not in exclude_bandwidth]\n",
    "\n",
    "print(f\" Traffic features: {len(Xcols_traffic)}\")\n",
    "print(f\" Duration features: {len(Xcols_duration)}\")\n",
    "print(f\" Bandwidth features: {len(Xcols_bandwidth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train samples: 10000\n",
      " Test samples: 2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = len(df)\n",
    "indices = np.arange(n)\n",
    "train_idx, test_idx = train_test_split(\n",
    " indices, \n",
    " test_size=CFG['test_size'], \n",
    " random_state=seed, \n",
    " shuffle=True\n",
    ")\n",
    "\n",
    "train_df = df.iloc[train_idx].copy()\n",
    "test_df = df.iloc[test_idx].copy()\n",
    "\n",
    "print(f\" Train samples: {len(train_df)}\")\n",
    "print(f\" Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Winsorization (Outlier Handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "all_features = list(set(Xcols_traffic + Xcols_duration + Xcols_bandwidth))\n",
    "\n",
    "# Calculate winsorization bounds from training data\n",
    "winsor_bounds = {}\n",
    "for col in all_features:\n",
    "    if col in train_df.columns:\n",
    "     lower = train_df[col].quantile(0.01)\n",
    "     upper = train_df[col].quantile(0.99)\n",
    "     winsor_bounds[col] = (lower, upper)\n",
    "\n",
    "# Apply winsorization\n",
    "for col, (lower, upper) in winsor_bounds.items():\n",
    " lower_limit = (train_df[col] < lower).mean()\n",
    " upper_limit = (train_df[col] > upper).mean()\n",
    " \n",
    " for df_temp in [train_df, test_df]:\n",
    "     df_temp[col] = winsorize(df_temp[col], limits=(lower_limit, upper_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Target Variable Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantile thresholds computed for duration and bandwidth\n"
     ]
    }
   ],
   "source": [
    "# Create quantile-based labels for duration and bandwidth (5 classes each)\n",
    "y_dur_raw_train = train_df['flow_duration'].values\n",
    "y_bw_raw_train = train_df['bandwidth_bps'].values\n",
    "\n",
    "# Log-transform\n",
    "bw_log = np.log1p(y_bw_raw_train)\n",
    "dur_log = np.log1p(y_dur_raw_train)\n",
    "\n",
    "# Compute 5-bin quantiles (20%, 40%, 60%, 80%)\n",
    "bw_quantiles = np.quantile(bw_log, [0.20, 0.40, 0.60, 0.80])\n",
    "dur_quantiles = np.quantile(dur_log, [0.20, 0.40, 0.60, 0.80])\n",
    "\n",
    "def create_quantile_labels(raw_values, quantiles):\n",
    " \"\"\"Create 5-class labels (0-4) using quantile thresholds\"\"\"\n",
    " v = np.log1p(raw_values)\n",
    " labels = np.digitize(v, quantiles, right=False) # returns 0..4\n",
    " return labels\n",
    "\n",
    "print(\" Quantile thresholds computed for duration and bandwidth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Label Encoding and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Labels created and features standardized\n",
      " Traffic classes: 5\n",
      " Duration classes: 5\n",
      " Bandwidth classes: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Create labels for all tasks\n",
    "y_dur_train = create_quantile_labels(train_df['flow_duration'].values, dur_quantiles)\n",
    "y_dur_test = create_quantile_labels(test_df['flow_duration'].values, dur_quantiles)\n",
    "\n",
    "y_bw_train = create_quantile_labels(train_df['bandwidth_bps'].values, bw_quantiles)\n",
    "y_bw_test = create_quantile_labels(test_df['bandwidth_bps'].values, bw_quantiles)\n",
    "\n",
    "# Traffic classification (label encoding)\n",
    "le_traf = LabelEncoder()\n",
    "y_traf_train = le_traf.fit_transform(train_df['label'])\n",
    "y_traf_test = le_traf.transform(test_df['label'])\n",
    "\n",
    "# Standardize features\n",
    "feature_scaler = StandardScaler()\n",
    "train_df[all_features] = feature_scaler.fit_transform(train_df[all_features])\n",
    "test_df[all_features] = feature_scaler.transform(test_df[all_features])\n",
    "\n",
    "print(\" Labels created and features standardized\")\n",
    "print(f\" Traffic classes: {len(np.unique(y_traf_train))}\")\n",
    "print(f\" Duration classes: {len(np.unique(y_dur_train))}\")\n",
    "print(f\" Bandwidth classes: {len(np.unique(y_bw_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Feature Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature matrices extracted\n",
      " Traffic: (10000, 39)\n",
      " Duration: (10000, 39)\n",
      " Bandwidth: (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "# Extract feature matrices for each task\n",
    "X_traffic_train = train_df[Xcols_traffic].values\n",
    "X_duration_train = train_df[Xcols_duration].values\n",
    "X_bandwidth_train = train_df[Xcols_bandwidth].values\n",
    "\n",
    "X_traffic_test = test_df[Xcols_traffic].values\n",
    "X_duration_test = test_df[Xcols_duration].values\n",
    "X_bandwidth_test = test_df[Xcols_bandwidth].values\n",
    "\n",
    "print(\" Feature matrices extracted\")\n",
    "print(f\" Traffic: {X_traffic_train.shape}\")\n",
    "print(f\" Duration: {X_duration_train.shape}\")\n",
    "print(f\" Bandwidth: {X_bandwidth_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Padding (Uniform Dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All features padded to dimension: 39\n",
      "  Traffic: (10000, 39)\n",
      "  Duration: (10000, 39)\n",
      "  Bandwidth: (10000, 39)\n"
     ]
    }
   ],
   "source": [
    "# Pad all feature matrices to the same dimension\n",
    "max_dim = max(X_traffic_train.shape[1], X_duration_train.shape[1], X_bandwidth_train.shape[1])\n",
    "\n",
    "def pad_features(X, target_size):\n",
    "    \"\"\"Pad features with zeros to reach target size\"\"\"\n",
    "    if X.shape[1] < target_size:\n",
    "        padding = np.zeros((X.shape[0], target_size - X.shape[1]))\n",
    "        return np.concatenate([X, padding], axis=1)\n",
    "    return X\n",
    "\n",
    "X_traffic_train = pad_features(X_traffic_train, max_dim)\n",
    "X_duration_train = pad_features(X_duration_train, max_dim)\n",
    "X_bandwidth_train = pad_features(X_bandwidth_train, max_dim)\n",
    "\n",
    "X_traffic_test = pad_features(X_traffic_test, max_dim)\n",
    "X_duration_test = pad_features(X_duration_test, max_dim)\n",
    "X_bandwidth_test = pad_features(X_bandwidth_test, max_dim)\n",
    "\n",
    "print(f\"✓ All features padded to dimension: {max_dim}\")\n",
    "print(f\"  Traffic: {X_traffic_train.shape}\")\n",
    "print(f\"  Duration: {X_duration_train.shape}\")\n",
    "print(f\"  Bandwidth: {X_bandwidth_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mutual Information Analysis (Feature Leakage Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing mutual information between features and labels...\n",
      "\n",
      "Duration - Found 30 features with MI > 0.2:\n",
      "  • bwd_packets_per_s: 0.9275\n",
      "  • flow_packets_per_s: 0.9051\n",
      "  • fwd_packets_per_s: 0.8397\n",
      "  • flow_iat_mean: 0.7458\n",
      "  • flow_iat_max: 0.7057\n",
      "  • flow_iat_std: 0.6616\n",
      "  • fwd_iat_total: 0.5753\n",
      "  • fwd_iat_std: 0.5706\n",
      "  • fwd_iat_max: 0.5513\n",
      "  • fwd_iat_mean: 0.5258\n",
      "  ... and 20 more\n",
      "\n",
      "Bandwidth - Found 30 features with MI > 0.2:\n",
      "  • bwd_packets_per_s: 1.2198\n",
      "  • flow_packets_per_s: 1.1862\n",
      "  • fwd_packets_per_s: 1.0154\n",
      "  • flow_iat_mean: 0.7372\n",
      "  • flow_iat_max: 0.6862\n",
      "  • flow_iat_std: 0.6665\n",
      "  • fwd_iat_std: 0.5595\n",
      "  • fwd_iat_mean: 0.5562\n",
      "  • fwd_iat_max: 0.5272\n",
      "  • fwd_iat_total: 0.5123\n",
      "  ... and 20 more\n",
      "\n",
      "Traffic - Found 35 features with MI > 0.2:\n",
      "  • bwd_pkt_len_min: 1.1980\n",
      "  • bwd_seg_size_min: 1.1928\n",
      "  • fwd_seg_size_min: 1.0199\n",
      "  • fwd_pkt_len_min: 1.0194\n",
      "  • bwd_pkt_len_max: 0.9857\n",
      "  • fwd_pkt_len_max: 0.7743\n",
      "  • flow_rate_entropy: 0.6934\n",
      "  • bwd_pkt_len_mean: 0.6692\n",
      "  • total_len_bwd_packets: 0.6347\n",
      "  • avg_bwd_segment_size: 0.5876\n",
      "  ... and 25 more\n",
      "\n",
      "✓ Mutual information analysis complete\n",
      "  Note: High MI features may indicate correlation with labels\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import sys\n",
    "\n",
    "def find_high_mi_features(X_cols, y_train, train_df, task_name, seed, threshold=0.2):\n",
    "    \"\"\"Find features with high mutual information (potential label leakage)\"\"\"\n",
    "    X_train = train_df[X_cols].values\n",
    "    \n",
    "    try:\n",
    "        mi_scores = mutual_info_classif(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            discrete_features=False,\n",
    "            random_state=seed\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating MI for {task_name}: {e}\", file=sys.stderr)\n",
    "        return []\n",
    "    \n",
    "    mi_results = dict(zip(X_cols, mi_scores))\n",
    "    \n",
    "    problematic = []\n",
    "    for feat, mi in mi_results.items():\n",
    "        if mi > threshold:\n",
    "            problematic.append((feat, mi))\n",
    "    \n",
    "    if problematic:\n",
    "        print(f\"\\n{task_name} - Found {len(problematic)} features with MI > {threshold}:\")\n",
    "        problematic.sort(key=lambda x: x[1], reverse=True)\n",
    "        for feat, mi in problematic[:10]:  # Show top 10\n",
    "            print(f\"  • {feat}: {mi:.4f}\")\n",
    "        if len(problematic) > 10:\n",
    "            print(f\"  ... and {len(problematic) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n{task_name} - No features found with MI > {threshold}\")\n",
    "    \n",
    "    return problematic\n",
    "\n",
    "print(\"Analyzing mutual information between features and labels...\")\n",
    "\n",
    "problematic_dur = find_high_mi_features(\n",
    "    Xcols_duration, y_dur_train, train_df, 'Duration', seed\n",
    ")\n",
    "\n",
    "problematic_bw = find_high_mi_features(\n",
    "    Xcols_bandwidth, y_bw_train, train_df, 'Bandwidth', seed\n",
    ")\n",
    "\n",
    "problematic_tf = find_high_mi_features(\n",
    "    Xcols_traffic, y_traf_train, train_df, 'Traffic', seed\n",
    ")\n",
    "\n",
    "all_diagnostics = {\n",
    "    'duration': problematic_dur,\n",
    "    'bandwidth': problematic_bw,\n",
    "    'traffic': problematic_tf\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Mutual information analysis complete\")\n",
    "print(\"  Note: High MI features may indicate correlation with labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Client Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Created 600 clients\n",
      "  Cluster split: equal\n",
      "  Client split: dirichlet\n",
      "  Sample sizes: min=50, max=92, avg=50.2\n",
      "\n",
      "  Cluster distribution:\n",
      "    Cluster 0: 200 clients, 10016 samples\n",
      "    Cluster 1: 200 clients, 10067 samples\n",
      "    Cluster 2: 200 clients, 10061 samples\n",
      "\n",
      "  Sample client label distributions:\n",
      "    Client 0 (Cluster 0): {0: 16, 1: 9, 2: 9, 3: 7, 4: 9}\n",
      "    Client 1 (Cluster 0): {0: 16, 1: 11, 2: 8, 3: 9, 4: 6}\n",
      "    Client 2 (Cluster 0): {0: 8, 1: 12, 2: 5, 3: 13, 4: 12}\n"
     ]
    }
   ],
   "source": [
    "def build_client_partitions(cluster_split='equal', client_split='dirichlet', verbose=True):\n",
    "    \"\"\"\n",
    "    Build client partitions with TWO-LEVEL data distribution:\n",
    "    - Level 1: Distribute data among CLUSTERS (equal or dirichlet)\n",
    "    - Level 2: Distribute each cluster's data among CLIENTS (dirichlet)\n",
    "    \n",
    "    Args:\n",
    "        cluster_split: 'equal' or 'dirichlet' - how to split data among clusters\n",
    "        client_split: 'dirichlet' - how to split data among clients within clusters\n",
    "        verbose: Print statistics\n",
    "    \n",
    "    Returns:\n",
    "        client_indices_flat: List of client data indices\n",
    "        client_index_to_cluster: Dict mapping client idx to cluster id\n",
    "    \"\"\"\n",
    "    n_clients = CFG['n_clients_flat']\n",
    "    n_clusters = CFG['n_clusters']\n",
    "    clients_per_cluster = CFG['clients_per_cluster']\n",
    "    alpha_client = CFG['alpha_client']\n",
    "    alpha_cluster = CFG['alpha_cluster']\n",
    "    min_size = 50\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    train_indices = np.arange(len(y_traf_train))\n",
    "    labels = np.unique(y_traf_train)\n",
    "    \n",
    "    # Cluster_level_split\n",
    "    \n",
    "    if cluster_split == 'equal':\n",
    "        # Equal split: each cluster gets 1/n_clusters of data\n",
    "        samples_per_cluster = len(train_indices) // n_clusters\n",
    "        cluster_indices = []\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            start_idx = cluster_id * samples_per_cluster\n",
    "            end_idx = start_idx + samples_per_cluster if cluster_id < n_clusters - 1 else len(train_indices)\n",
    "            cluster_indices.append(train_indices[start_idx:end_idx])\n",
    "    \n",
    "    elif cluster_split == 'dirichlet':\n",
    "        # Dirichlet split: non-IID distribution among clusters\n",
    "        cluster_bins = [[] for _ in range(n_clusters)]\n",
    "        label_indices = {}\n",
    "        \n",
    "        for lbl in labels:\n",
    "            label_indices[lbl] = train_indices[y_traf_train == lbl]\n",
    "        \n",
    "        for lbl in labels:\n",
    "            idxs = label_indices[lbl]\n",
    "            rng.shuffle(idxs)\n",
    "            proportions = rng.dirichlet([alpha_cluster] * n_clusters)\n",
    "            cuts = (np.cumsum(proportions) * len(idxs)).astype(int)\n",
    "            parts = np.split(idxs, cuts[:-1])\n",
    "            \n",
    "            for cluster_id, part in enumerate(parts):\n",
    "                cluster_bins[cluster_id].extend(part.tolist())\n",
    "        \n",
    "        cluster_indices = [np.array(sorted(set(cluster_bins[i])), dtype=int) for i in range(n_clusters)]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cluster_split: {cluster_split}\")\n",
    "\n",
    "    # Client_level_split\n",
    "    \n",
    "    client_indices_flat = []\n",
    "    client_index_to_cluster = {}\n",
    "    \n",
    "    for cluster_id, cluster_data_indices in enumerate(cluster_indices):\n",
    "        # Get labels for this cluster's data\n",
    "        cluster_labels = y_traf_train[cluster_data_indices]\n",
    "        unique_cluster_labels = np.unique(cluster_labels)\n",
    "        \n",
    "        # Build client bins for this cluster using Dirichlet\n",
    "        client_bins = [[] for _ in range(clients_per_cluster)]\n",
    "        \n",
    "        for lbl in unique_cluster_labels:\n",
    "            # Get indices within cluster that have this label\n",
    "            lbl_mask = cluster_labels == lbl\n",
    "            lbl_indices = cluster_data_indices[lbl_mask]\n",
    "            \n",
    "            if len(lbl_indices) > 0:\n",
    "                rng.shuffle(lbl_indices)\n",
    "                proportions = rng.dirichlet([alpha_client] * clients_per_cluster)\n",
    "                cuts = (np.cumsum(proportions) * len(lbl_indices)).astype(int)\n",
    "                parts = np.split(lbl_indices, cuts[:-1])\n",
    "                \n",
    "                for local_client_id, part in enumerate(parts):\n",
    "                    client_bins[local_client_id].extend(part.tolist())\n",
    "        \n",
    "        # Create clients for this cluster\n",
    "        for local_client_id in range(clients_per_cluster):\n",
    "            client_data = np.array(sorted(set(client_bins[local_client_id])), dtype=int)\n",
    "            \n",
    "            # Ensure minimum size\n",
    "            if len(client_data) < min_size:\n",
    "                need = min_size - len(client_data)\n",
    "                # Sample from cluster's data\n",
    "                available = list(set(cluster_data_indices) - set(client_data))\n",
    "                if len(available) >= need:\n",
    "                    extra = rng.choice(available, size=need, replace=False)\n",
    "                else:\n",
    "                    extra = rng.choice(cluster_data_indices, size=need, replace=True)\n",
    "                client_data = np.concatenate([client_data, extra])\n",
    "                client_data = np.unique(client_data).astype(int)\n",
    "            \n",
    "            global_client_id = cluster_id * clients_per_cluster + local_client_id\n",
    "            client_indices_flat.append(client_data.astype(int))\n",
    "            client_index_to_cluster[global_client_id] = cluster_id\n",
    "    \n",
    "    # Statistics\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n✓ Created {len(client_indices_flat)} clients\")\n",
    "        print(f\"  Cluster split: {cluster_split}\")\n",
    "        print(f\"  Client split: {client_split}\")\n",
    "        print(f\"  Sample sizes: min={min([len(c) for c in client_indices_flat])}, \"\n",
    "              f\"max={max([len(c) for c in client_indices_flat])}, \"\n",
    "              f\"avg={np.mean([len(c) for c in client_indices_flat]):.1f}\")\n",
    "        \n",
    "        print(\"\\n  Cluster distribution:\")\n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_clients = [i for i in range(n_clients) if client_index_to_cluster[i] == cluster_id]\n",
    "            cluster_samples = sum(len(client_indices_flat[i]) for i in cluster_clients)\n",
    "            print(f\"    Cluster {cluster_id}: {len(cluster_clients)} clients, {cluster_samples} samples\")\n",
    "        \n",
    "        print(\"\\n  Sample client label distributions:\")\n",
    "        for i in range(min(3, len(client_indices_flat))):\n",
    "            indices = client_indices_flat[i]\n",
    "            labels_count = {}\n",
    "            for lbl in labels:\n",
    "                count = np.sum(y_traf_train[indices] == lbl)\n",
    "                if count > 0:\n",
    "                    labels_count[int(lbl)] = int(count)\n",
    "            print(f\"    Client {i} (Cluster {client_index_to_cluster[i]}): {labels_count}\")\n",
    "    \n",
    "    return client_indices_flat, client_index_to_cluster\n",
    "# Build clients with specified split type\n",
    "client_indices_flat, client_index_to_cluster = build_client_partitions(\n",
    "    cluster_split=CFG['cluster_split'],  # ✓ NEW: equal or dirichlet for clusters\n",
    "    client_split=CFG['client_split'],    # ✓ NEW: dirichlet for clients\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client data structures created\n",
      " Total clients: 600\n",
      " Clusters: 3\n",
      "\n",
      "Client 0 data shapes:\n",
      " Traffic:   X=(50, 39),   y=(50,)\n",
      " Duration:  X=(50, 39),  y=(50,)\n",
      " Bandwidth: X=(50, 39), y=(50,)\n"
     ]
    }
   ],
   "source": [
    "class ClientData:\n",
    "    \"\"\"Container for client data and metadata\"\"\"\n",
    "    def __init__(self, data_dict, cluster_id):\n",
    "        self.ds = data_dict\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "\n",
    "# Create client objects\n",
    "clients = []\n",
    "\n",
    "for i, indices in enumerate(client_indices_flat):\n",
    "\n",
    "    # ensure numpy integer index array\n",
    "    indices = np.asarray(indices, dtype=np.int32)\n",
    "\n",
    "    # Slice features\n",
    "    X_traffic_client   = X_traffic_train[indices].astype(np.float32).copy()\n",
    "    X_duration_client  = X_duration_train[indices].astype(np.float32).copy()\n",
    "    X_bandwidth_client = X_bandwidth_train[indices].astype(np.float32).copy()\n",
    "\n",
    "    # Slice labels\n",
    "    y_traffic_client   = y_traf_train[indices].astype(np.int32).copy()\n",
    "    y_duration_client  = y_dur_train[indices].astype(np.int32).copy()\n",
    "    y_bandwidth_client = y_bw_train[indices].astype(np.int32).copy()\n",
    "\n",
    "    # Package\n",
    "    client_data_dict = {\n",
    "        'traffic':   (X_traffic_client,   y_traffic_client),\n",
    "        'duration':  (X_duration_client,  y_duration_client),\n",
    "        'bandwidth': (X_bandwidth_client, y_bandwidth_client)\n",
    "    }\n",
    "\n",
    "    # Cluster ID lookup\n",
    "    cluster_id = client_index_to_cluster[i]\n",
    "\n",
    "    # Create client object\n",
    "    clients.append(ClientData(client_data_dict, cluster_id))\n",
    "\n",
    "\n",
    "# Diagnostics\n",
    "print(\"\\nClient data structures created\")\n",
    "print(f\" Total clients: {len(clients)}\")\n",
    "print(f\" Clusters: {CFG['n_clusters']}\")\n",
    "\n",
    "print(\"\\nClient 0 data shapes:\")\n",
    "print(f\" Traffic:   X={clients[0].ds['traffic'][0].shape},   y={clients[0].ds['traffic'][1].shape}\")\n",
    "print(f\" Duration:  X={clients[0].ds['duration'][0].shape},  y={clients[0].ds['duration'][1].shape}\")\n",
    "print(f\" Bandwidth: X={clients[0].ds['bandwidth'][0].shape}, y={clients[0].ds['bandwidth'][1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test data prepared\n",
      " Traffic: (2500, 39)\n",
      " Duration: (2500, 39)\n",
      " Bandwidth: (2500, 39)\n"
     ]
    }
   ],
   "source": [
    "test_data = {\n",
    " 'traffic': (X_traffic_test.astype(np.float32), y_traf_test.astype(int)),\n",
    " 'duration': (X_duration_test.astype(np.float32), y_dur_test.astype(int)),\n",
    " 'bandwidth': (X_bandwidth_test.astype(np.float32), y_bw_test.astype(int))\n",
    "}\n",
    "\n",
    "print(\"\\n Test data prepared\")\n",
    "print(f\" Traffic: {test_data['traffic'][0].shape}\")\n",
    "print(f\" Duration: {test_data['duration'][0].shape}\")\n",
    "print(f\" Bandwidth: {test_data['bandwidth'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed test data saved to: /Users/sadmanrahin/Documents/gym-pybullet-drones/cesnet_zoo_clean/trained_models/preprocessed_test_data.pkl\n",
      "  Samples: 2500\n",
      "  Input dim: 39\n",
      "  Classes: {'traffic': 5, 'duration': 5, 'bandwidth': 5}\n",
      "  Traffic labels: ['discord', 'facebook-web', 'google-services', 'instagram', 'youtube']\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed test data for PyBullet simulation inference\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create test data dict with all preprocessed arrays\n",
    "preprocessed_test_data = {\n",
    "    'X_traffic': X_traffic_test.astype(np.float32),\n",
    "    'X_duration': X_duration_test.astype(np.float32),\n",
    "    'X_bandwidth': X_bandwidth_test.astype(np.float32),\n",
    "    'y_traffic': y_traf_test.astype(np.int32),\n",
    "    'y_duration': y_dur_test.astype(np.int32),\n",
    "    'y_bandwidth': y_bw_test.astype(np.int32),\n",
    "    'n_samples': len(y_traf_test),\n",
    "    'input_dim': X_traffic_test.shape[1],\n",
    "    'n_classes': {\n",
    "        'traffic': len(np.unique(y_traf_test)),\n",
    "        'duration': len(np.unique(y_dur_test)),\n",
    "        'bandwidth': len(np.unique(y_bw_test))\n",
    "    },\n",
    "    'traffic_label_encoder_classes': le_traf.classes_.tolist()  # Save label mapping\n",
    "}\n",
    "\n",
    "# Save to trained_models directory\n",
    "save_path = '/Users/sadmanrahin/Documents/gym-pybullet-drones/cesnet_zoo_clean/trained_models/preprocessed_test_data.pkl'\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(preprocessed_test_data, f)\n",
    "\n",
    "print(f\"✓ Preprocessed test data saved to: {save_path}\")\n",
    "print(f\"  Samples: {preprocessed_test_data['n_samples']}\")\n",
    "print(f\"  Input dim: {preprocessed_test_data['input_dim']}\")\n",
    "print(f\"  Classes: {preprocessed_test_data['n_classes']}\")\n",
    "print(f\"  Traffic labels: {preprocessed_test_data['traffic_label_encoder_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Data Distribution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DISTRIBUTION SUMMARY\n",
      "\n",
      "Unique Classes:\n",
      " Traffic classes: 5\n",
      " Duration classes: 5\n",
      " Bandwidth classes: 5\n",
      "\n",
      "Duration (Train) Distribution:\n",
      " Very Short (0): 2000\n",
      " Short (1): 2000\n",
      " Medium (2): 2000\n",
      " Long (3): 2000\n",
      " Very Long (4): 2000\n",
      "\n",
      "Bandwidth (Train) Distribution:\n",
      " Very Low (0): 2000\n",
      " Low (1): 2000\n",
      " Medium (2): 2000\n",
      " High (3): 2000\n",
      " Very High (4): 2000\n",
      "\n",
      "Traffic (Train) Distribution:\n",
      " Class 0: 1999\n",
      " Class 1: 1991\n",
      " Class 2: 2006\n",
      " Class 3: 1984\n",
      " Class 4: 2020\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(labels, name, mapping=None):\n",
    "    \"\"\"Print class distribution with optional name mapping.\"\"\"\n",
    "    print(f\"\\n{name} Distribution:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    for u, c in zip(unique, counts):\n",
    "        if mapping:\n",
    "            label_name = mapping.get(u, f\"Class {u}\")\n",
    "            print(f\" {label_name} ({u}): {c}\")\n",
    "        else:\n",
    "            print(f\" Class {u}: {c}\")\n",
    "\n",
    "\n",
    "duration_map = {\n",
    "    0: \"Very Short\",\n",
    "    1: \"Short\",\n",
    "    2: \"Medium\",\n",
    "    3: \"Long\",\n",
    "    4: \"Very Long\"\n",
    "}\n",
    "\n",
    "bandwidth_map = {\n",
    "    0: \"Very Low\",\n",
    "    1: \"Low\",\n",
    "    2: \"Medium\",\n",
    "    3: \"High\",\n",
    "    4: \"Very High\"\n",
    "}\n",
    "\n",
    "print(\"DATA DISTRIBUTION SUMMARY\")\n",
    "\n",
    "print(\"\\nUnique Classes:\")\n",
    "print(f\" Traffic classes: {len(np.unique(y_traf_train))}\")\n",
    "print(f\" Duration classes: {len(np.unique(y_dur_train))}\")\n",
    "print(f\" Bandwidth classes: {len(np.unique(y_bw_train))}\")\n",
    "\n",
    "print_distribution(y_dur_train, \"Duration (Train)\", duration_map)\n",
    "print_distribution(y_bw_train, \"Bandwidth (Train)\", bandwidth_map)\n",
    "print_distribution(y_traf_train, \"Traffic (Train)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Model Architecture (FedMTL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FedMTLModel architecture defined\n",
      " Architecture:\n",
      " Shared: Input → Dense(256) → Dropout → Dense(128) → Dropout\n",
      " Traffic:   → Dense(64) → Dense(n_classes)\n",
      " Duration:  → Dense(32) → Dense(n_classes)\n",
      " Bandwidth: → Dense(64) → Dense(n_classes)\n"
     ]
    }
   ],
   "source": [
    "class FedMTLModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Federated Multi-Task Learning Model\n",
    "\n",
    "    Architecture:\n",
    "    - Shared layers: 2 dense layers (256 → 128) with dropout\n",
    "    - Task-specific layers: 1 dense layer per task\n",
    "    - Task heads: 3 classification heads (traffic, duration, bandwidth)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dims, n_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tasks = ['traffic', 'duration', 'bandwidth']\n",
    "\n",
    "        # Shared layers (learned across all tasks)\n",
    "        self.shared_dense1 = keras.layers.Dense(256, activation='relu', name='shared_dense1')\n",
    "        self.shared_drop1  = keras.layers.Dropout(dropout)\n",
    "        self.shared_dense2 = keras.layers.Dense(128, activation='relu', name='shared_dense2')\n",
    "        self.shared_drop2  = keras.layers.Dropout(dropout)\n",
    "\n",
    "        # Task-specific layers\n",
    "        self.task_dense = {\n",
    "            'traffic':   keras.layers.Dense(6, activation='relu', name='task_traffic_dense'),\n",
    "            'duration':  keras.layers.Dense(32, activation='relu', name='task_duration_dense'),\n",
    "            'bandwidth': keras.layers.Dense(64, activation='relu', name='task_bandwidth_dense'),\n",
    "        }\n",
    "\n",
    "        # Task heads (output logits)\n",
    "        self.task_heads = {\n",
    "            'traffic':   keras.layers.Dense(n_classes['traffic'],   name='traffic_output'),\n",
    "            'duration':  keras.layers.Dense(n_classes['duration'],  name='duration_output'),\n",
    "            'bandwidth': keras.layers.Dense(n_classes['bandwidth'], name='bandwidth_output'),\n",
    "        }\n",
    "\n",
    "    def call(self, x, task, training=False):\n",
    "        \"\"\"Forward pass for a specific task\"\"\"\n",
    "        # Shared layers\n",
    "        x = self.shared_dense1(x)\n",
    "        x = self.shared_drop1(x, training=training)\n",
    "        x = self.shared_dense2(x)\n",
    "        x = self.shared_drop2(x, training=training)\n",
    "\n",
    "        # Task-specific branch\n",
    "        x = self.task_dense[task](x)\n",
    "\n",
    "        # Final classification head\n",
    "        return self.task_heads[task](x)\n",
    "\n",
    "    def build_all(self, input_dim):\n",
    "        \"\"\"Build all task heads with a dummy forward pass\"\"\"\n",
    "        tf.random.set_seed(seed)\n",
    "        dummy = tf.random.normal((1, input_dim))\n",
    "\n",
    "        for task in self.tasks:\n",
    "            _ = self.call(dummy, task=task, training=False)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "# Register in Keras custom objects\n",
    "tf.keras.utils.get_custom_objects().update({'FedMTLModel': FedMTLModel})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Flower Client Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MTLFlowerClient implementation complete\n",
      "Features:\n",
      " - Multi-task local training\n",
      " - Weighted loss aggregation over active tasks\n",
      " - Task-specific evaluation\n",
      " - Parameter change tracking\n"
     ]
    }
   ],
   "source": [
    "class MTLFlowerClient(fl.client.NumPyClient):\n",
    "    \"\"\"\n",
    "    Flower client for Multi-Task Learning\n",
    "\n",
    "    Handles:\n",
    "    - Local training on multiple tasks\n",
    "    - Parameter synchronization with server\n",
    "    - Task-specific evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, client_data, cfg, cluster_id):\n",
    "        self.model = model\n",
    "        self.client_data = client_data \n",
    "        self.cfg = cfg\n",
    "        self.cluster_id = cluster_id\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.cfg['lr'])\n",
    "\n",
    "        # Loss functions (all classification)\n",
    "        self.loss_fns = {\n",
    "            'traffic': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'duration': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            'bandwidth': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        }\n",
    "\n",
    "        # Task-specific loss weights\n",
    "        self.loss_weights = cfg['loss_weights']\n",
    "\n",
    "    # -------- Utility --------\n",
    "    def _ensure_model_built(self):\n",
    "        \"\"\"Make sure the Keras model is built before use.\"\"\"\n",
    "        if self.model.built:\n",
    "            return\n",
    "\n",
    "        # Try to build from local data\n",
    "        x = None\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task in self.client_data:\n",
    "                X_task, _ = self.client_data[task]\n",
    "                if len(X_task) > 0:\n",
    "                    x = tf.convert_to_tensor(X_task[:1], dtype=tf.float32)\n",
    "                    break\n",
    "\n",
    "        # If this client has absolutely no data, fall back to cfg['max_dim'] if available\n",
    "        if x is None:\n",
    "            if 'max_dim' in self.cfg:\n",
    "                input_dim = self.cfg['max_dim']\n",
    "            else:\n",
    "                # Try to infer from any task across this client\n",
    "                all_dims = [\n",
    "                    v[0].shape[1] for v in self.client_data.values()\n",
    "                    if v[0].shape[0] > 0\n",
    "                ]\n",
    "                input_dim = all_dims[0] if all_dims else 1\n",
    "            x = tf.random.normal((1, input_dim))\n",
    "\n",
    "        for t in ['traffic', 'duration', 'bandwidth']:\n",
    "            _ = self.model(x, task=t, training=False)\n",
    "        self.model.built = True\n",
    "\n",
    "    # -------- Flower API --------\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current model weights\"\"\"\n",
    "        self._ensure_model_built()\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Local training on client data\"\"\"\n",
    "        self._ensure_model_built()\n",
    "\n",
    "        # Set global weights\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "        # Local training loop\n",
    "        for epoch in range(self.cfg['local_epochs']):\n",
    "            with tf.GradientTape() as tape:\n",
    "                total_loss = 0.0\n",
    "                used_tasks = []\n",
    "\n",
    "                # Loop through all available tasks\n",
    "                for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                    if task not in self.client_data:\n",
    "                        continue\n",
    "\n",
    "                    X_task, y_task = self.client_data[task]\n",
    "                    if len(X_task) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Convert data to tensors\n",
    "                    X_task_tf = tf.convert_to_tensor(X_task, dtype=tf.float32)\n",
    "                    y_task_tf = tf.convert_to_tensor(y_task, dtype=tf.int32)\n",
    "\n",
    "                    # Forward pass\n",
    "                    logits = self.model(X_task_tf, task=task, training=True)\n",
    "\n",
    "                    # Compute loss and apply task weight\n",
    "                    task_loss = self.loss_fns[task](y_task_tf, logits)\n",
    "                    weighted_loss = task_loss * self.loss_weights[task]\n",
    "\n",
    "                    total_loss += weighted_loss\n",
    "                    used_tasks.append(task)\n",
    "\n",
    "                if len(used_tasks) > 0:\n",
    "                    # Normalize by sum of weights of tasks that are actually present\n",
    "                    norm = sum(self.loss_weights[t] for t in used_tasks)\n",
    "                    total_loss = total_loss / norm\n",
    "\n",
    "                    # Apply gradients\n",
    "                    grads = tape.gradient(total_loss, self.model.trainable_weights)\n",
    "                    if grads is not None and any(g is not None for g in grads):\n",
    "                        self.optimizer.apply_gradients(\n",
    "                            zip(grads, self.model.trainable_weights)\n",
    "                        )\n",
    "                else:\n",
    "                    total_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "        # Return updated weights and metadata\n",
    "        num_examples = sum(len(data[1]) for data in self.client_data.values())\n",
    "        avg_loss = float(total_loss.numpy()) if isinstance(total_loss, tf.Tensor) else float(total_loss)\n",
    "\n",
    "        return self.model.get_weights(), num_examples, {\n",
    "            \"loss\": avg_loss,\n",
    "            \"num_tasks\": len(self.client_data),\n",
    "            \"cluster_id\": self.cluster_id,\n",
    "            \"num_examples\": num_examples\n",
    "        }\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate model on client data\"\"\"\n",
    "        self._ensure_model_built()\n",
    "        self.model.set_weights(parameters)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        task_accuracies = {}\n",
    "        used_tasks = []\n",
    "\n",
    "        for task in ['traffic', 'duration', 'bandwidth']:\n",
    "            if task not in self.client_data:\n",
    "                continue\n",
    "\n",
    "            X_task, y_task = self.client_data[task]\n",
    "            if len(X_task) == 0:\n",
    "                continue\n",
    "\n",
    "            X_task_tf = tf.convert_to_tensor(X_task, dtype=tf.float32)\n",
    "            y_task_tf = tf.convert_to_tensor(y_task, dtype=tf.int32)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(X_task_tf, task=task, training=False)\n",
    "\n",
    "            # Compute loss and apply weights\n",
    "            task_loss = self.loss_fns[task](y_task_tf, logits)\n",
    "            weighted_loss = task_loss * self.loss_weights[task]\n",
    "            total_loss += weighted_loss\n",
    "\n",
    "            # Classification evaluation\n",
    "            predictions = tf.argmax(logits, axis=1)\n",
    "            accuracy = tf.reduce_mean(\n",
    "                tf.cast(\n",
    "                    tf.equal(predictions, tf.cast(y_task_tf, tf.int64)),\n",
    "                    tf.float32,\n",
    "                )\n",
    "            )\n",
    "            task_accuracies[f\"{task}_accuracy\"] = float(accuracy)\n",
    "            task_accuracies[f\"{task}_loss\"] = float(task_loss)\n",
    "\n",
    "            total_samples += len(y_task)\n",
    "            used_tasks.append(task)\n",
    "\n",
    "        if len(used_tasks) > 0:\n",
    "            norm = sum(self.loss_weights[t] for t in used_tasks)\n",
    "            avg_loss = float(total_loss / norm)\n",
    "            overall_accuracy = np.mean([\n",
    "                task_accuracies[f\"{task}_accuracy\"]\n",
    "                for task in used_tasks\n",
    "            ])\n",
    "        else:\n",
    "            avg_loss = 0.0\n",
    "            overall_accuracy = 0.0\n",
    "\n",
    "        task_accuracies[\"accuracy\"] = overall_accuracy\n",
    "\n",
    "        return float(avg_loss), int(total_samples), task_accuracies\n",
    "\n",
    "\n",
    "print(\"\\nMTLFlowerClient implementation complete\")\n",
    "print(\"Features:\")\n",
    "print(\" - Multi-task local training\")\n",
    "print(\" - Weighted loss aggregation over active tasks\")\n",
    "print(\" - Task-specific evaluation\")\n",
    "print(\" - Parameter change tracking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CONFIGURATION SUMMARY\n",
      "\n",
      "Input dimensions:\n",
      " traffic: 39\n",
      " duration: 39\n",
      " bandwidth: 39\n",
      "\n",
      "Number of classes:\n",
      " traffic: 5\n",
      " duration: 5\n",
      " bandwidth: 5\n",
      "\n",
      "Training configuration:\n",
      " Local epochs: 1\n",
      " Learning rate: 0.001\n",
      " Loss weights: {'traffic': 1, 'duration': 1, 'bandwidth': 1}\n",
      " Client participation: 100.0%\n",
      "\n",
      "Federation structure:\n",
      " Total clients: 600\n",
      " Number of clusters: 3\n",
      " Clients per cluster: 200\n",
      " Global aggregator: Cluster 1\n",
      " Split type: equal\n"
     ]
    }
   ],
   "source": [
    "in_dims = {\n",
    " 'traffic': max_dim,\n",
    " 'duration': max_dim,\n",
    " 'bandwidth': max_dim \n",
    "}\n",
    "\n",
    "n_classes = {\n",
    " 'traffic': len(np.unique(y_traf_train)),\n",
    " 'duration': len(np.unique(y_dur_train)),\n",
    " 'bandwidth': len(np.unique(y_bw_train))\n",
    "}\n",
    "\n",
    "print(\"MODEL CONFIGURATION SUMMARY\")\n",
    "print(f\"\\nInput dimensions:\")\n",
    "for task, dim in in_dims.items():\n",
    " print(f\" {task}: {dim}\")\n",
    "print(f\"\\nNumber of classes:\")\n",
    "for task, n in n_classes.items():\n",
    " print(f\" {task}: {n}\")\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\" Local epochs: {CFG['local_epochs']}\")\n",
    "print(f\" Learning rate: {CFG['lr']}\")\n",
    "print(f\" Loss weights: {CFG['loss_weights']}\")\n",
    "print(f\" Client participation: {CFG['client_frac']*100}%\")\n",
    "\n",
    "print(f\"\\nFederation structure:\")\n",
    "print(f\" Total clients: {CFG['n_clients_flat']}\")\n",
    "print(f\" Number of clusters: {CFG['n_clusters']}\")\n",
    "print(f\" Clients per cluster: {CFG['clients_per_cluster']}\")\n",
    "print(f\" Global aggregator: Cluster {CFG['global_aggregator_cluster']}\")\n",
    "print(f\" Split type: {CFG['cluster_split']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Verification Complete\n",
    "\n",
    "### Key Verifications:\n",
    "\n",
    "**1. Model Architecture** \n",
    "- Shared layers: 256 → 128 with dropout (0.1)\n",
    "- Task-specific layers: Traffic(64), Duration(32), Bandwidth(64)\n",
    "- All tasks use classification heads with sparse categorical crossentropy\n",
    "- Model properly handles task-specific forward passes\n",
    "\n",
    "**2. Data Partitioning** \n",
    "- 600 clients across 3 clusters (200 clients each)\n",
    "- Supports both Equal and Dirichlet splits\n",
    "- Dirichlet α = 0.4 creates moderate non-IID distribution\n",
    "- Minimum 50 samples per client enforced\n",
    "- Sequential cluster assignment:\n",
    " - Clients 0-199 → Cluster 0\n",
    " - Clients 200-399 → Cluster 1 (Global Aggregator)\n",
    " - Clients 400-599 → Cluster 2\n",
    "\n",
    "**3. Data Preprocessing** \n",
    "- Winsorization applied at 1st-99th percentile\n",
    "- StandardScaler normalization on all features\n",
    "- Quantile-based binning (5 classes) for duration/bandwidth using log1p transform\n",
    "- Label encoding for traffic classification (5 classes)\n",
    "- Train/test split: 80/20\n",
    "\n",
    "**4. Feature Engineering** \n",
    "- High-leakage features removed (TCP flags, headers, idle/active times)\n",
    "- Identity features excluded (IPs, ports)\n",
    "- Protocol excluded (QUIC-only dataset)\n",
    "- Feature padding ensures uniform 39-dimensional input\n",
    "- Mutual information analysis identifies remaining correlated features\n",
    "\n",
    "**5. Client Implementation** \n",
    "- Multi-task training with weighted loss aggregation\n",
    "- Gradients computed via TensorFlow GradientTape\n",
    "- Adam optimizer with lr=1e-3\n",
    "- Parameter change tracking for debugging\n",
    "- Cluster-aware metadata included in responses\n",
    "\n",
    "**6. Data Quality Checks** \n",
    "- Balanced classes in all three tasks (5 classes each, ~2000 samples per class)\n",
    "- Test set: 2,500 samples (20%)\n",
    "- Training set: 10,000 samples (80%) distributed across 600 clients\n",
    "- Average ~50 samples per client (range: 48-85)\n",
    "\n",
    "### Ready for:\n",
    "- Multi-cluster hierarchical federated learning experiments\n",
    "- Cluster Head (CH) compromisation scenarios\n",
    "- Communication cost tracking and analysis\n",
    "- Byzantine attack simulations\n",
    "- Model poisoning detection\n",
    "- Privacy-preserving mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIERARCHICAL FEDERATED LEARNING IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Shutdown Ray (if running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ray shutdown complete\n"
     ]
    }
   ],
   "source": [
    "# Shutdown Ray to clear all workers and memory\n",
    "if ray.is_initialized():\n",
    " ray.shutdown()\n",
    " print(\" Ray shutdown complete\")\n",
    "else:\n",
    " print(\" Ray not running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH COMPROMISATION EXPERIMENTS\n",
    "\n",
    "## Test Plan:\n",
    "1. **Baseline (100 rounds)**: Normal training to convergence\n",
    "2. **CH Compromise After Convergence**: Train 100 rounds → Compromise CH → Continue 25 rounds (total 125)\n",
    "3. **Transient CH Compromise**: Compromise CH during training (125 rounds total)\n",
    "\n",
    "All tests use the same hierarchical architecture with 3 clusters and CH1 as global aggregator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27c. Training-Only Strategies (Save Models, No Testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training strategies ready (save models + track per-task accuracy during training)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class TrainingOnlyStrategy(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Training strategy that saves model params every round\n",
    "    AND evaluates on clients to track training accuracy\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir='trained_models', *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        self.saved_models = []\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Standard FedAvg aggregation\n",
    "        aggregated_params, metrics = super().aggregate_fit(server_round, results, failures)\n",
    "        \n",
    "        # Save model params after every round\n",
    "        model_weights = fl.common.parameters_to_ndarrays(aggregated_params)\n",
    "        save_path = os.path.join(self.save_dir, f'model_round_{server_round}.pkl')\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'round': server_round,\n",
    "                'weights': model_weights,\n",
    "                'metrics': metrics\n",
    "            }, f)\n",
    "        \n",
    "        self.saved_models.append(save_path)\n",
    "        \n",
    "        # Print training progress\n",
    "        if metrics:\n",
    "            avg_loss = metrics.get('loss', 0.0)\n",
    "            print(f\"[Round {server_round:3d}] Training Loss: {avg_loss:.4f} | Model saved\")\n",
    "        elif server_round % 20 == 0:\n",
    "            print(f\"[Round {server_round}] Model saved: {save_path}\")\n",
    "        \n",
    "        return aggregated_params, metrics\n",
    "    \n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        \"\"\"Aggregate evaluation results and print metrics\"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Per-task accuracy aggregation\n",
    "        task_totals = {\n",
    "            'traffic_accuracy': 0.0,\n",
    "            'duration_accuracy': 0.0,\n",
    "            'bandwidth_accuracy': 0.0\n",
    "        }\n",
    "        \n",
    "        for _, eval_res in results:\n",
    "            num_examples = eval_res.num_examples\n",
    "            total_loss += eval_res.loss * num_examples\n",
    "            if 'accuracy' in eval_res.metrics:\n",
    "                total_accuracy += eval_res.metrics['accuracy'] * num_examples\n",
    "            \n",
    "            # Aggregate per-task accuracies\n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                task_key = f'{task}_accuracy'\n",
    "                if task_key in eval_res.metrics:\n",
    "                    task_totals[task_key] += eval_res.metrics[task_key] * num_examples\n",
    "            \n",
    "            total_samples += num_examples\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "        avg_accuracy = total_accuracy / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Calculate per-task averages\n",
    "        aggregated_metrics = {'accuracy': avg_accuracy, 'loss': avg_loss}\n",
    "        for task_key in task_totals:\n",
    "            aggregated_metrics[task_key] = task_totals[task_key] / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        if server_round % 10 == 0 or server_round == 1:\n",
    "            print(f\"[Round {server_round:3d}] Eval - Traffic: {aggregated_metrics['traffic_accuracy']:.4f}, \"\n",
    "                  f\"Duration: {aggregated_metrics['duration_accuracy']:.4f}, \"\n",
    "                  f\"Bandwidth: {aggregated_metrics['bandwidth_accuracy']:.4f}\")\n",
    "        \n",
    "        return avg_loss, aggregated_metrics\n",
    "\n",
    "class HierarchicalTrainingOnlyStrategy(fl.server.strategy.FedAvg):\n",
    "    \"\"\"\n",
    "    Hierarchical training-only strategy (saves models, no testing)\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir='trained_models', *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_dir = save_dir\n",
    "        self.saved_models = []\n",
    "        self.global_aggregator_cluster = CFG['global_aggregator_cluster']\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def _ndarrays_weighted_average(self, param_list):\n",
    "        if not param_list:\n",
    "            return None\n",
    "        total_weight = float(sum(w for _, w in param_list))\n",
    "        if total_weight <= 0:\n",
    "            total_weight = 1.0\n",
    "        summed = [np.zeros_like(arr, dtype=arr.dtype) for arr in param_list[0][0]]\n",
    "        for arrays, w in param_list:\n",
    "            for i, arr in enumerate(arrays):\n",
    "                summed[i] = summed[i] + (arr * (w / total_weight))\n",
    "        return summed\n",
    "    \n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Tier 1: Aggregate within clusters\n",
    "        cluster_to_pairs = {}\n",
    "        for client_proxy, fit_res in results:\n",
    "            nds = fl.common.parameters_to_ndarrays(fit_res.parameters)\n",
    "            weight = getattr(fit_res, 'num_examples', None)\n",
    "            if weight is None:\n",
    "                weight = int(fit_res.metrics.get('num_examples', 1)) if hasattr(fit_res, 'metrics') else 1\n",
    "            cluster_id = int(fit_res.metrics.get('cluster_id', 0)) if hasattr(fit_res, 'metrics') else 0\n",
    "            cluster_to_pairs.setdefault(cluster_id, []).append((nds, weight))\n",
    "        \n",
    "        cluster_params = {}\n",
    "        cluster_weights = {}\n",
    "        \n",
    "        for cid, pairs in cluster_to_pairs.items():\n",
    "            if pairs:\n",
    "                cluster_params[cid] = self._ndarrays_weighted_average(pairs)\n",
    "                cluster_weights[cid] = float(sum(w for _, w in pairs))\n",
    "        \n",
    "        # Tier 2: Global aggregation at CH1\n",
    "        global_agg_cluster = self.global_aggregator_cluster\n",
    "        \n",
    "        if global_agg_cluster in cluster_params:\n",
    "            global_pairs = []\n",
    "            for cid in [0, 2]:\n",
    "                if cid in cluster_params:\n",
    "                    global_pairs.append((cluster_params[cid], cluster_weights[cid]))\n",
    "            \n",
    "            if global_agg_cluster in cluster_params:\n",
    "                global_pairs.append((cluster_params[global_agg_cluster], cluster_weights[global_agg_cluster]))\n",
    "            \n",
    "            if global_pairs:\n",
    "                global_params = self._ndarrays_weighted_average(global_pairs)\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(global_params)\n",
    "            else:\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(cluster_params[global_agg_cluster])\n",
    "        else:\n",
    "            all_pairs = [(cluster_params[cid], cluster_weights[cid]) for cid in cluster_params.keys()]\n",
    "            if all_pairs:\n",
    "                global_params = self._ndarrays_weighted_average(all_pairs)\n",
    "                aggregated_params = fl.common.ndarrays_to_parameters(global_params)\n",
    "            else:\n",
    "                return None, {}\n",
    "        \n",
    "        # Save model params after every round\n",
    "        model_weights = fl.common.parameters_to_ndarrays(aggregated_params)\n",
    "        save_path = os.path.join(self.save_dir, f'model_round_{server_round}.pkl')\n",
    "        \n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'round': server_round,\n",
    "                'weights': model_weights,\n",
    "                'cluster_params': {cid: params for cid, params in cluster_params.items()},\n",
    "                'metrics': {'participating_clusters': len(cluster_params)}\n",
    "            }, f)\n",
    "        \n",
    "        self.saved_models.append(save_path)\n",
    "        \n",
    "        # Print training progress\n",
    "        print(f\"[Round {server_round:3d}] Clusters: {len(cluster_params)} | Model saved\")\n",
    "        \n",
    "        return aggregated_params, {}\n",
    "    \n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        \"\"\"Aggregate evaluation results and print metrics\"\"\"\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Per-task accuracy aggregation\n",
    "        task_totals = {\n",
    "            'traffic_accuracy': 0.0,\n",
    "            'duration_accuracy': 0.0,\n",
    "            'bandwidth_accuracy': 0.0\n",
    "        }\n",
    "        \n",
    "        for _, eval_res in results:\n",
    "            num_examples = eval_res.num_examples\n",
    "            total_loss += eval_res.loss * num_examples\n",
    "            if 'accuracy' in eval_res.metrics:\n",
    "                total_accuracy += eval_res.metrics['accuracy'] * num_examples\n",
    "            \n",
    "            # Aggregate per-task accuracies\n",
    "            for task in ['traffic', 'duration', 'bandwidth']:\n",
    "                task_key = f'{task}_accuracy'\n",
    "                if task_key in eval_res.metrics:\n",
    "                    task_totals[task_key] += eval_res.metrics[task_key] * num_examples\n",
    "            \n",
    "            total_samples += num_examples\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0\n",
    "        avg_accuracy = total_accuracy / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Calculate per-task averages\n",
    "        aggregated_metrics = {'accuracy': avg_accuracy, 'loss': avg_loss}\n",
    "        for task_key in task_totals:\n",
    "            aggregated_metrics[task_key] = task_totals[task_key] / total_samples if total_samples > 0 else 0.0\n",
    "        \n",
    "        # Print evaluation metrics\n",
    "        if server_round % 10 == 0 or server_round == 1:\n",
    "            print(f\"[Round {server_round:3d}] Eval - Traffic: {aggregated_metrics['traffic_accuracy']:.4f}, \"\n",
    "                  f\"Duration: {aggregated_metrics['duration_accuracy']:.4f}, \"\n",
    "                  f\"Bandwidth: {aggregated_metrics['bandwidth_accuracy']:.4f}\")\n",
    "        \n",
    "        return avg_loss, aggregated_metrics\n",
    "\n",
    "print(\"Training strategies ready (save models + track per-task accuracy during training)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 1: TRAINING (Save Models Every Round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING PHASE: SAVING MODELS FOR LATER TESTING\n",
      "================================================================================\n",
      "\n",
      "Training Configuration:\n",
      "  Rounds: 125\n",
      "  Save: Model params after EVERY round\n",
      "  Testing: Done separately after training\n",
      "\n",
      "  Cluster-level: EQUAL split (for training)\n",
      "  Client-level: Dirichlet split (alpha=0.4)\n",
      "================================================================================\n",
      "\n",
      "Building client partitions for training (equal cluster split)...\n",
      "\n",
      "✓ Created 600 clients\n",
      "  Cluster split: equal\n",
      "  Client split: dirichlet\n",
      "  Sample sizes: min=50, max=92, avg=50.2\n",
      "\n",
      "  Cluster distribution:\n",
      "    Cluster 0: 200 clients, 10016 samples\n",
      "    Cluster 1: 200 clients, 10067 samples\n",
      "    Cluster 2: 200 clients, 10061 samples\n",
      "\n",
      "  Sample client label distributions:\n",
      "    Client 0 (Cluster 0): {0: 16, 1: 9, 2: 9, 3: 7, 4: 9}\n",
      "    Client 1 (Cluster 0): {0: 16, 1: 11, 2: 8, 3: 9, 4: 6}\n",
      "    Client 2 (Cluster 0): {0: 8, 1: 12, 2: 5, 3: 13, 4: 12}\n",
      " Clients created: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=125, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING 1/2: SINGLE CLUSTER\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:05:16,727\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'memory': 8194755789.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2147483648.0, 'node:__internal_head__': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1.0, 'num_gpus': 0.0}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 8 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   1] Training Loss: 1.6322 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   1] Eval - Traffic: 0.2579, Duration: 0.2558, Bandwidth: 0.1849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   2] Training Loss: 1.5866 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   3] Training Loss: 1.5515 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   4] Training Loss: 1.5214 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   5] Training Loss: 1.4950 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   6] Training Loss: 1.4711 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   7] Training Loss: 1.4477 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   8] Training Loss: 1.4251 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round   9] Training Loss: 1.4036 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  10] Training Loss: 1.3818 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  10] Eval - Traffic: 0.5389, Duration: 0.5292, Bandwidth: 0.5156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  11] Training Loss: 1.3624 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  12] Training Loss: 1.3431 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 13]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  13] Training Loss: 1.3237 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 14]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  14] Training Loss: 1.3056 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 15]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  15] Training Loss: 1.2870 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 16]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  16] Training Loss: 1.2696 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 17]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  17] Training Loss: 1.2526 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 18]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  18] Training Loss: 1.2363 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 19]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  19] Training Loss: 1.2201 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 20]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  20] Training Loss: 1.2043 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 21]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  20] Eval - Traffic: 0.5852, Duration: 0.5736, Bandwidth: 0.5974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  21] Training Loss: 1.1901 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 22]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  22] Training Loss: 1.1746 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 23]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  23] Training Loss: 1.1600 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 24]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  24] Training Loss: 1.1457 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 25]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  25] Training Loss: 1.1324 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 26]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  26] Training Loss: 1.1192 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 27]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  27] Training Loss: 1.1057 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 28]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  28] Training Loss: 1.0921 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 29]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  29] Training Loss: 1.0811 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 30]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  30] Training Loss: 1.0678 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 31]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  30] Eval - Traffic: 0.6120, Duration: 0.6101, Bandwidth: 0.6679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  31] Training Loss: 1.0559 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 32]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  32] Training Loss: 1.0428 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 33]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  33] Training Loss: 1.0325 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 34]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  34] Training Loss: 1.0206 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 35]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  35] Training Loss: 1.0095 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 36]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  36] Training Loss: 0.9986 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 37]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  37] Training Loss: 0.9877 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 38]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  38] Training Loss: 0.9779 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 39]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  39] Training Loss: 0.9677 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 40]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  40] Training Loss: 0.9560 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 41]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  40] Eval - Traffic: 0.6347, Duration: 0.6429, Bandwidth: 0.7485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  41] Training Loss: 0.9461 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 42]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  42] Training Loss: 0.9355 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 43]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  43] Training Loss: 0.9273 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 44]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  44] Training Loss: 0.9172 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 45]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  45] Training Loss: 0.9084 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 46]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  46] Training Loss: 0.8996 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 47]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  47] Training Loss: 0.8902 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 48]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  48] Training Loss: 0.8814 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 49]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  49] Training Loss: 0.8749 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 50]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  50] Training Loss: 0.8680 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 51]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  50] Eval - Traffic: 0.6459, Duration: 0.6771, Bandwidth: 0.7911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  51] Training Loss: 0.8584 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 52]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  52] Training Loss: 0.8515 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 53]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  53] Training Loss: 0.8446 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 54]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  54] Training Loss: 0.8383 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 55]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  55] Training Loss: 0.8299 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 56]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  56] Training Loss: 0.8234 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 57]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  57] Training Loss: 0.8161 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 58]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  58] Training Loss: 0.8107 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 59]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  59] Training Loss: 0.8034 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 60]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  60] Training Loss: 0.7985 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 61]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  60] Eval - Traffic: 0.6545, Duration: 0.7176, Bandwidth: 0.8120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  61] Training Loss: 0.7927 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 62]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  62] Training Loss: 0.7874 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 63]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  63] Training Loss: 0.7802 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 64]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 600 clients (out of 600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Round  64] Training Loss: 0.7751 | Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 600 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 65]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 600 clients (out of 600)\n",
      "python(98635) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/server/server.py:353\u001b[39m, in \u001b[36mfit_clients\u001b[39m\u001b[34m(client_instructions, max_workers, timeout, group_id)\u001b[39m\n\u001b[32m    349\u001b[39m     submitted_fs = {\n\u001b[32m    350\u001b[39m         executor.submit(fit_client, client_proxy, ins, timeout, group_id)\n\u001b[32m    351\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m client_proxy, ins \u001b[38;5;129;01min\u001b[39;00m client_instructions\n\u001b[32m    352\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     finished_fs, _ = \u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubmitted_fs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Handled in the respective communication stack\u001b[39;49;00m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Gather results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/concurrent/futures/_base.py:305\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    303\u001b[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Single cluster training\u001b[39;00m\n\u001b[32m     89\u001b[39m strategy_single = TrainingOnlyStrategy(\n\u001b[32m     90\u001b[39m     save_dir=\u001b[33m'\u001b[39m\u001b[33mtrained_models/single_cluster\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     91\u001b[39m     fraction_fit=CFG[\u001b[33m'\u001b[39m\u001b[33mclient_frac\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     97\u001b[39m     evaluate_metrics_aggregation_fn=aggregate_metrics\n\u001b[32m     98\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m history_single = \u001b[43mfl\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimulation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_simulation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_clients\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mServerConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_ROUNDS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy_single\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_resources\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_cpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnum_gpus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSingle cluster training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(strategy_single.saved_models)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/simulation/legacy_app.py:361\u001b[39m, in \u001b[36mstart_simulation\u001b[39m\u001b[34m(client_fn, num_clients, clients_ids, client_resources, server, config, strategy, client_manager, ray_init_args, keep_initialised, actor_type, actor_kwargs, actor_scheduling)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    360\u001b[39m     \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     hist = \u001b[43mrun_fl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitialized_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitialized_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    366\u001b[39m     log(ERROR, ex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/server/server.py:492\u001b[39m, in \u001b[36mrun_fl\u001b[39m\u001b[34m(server, config)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_fl\u001b[39m(\n\u001b[32m    488\u001b[39m     server: Server,\n\u001b[32m    489\u001b[39m     config: ServerConfig,\n\u001b[32m    490\u001b[39m ) -> History:\n\u001b[32m    491\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Train a model on the given server and return the History object.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     hist, elapsed_time = \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mround_timeout\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     log(INFO, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    497\u001b[39m     log(INFO, \u001b[33m\"\u001b[39m\u001b[33m[SUMMARY]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/server/server.py:115\u001b[39m, in \u001b[36mServer.fit\u001b[39m\u001b[34m(self, num_rounds, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m log(INFO, \u001b[33m\"\u001b[39m\u001b[33m[ROUND \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m, current_round)\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Train model and replace previous global model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m res_fit = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_round\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res_fit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    120\u001b[39m     parameters_prime, fit_metrics, _ = res_fit  \u001b[38;5;66;03m# fit_metrics_aggregated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/server/server.py:234\u001b[39m, in \u001b[36mServer.fit_round\u001b[39m\u001b[34m(self, server_round, timeout)\u001b[39m\n\u001b[32m    226\u001b[39m log(\n\u001b[32m    227\u001b[39m     INFO,\n\u001b[32m    228\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfigure_fit: strategy sampled \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m clients (out of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mlen\u001b[39m(client_instructions),\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m._client_manager.num_available(),\n\u001b[32m    231\u001b[39m )\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Collect `fit` results from all clients participating in this round\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m results, failures = \u001b[43mfit_clients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_instructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m log(\n\u001b[32m    241\u001b[39m     INFO,\n\u001b[32m    242\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maggregate_fit: received \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m results and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m failures\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    243\u001b[39m     \u001b[38;5;28mlen\u001b[39m(results),\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mlen\u001b[39m(failures),\n\u001b[32m    245\u001b[39m )\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Aggregate training results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/CESNET Zoo/cesnet/lib/python3.12/site-packages/flwr/server/server.py:348\u001b[39m, in \u001b[36mfit_clients\u001b[39m\u001b[34m(client_instructions, max_workers, timeout, group_id)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_clients\u001b[39m(\n\u001b[32m    342\u001b[39m     client_instructions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[ClientProxy, FitIns]],\n\u001b[32m    343\u001b[39m     max_workers: Optional[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m    344\u001b[39m     timeout: Optional[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[32m    345\u001b[39m     group_id: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m    346\u001b[39m ) -> FitResultsAndFailures:\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Refine parameters concurrently on all selected clients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m    349\u001b[39m         submitted_fs = {\n\u001b[32m    350\u001b[39m             executor.submit(fit_client, client_proxy, ins, timeout, group_id)\n\u001b[32m    351\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m client_proxy, ins \u001b[38;5;129;01min\u001b[39;00m client_instructions\n\u001b[32m    352\u001b[39m         }\n\u001b[32m    353\u001b[39m         finished_fs, _ = concurrent.futures.wait(\n\u001b[32m    354\u001b[39m             fs=submitted_fs,\n\u001b[32m    355\u001b[39m             timeout=\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Handled in the respective communication stack\u001b[39;00m\n\u001b[32m    356\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/concurrent/futures/thread.py:239\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/threading.py:1149\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1151\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1153\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.12/threading.py:1169\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1170\u001b[39m         lock.release()\n\u001b[32m   1171\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "TRAINING_ROUNDS = 125\n",
    "\n",
    "# Shutdown Ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING PHASE: SAVING MODELS FOR LATER TESTING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Rounds: {TRAINING_ROUNDS}\")\n",
    "print(f\"  Save: Model params after EVERY round\")\n",
    "print(f\"  Testing: Done separately after training\")\n",
    "print(f\"\\n  Cluster-level: EQUAL split (for training)\")\n",
    "print(f\"  Client-level: Dirichlet split (alpha={CFG['alpha_client']})\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build client partitions with EQUAL cluster split for training\n",
    "print(\"\\nBuilding client partitions for training (equal cluster split)...\")\n",
    "client_indices_flat, client_index_to_cluster = build_client_partitions(\n",
    "    cluster_split='equal',\n",
    "    client_split='dirichlet',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create client objects\n",
    "clients = []\n",
    "for i, indices in enumerate(client_indices_flat):\n",
    "    X_traffic_client = X_traffic_train[indices]\n",
    "    X_duration_client = X_duration_train[indices]\n",
    "    X_bandwidth_client = X_bandwidth_train[indices]\n",
    "    \n",
    "    y_traffic_client = y_traf_train[indices]\n",
    "    y_duration_client = y_dur_train[indices]\n",
    "    y_bandwidth_client = y_bw_train[indices]\n",
    "    \n",
    "    client_data_dict = {\n",
    "        'traffic': (X_traffic_client.astype(np.float32), y_traffic_client),\n",
    "        'duration': (X_duration_client.astype(np.float32), y_duration_client),\n",
    "        'bandwidth': (X_bandwidth_client.astype(np.float32), y_bandwidth_client)\n",
    "    }\n",
    "    \n",
    "    cluster_id = client_index_to_cluster[i]\n",
    "    clients.append(ClientData(client_data_dict, cluster_id))\n",
    "\n",
    "print(f\" Clients created: {len(clients)}\")\n",
    "\n",
    "# Prepare for training\n",
    "max_dim = max(in_dims.values())\n",
    "in_dims_uniform = {\n",
    "    'traffic': max_dim,\n",
    "    'duration': max_dim,\n",
    "    'bandwidth': max_dim\n",
    "}\n",
    "\n",
    "def client_fn(context: fl.common.Context) -> fl.client.Client:\n",
    "    tf.random.set_seed(seed)\n",
    "    client_id = hash(context.node_id) % len(clients)\n",
    "    client_obj = clients[client_id]\n",
    "    client_data = client_obj.ds\n",
    "    cluster_id = client_obj.cluster_id\n",
    "    \n",
    "    model = FedMTLModel(in_dims_uniform, n_classes, dropout=0.1)\n",
    "    model.build_all(max_dim)\n",
    "    \n",
    "    numpy_client = MTLFlowerClient(model, client_data, CFG, cluster_id)\n",
    "    return numpy_client.to_client()\n",
    "\n",
    "# Create global model template\n",
    "global_model_template = FedMTLModel(in_dims_uniform, n_classes, dropout=0.1)\n",
    "global_model_template.build_all(max_dim)\n",
    "\n",
    "def aggregate_metrics(metrics):\n",
    "    aggregated = {}\n",
    "    for num_examples, client_metrics in metrics:\n",
    "        for metric_name, metric_value in client_metrics.items():\n",
    "            if metric_name not in aggregated:\n",
    "                aggregated[metric_name] = []\n",
    "            aggregated[metric_name].append(metric_value)\n",
    "    for metric_name in aggregated:\n",
    "        aggregated[metric_name] = np.mean(aggregated[metric_name])\n",
    "    return aggregated\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 1/2: SINGLE CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Single cluster training\n",
    "strategy_single = TrainingOnlyStrategy(\n",
    "    save_dir='trained_models/single_cluster',\n",
    "    fraction_fit=CFG['client_frac'],\n",
    "    fraction_evaluate=CFG['client_frac'],\n",
    "    min_fit_clients=10,\n",
    "    min_available_clients=10,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(global_model_template.get_weights()),\n",
    "    fit_metrics_aggregation_fn=aggregate_metrics,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_metrics\n",
    ")\n",
    "\n",
    "history_single = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=len(clients),\n",
    "    config=fl.server.ServerConfig(num_rounds=TRAINING_ROUNDS),\n",
    "    strategy=strategy_single,\n",
    "    client_resources={'num_cpus': 1.0, 'num_gpus': 0.0},\n",
    ")\n",
    "\n",
    "print(f\"\\nSingle cluster training complete!\")\n",
    "print(f\"  Saved {len(strategy_single.saved_models)} models\")\n",
    "\n",
    "# Shutdown Ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 2/2: THREE CLUSTER HIERARCHICAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Hierarchical training\n",
    "strategy_hierarchical = HierarchicalTrainingOnlyStrategy(\n",
    "    save_dir='trained_models/hierarchical_equal',\n",
    "    fraction_fit=CFG['client_frac'],\n",
    "    fraction_evaluate=CFG['client_frac'],\n",
    "    min_fit_clients=10,\n",
    "    min_available_clients=10,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(global_model_template.get_weights()),\n",
    "    fit_metrics_aggregation_fn=aggregate_metrics,\n",
    "    evaluate_metrics_aggregation_fn=aggregate_metrics\n",
    ")\n",
    "\n",
    "history_hierarchical = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=len(clients),\n",
    "    config=fl.server.ServerConfig(num_rounds=TRAINING_ROUNDS),\n",
    "    strategy=strategy_hierarchical,\n",
    "    client_resources={'num_cpus': 1.0, 'num_gpus': 0.0},\n",
    ")\n",
    "\n",
    "print(f\"\\nHierarchical training complete!\")\n",
    "print(f\"  Saved {len(strategy_hierarchical.saved_models)} models\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING PHASE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal models saved:\")\n",
    "print(f\"  Single cluster: {len(strategy_single.saved_models)} models\")\n",
    "print(f\"  Hierarchical: {len(strategy_hierarchical.saved_models)} models\")\n",
    "print(f\"  TOTAL: {len(strategy_single.saved_models) + len(strategy_hierarchical.saved_models)} models\")\n",
    "print(f\"\\nReady for testing phase!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy Visualization (from history object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING ACCURACY CURVES (During Training)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if training history objects are available\n",
    "if 'history_single' not in locals() or 'history_hierarchical' not in locals():\n",
    "    print(\"⚠️ Training history not available. Run training cells first (cell 45).\")\n",
    "else:\n",
    "    # Create figure with 2 subplots (single cluster vs hierarchical)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    experiments = [\n",
    "        ('history_single', history_single, 'Single Cluster', axes[0]),\n",
    "        ('history_hierarchical', history_hierarchical, 'Hierarchical (Equal Split)', axes[1])\n",
    "    ]\n",
    "    \n",
    "    for exp_name, history, title, ax in experiments:\n",
    "        # Check if evaluation metrics are available\n",
    "        if not hasattr(history, 'metrics_distributed') or not history.metrics_distributed:\n",
    "            ax.text(0.5, 0.5, f'No evaluation metrics available\\nfor {exp_name}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "            ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        metrics = history.metrics_distributed\n",
    "        \n",
    "        # Check for per-task accuracy metrics\n",
    "        required_metrics = ['traffic_accuracy', 'duration_accuracy', 'bandwidth_accuracy']\n",
    "        missing = [m for m in required_metrics if m not in metrics]\n",
    "        \n",
    "        if missing:\n",
    "            ax.text(0.5, 0.5, f'Missing metrics:\\n{\", \".join(missing)}', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "            ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "        \n",
    "        # Extract rounds and accuracies\n",
    "        rounds = [r for r, _ in metrics['traffic_accuracy']]\n",
    "        traffic_acc = [float(v) for _, v in metrics['traffic_accuracy']]\n",
    "        duration_acc = [float(v) for _, v in metrics['duration_accuracy']]\n",
    "        bandwidth_acc = [float(v) for _, v in metrics['bandwidth_accuracy']]\n",
    "        \n",
    "        # Plot per-task accuracies\n",
    "        ax.plot(rounds, traffic_acc, color='green', label='Traffic Classification', \n",
    "               linewidth=2, marker='o', markersize=4, markevery=max(1, len(rounds)//10))\n",
    "        ax.plot(rounds, duration_acc, color='blue', label='Flow Duration Classification', \n",
    "               linewidth=2, marker='s', markersize=4, markevery=max(1, len(rounds)//10))\n",
    "        ax.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "               linewidth=2, marker='^', markersize=4, markevery=max(1, len(rounds)//10))\n",
    "        \n",
    "        # Format subplot\n",
    "        ax.set_xlabel('Rounds', fontsize=11)\n",
    "        ax.set_ylabel('Accuracy', fontsize=11)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Set y-axis limits with padding\n",
    "        all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "        y_min = max(0.0, min(all_acc) - 0.05)\n",
    "        y_max = min(1.0, max(all_acc) + 0.05)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(f\"  Rounds: {len(rounds)}\")\n",
    "        print(f\"  Final Accuracies:\")\n",
    "        print(f\"    Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "        print(f\"    Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "        print(f\"    Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "        print(f\"  Best Accuracies:\")\n",
    "        print(f\"    Traffic:   {max(traffic_acc):.4f} at Round {rounds[np.argmax(traffic_acc)]}\")\n",
    "        print(f\"    Duration:  {max(duration_acc):.4f} at Round {rounds[np.argmax(duration_acc)]}\")\n",
    "        print(f\"    Bandwidth: {max(bandwidth_acc):.4f} at Round {rounds[np.argmax(bandwidth_acc)]}\")\n",
    "    \n",
    "    plt.suptitle('Training Accuracy Curves (Evaluated on Client Data During Training)', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Note: These accuracies are from evaluating the global model on client data\")\n",
    "    print(\"during training (not on the separate test set).\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHASE 2: TESTING (Evaluate Saved Models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data_partitions(cluster_split='equal'):\n",
    "    \"\"\"\n",
    "    Create test data partitions for evaluation\n",
    "    \n",
    "    Args:\n",
    "        cluster_split: 'equal' or 'dirichlet' - how to split test data among clusters\n",
    "    \n",
    "    Returns:\n",
    "        test_data_dict: Dictionary with cluster-level and global test data\n",
    "    \"\"\"\n",
    "    n_clusters = CFG['n_clusters']\n",
    "    alpha_cluster = CFG['alpha_cluster']\n",
    "    rng = np.random.default_rng(seed + 1000)  # Different seed for test\n",
    "    \n",
    "    test_indices = np.arange(len(y_traf_test))\n",
    "    labels_test = np.unique(y_traf_test)\n",
    "    \n",
    "    if cluster_split == 'equal':\n",
    "        # Equal split: each cluster gets 1/3 of test data\n",
    "        samples_per_cluster = len(test_indices) // n_clusters\n",
    "        cluster_test_indices = []\n",
    "        for cluster_id in range(n_clusters):\n",
    "            start_idx = cluster_id * samples_per_cluster\n",
    "            end_idx = start_idx + samples_per_cluster if cluster_id < n_clusters - 1 else len(test_indices)\n",
    "            cluster_test_indices.append(test_indices[start_idx:end_idx])\n",
    "    \n",
    "    elif cluster_split == 'dirichlet':\n",
    "        # Dirichlet split: non-IID distribution among clusters\n",
    "        cluster_bins = [[] for _ in range(n_clusters)]\n",
    "        label_indices_test = {}\n",
    "        \n",
    "        for lbl in labels_test:\n",
    "            label_indices_test[lbl] = test_indices[y_traf_test == lbl]\n",
    "        \n",
    "        for lbl in labels_test:\n",
    "            idxs = label_indices_test[lbl]\n",
    "            rng.shuffle(idxs)\n",
    "            proportions = rng.dirichlet([alpha_cluster] * n_clusters)\n",
    "            cuts = (np.cumsum(proportions) * len(idxs)).astype(int)\n",
    "            parts = np.split(idxs, cuts[:-1])\n",
    "            \n",
    "            for cluster_id, part in enumerate(parts):\n",
    "                cluster_bins[cluster_id].extend(part.tolist())\n",
    "        \n",
    "        cluster_test_indices = [np.array(sorted(set(cluster_bins[i]))) for i in range(n_clusters)]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cluster_split: {cluster_split}\")\n",
    "    \n",
    "    # Create test data dictionary\n",
    "    test_data_dict = {\n",
    "        'global': {\n",
    "            'X_traffic': X_traffic_test.astype(np.float32),\n",
    "            'y_traffic': y_traf_test.astype(int),\n",
    "            'X_duration': X_duration_test.astype(np.float32),\n",
    "            'y_duration': y_dur_test.astype(int),\n",
    "            'X_bandwidth': X_bandwidth_test.astype(np.float32),\n",
    "            'y_bandwidth': y_bw_test.astype(int)\n",
    "        },\n",
    "        'clusters': {}\n",
    "    }\n",
    "    \n",
    "    # Add per-cluster test data\n",
    "    for cluster_id in range(n_clusters):\n",
    "        indices = cluster_test_indices[cluster_id]\n",
    "        test_data_dict['clusters'][cluster_id] = {\n",
    "            'X_traffic': X_traffic_test[indices].astype(np.float32),\n",
    "            'y_traffic': y_traf_test[indices].astype(int),\n",
    "            'X_duration': X_duration_test[indices].astype(np.float32),\n",
    "            'y_duration': y_dur_test[indices].astype(int),\n",
    "            'X_bandwidth': X_bandwidth_test[indices].astype(np.float32),\n",
    "            'y_bandwidth': y_bw_test[indices].astype(int),\n",
    "            'size': len(indices)\n",
    "        }\n",
    "    \n",
    "    print(f\"\\nTest data partitioned ({cluster_split} split):\")\n",
    "    print(f\"  Global: {len(test_indices)} samples\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        print(f\"  Cluster {cluster_id}: {test_data_dict['clusters'][cluster_id]['size']} samples\")\n",
    "    \n",
    "    return test_data_dict\n",
    "\n",
    "def evaluate_model_on_test(model_weights, test_data_dict, model_type='global', cluster_id=None):\n",
    "    \"\"\"\n",
    "    Evaluate saved model on test data\n",
    "    \n",
    "    Args:\n",
    "        model_weights: Saved model weights\n",
    "        test_data_dict: Test data dictionary\n",
    "        model_type: 'global' or 'cluster'\n",
    "        cluster_id: Which cluster to test (if model_type='cluster')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of accuracies\n",
    "    \"\"\"\n",
    "    # Create model and load weights\n",
    "    model = FedMTLModel(in_dims_uniform, n_classes, dropout=0.1)\n",
    "    model.build_all(max_dim)\n",
    "    model.set_weights(model_weights)\n",
    "    \n",
    "    # Get test data\n",
    "    if model_type == 'global':\n",
    "        test_data = test_data_dict['global']\n",
    "    elif model_type == 'cluster' and cluster_id is not None:\n",
    "        test_data = test_data_dict['clusters'][cluster_id]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type or missing cluster_id\")\n",
    "    \n",
    "    # Evaluate on all tasks\n",
    "    accuracies = {}\n",
    "    for task in ['traffic', 'duration', 'bandwidth']:\n",
    "        X_test = test_data[f'X_{task}']\n",
    "        y_test = test_data[f'y_{task}']\n",
    "        \n",
    "        logits = model(X_test, task=task, training=False)\n",
    "        predictions = tf.argmax(logits, axis=1).numpy()\n",
    "        \n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        accuracies[f'{task}_accuracy'] = float(accuracy)\n",
    "    \n",
    "    accuracies['overall_accuracy'] = np.mean([\n",
    "        accuracies['traffic_accuracy'],\n",
    "        accuracies['duration_accuracy'],\n",
    "        accuracies['bandwidth_accuracy']\n",
    "    ])\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "print(\"Test data partitioning and evaluation functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. Test Evaluation: Single Cluster & Three Cluster Equal Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING PHASE: EVALUATE SAVED MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEvaluating first 100 rounds only (convergence period)\")\n",
    "print(\"Testing with EQUAL cluster split\\n\")\n",
    "\n",
    "# Create test data with equal split\n",
    "test_data_equal = create_test_data_partitions(cluster_split='equal')\n",
    "\n",
    "# Store results\n",
    "test_results = {\n",
    "    'single_cluster': [],\n",
    "    'hierarchical_equal': []\n",
    "}\n",
    "\n",
    "# Evaluate single cluster models (rounds 1-100)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING: SINGLE CLUSTER MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for round_num in range(1, 101):\n",
    "    model_path = f'trained_models/single_cluster/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Evaluate on global test data\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    test_results['single_cluster'].append(accuracies)\n",
    "    \n",
    "    if round_num % 10 == 0 or round_num == 1:\n",
    "        print(f\"[Round {round_num:3d}/100] Traffic: {accuracies['traffic_accuracy']:.4f}, \"\n",
    "              f\"Duration: {accuracies['duration_accuracy']:.4f}, \"\n",
    "              f\"Bandwidth: {accuracies['bandwidth_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Single cluster evaluation complete: {len(test_results['single_cluster'])} rounds\")\n",
    "\n",
    "# Evaluate hierarchical models (rounds 1-100)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING: HIERARCHICAL MODELS (Equal Cluster Split)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for round_num in range(1, 101):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Evaluate on global test data\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    test_results['hierarchical_equal'].append(accuracies)\n",
    "    \n",
    "    if round_num % 10 == 0 or round_num == 1:\n",
    "        print(f\"[Round {round_num:3d}/100] Traffic: {accuracies['traffic_accuracy']:.4f}, \"\n",
    "              f\"Duration: {accuracies['duration_accuracy']:.4f}, \"\n",
    "              f\"Bandwidth: {accuracies['bandwidth_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Hierarchical evaluation complete: {len(test_results['hierarchical_equal'])} rounds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults stored:\")\n",
    "print(f\"  test_results['single_cluster']: {len(test_results['single_cluster'])} rounds\")\n",
    "print(f\"  test_results['hierarchical_equal']: {len(test_results['hierarchical_equal'])} rounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 29. Test Evaluation: Three Cluster with Dirichlet Split (Per-Cluster Graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TESTING WITH DIRICHLET CLUSTER SPLIT (Per-Cluster Evaluation)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEvaluating hierarchical models on Dirichlet cluster split\")\n",
    "print(\"Shows per-cluster performance when clusters have non-IID data\\n\")\n",
    "\n",
    "# Create test data with dirichlet split\n",
    "test_data_dirichlet = create_test_data_partitions(cluster_split='dirichlet')\n",
    "\n",
    "# Store results\n",
    "test_results['hierarchical_dirichlet_global'] = []\n",
    "test_results['hierarchical_dirichlet_per_cluster'] = {\n",
    "    0: [],\n",
    "    1: [],\n",
    "    2: []\n",
    "}\n",
    "\n",
    "# Evaluate hierarchical models (rounds 1-100)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING: HIERARCHICAL MODELS (Dirichlet Cluster Split)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for round_num in range(1, 101):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global evaluation\n",
    "    accuracies_global = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='global')\n",
    "    accuracies_global['round'] = round_num\n",
    "    test_results['hierarchical_dirichlet_global'].append(accuracies_global)\n",
    "    \n",
    "    # Per-cluster evaluation\n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        accuracies_cluster = evaluate_model_on_test(saved['weights'], test_data_dirichlet, \n",
    "                                                     model_type='cluster', cluster_id=cluster_id)\n",
    "        accuracies_cluster['round'] = round_num\n",
    "        test_results['hierarchical_dirichlet_per_cluster'][cluster_id].append(accuracies_cluster)\n",
    "    \n",
    "    if round_num % 10 == 0 or round_num == 1:\n",
    "        print(f\"[Round {round_num:3d}/100] Global - Traffic: {accuracies_global['traffic_accuracy']:.4f}\")\n",
    "        for cluster_id in range(CFG['n_clusters']):\n",
    "            acc = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][-1]\n",
    "            print(f\"               Cluster {cluster_id} - Traffic: {acc['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Hierarchical Dirichlet evaluation complete:\")\n",
    "print(f\"  Global: {len(test_results['hierarchical_dirichlet_global'])} rounds\")\n",
    "print(f\"  Per-cluster: {len(test_results['hierarchical_dirichlet_per_cluster'][0])} rounds each\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTest results available:\")\n",
    "print(f\"  test_results['single_cluster']\")\n",
    "print(f\"  test_results['hierarchical_equal']\")\n",
    "print(f\"  test_results['hierarchical_dirichlet_global']\")\n",
    "print(f\"  test_results['hierarchical_dirichlet_per_cluster'][0/1/2]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. CH Compromise After Convergence (Testing Phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CH COMPROMISE AFTER CONVERGENCE (Testing Phase)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nScenario: CH0 compromised at round 111, detected at round 112\")\n",
    "print(\"Uses trained models from rounds 1-100, then extends to 125\")\n",
    "print(\"Testing with EQUAL and DIRICHLET cluster splits\\n\")\n",
    "\n",
    "# Store compromise results (global and per-cluster)\n",
    "test_results['compromise_after_convergence'] = []\n",
    "test_results['compromise_after_convergence_per_cluster_equal'] = {0: [], 1: [], 2: []}\n",
    "test_results['compromise_after_convergence_per_cluster_dirichlet'] = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Create test data partitions for both splits (if not already created)\n",
    "if 'test_data_equal' not in globals():\n",
    "    test_data_equal = create_test_data_partitions(cluster_split='equal')\n",
    "if 'test_data_dirichlet' not in globals():\n",
    "    test_data_dirichlet = create_test_data_partitions(cluster_split='dirichlet')\n",
    "\n",
    "print(\"Testing rounds 1-110: Normal operation\")\n",
    "for round_num in range(1, 111):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global evaluation\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'normal'\n",
    "    test_results['compromise_after_convergence'].append(accuracies)\n",
    "    \n",
    "    # Per-cluster evaluation (equal split)\n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_cluster = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster['round'] = round_num\n",
    "        acc_cluster['phase'] = 'normal'\n",
    "        test_results['compromise_after_convergence_per_cluster_equal'][cluster_id].append(acc_cluster)\n",
    "    \n",
    "    # Per-cluster evaluation (dirichlet split)\n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_cluster = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster['round'] = round_num\n",
    "        acc_cluster['phase'] = 'normal'\n",
    "        test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id].append(acc_cluster)\n",
    "    \n",
    "    if round_num % 10 == 0 or round_num == 1:\n",
    "        print(f\"[Round {round_num:3d}/110] Normal - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRound 111: CH0 compromised (but not detected yet)\")\n",
    "model_path = f'trained_models/hierarchical_equal/model_round_111.pkl'\n",
    "with open(model_path, 'rb') as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "# Global + per-cluster\n",
    "accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "accuracies['round'] = 111\n",
    "accuracies['phase'] = 'compromised'\n",
    "test_results['compromise_after_convergence'].append(accuracies)\n",
    "\n",
    "for cluster_id in range(CFG['n_clusters']):\n",
    "    acc_cluster_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "    acc_cluster_eq['round'] = 111\n",
    "    acc_cluster_eq['phase'] = 'compromised'\n",
    "    test_results['compromise_after_convergence_per_cluster_equal'][cluster_id].append(acc_cluster_eq)\n",
    "    \n",
    "    acc_cluster_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "    acc_cluster_dir['round'] = 111\n",
    "    acc_cluster_dir['phase'] = 'compromised'\n",
    "    test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id].append(acc_cluster_dir)\n",
    "\n",
    "print(f\"[Round 111] Compromised - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRound 112: Compromise detected, D&R-E phase begins\")\n",
    "print(\"Rounds 112-118: D&R-E Phase (7 rounds) - CH0 offline, cluster excluded\")\n",
    "\n",
    "for round_num in range(112, 119):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'dre'\n",
    "    test_results['compromise_after_convergence'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_cluster_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_eq['round'] = round_num\n",
    "        acc_cluster_eq['phase'] = 'dre'\n",
    "        test_results['compromise_after_convergence_per_cluster_equal'][cluster_id].append(acc_cluster_eq)\n",
    "        \n",
    "        acc_cluster_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_dir['round'] = round_num\n",
    "        acc_cluster_dir['phase'] = 'dre'\n",
    "        test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id].append(acc_cluster_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] D&R-E - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRounds 119-121: Continuity Phase (3 rounds) - Gradual re-entry\")\n",
    "continuity_rates = {119: 0.30, 120: 0.70, 121: 1.00}\n",
    "\n",
    "for round_num in range(119, 122):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'continuity'\n",
    "    accuracies['participation_rate'] = continuity_rates[round_num]\n",
    "    test_results['compromise_after_convergence'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_cluster_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_eq['round'] = round_num\n",
    "        acc_cluster_eq['phase'] = 'continuity'\n",
    "        acc_cluster_eq['participation_rate'] = continuity_rates[round_num]\n",
    "        test_results['compromise_after_convergence_per_cluster_equal'][cluster_id].append(acc_cluster_eq)\n",
    "        \n",
    "        acc_cluster_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_dir['round'] = round_num\n",
    "        acc_cluster_dir['phase'] = 'continuity'\n",
    "        acc_cluster_dir['participation_rate'] = continuity_rates[round_num]\n",
    "        test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id].append(acc_cluster_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] Continuity ({int(continuity_rates[round_num]*100)}%) - \"\n",
    "          f\"Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRounds 122-125: Re-stabilization Phase\")\n",
    "for round_num in range(122, 126):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'stabilization'\n",
    "    test_results['compromise_after_convergence'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_cluster_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_eq['round'] = round_num\n",
    "        acc_cluster_eq['phase'] = 'stabilization'\n",
    "        test_results['compromise_after_convergence_per_cluster_equal'][cluster_id].append(acc_cluster_eq)\n",
    "        \n",
    "        acc_cluster_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_cluster_dir['round'] = round_num\n",
    "        acc_cluster_dir['phase'] = 'stabilization'\n",
    "        test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id].append(acc_cluster_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] Stabilization - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ CH Compromise After Convergence complete: {len(test_results['compromise_after_convergence'])} rounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. Transient CH Compromise (Testing Phase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRANSIENT CH COMPROMISE (Testing Phase)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nScenario: CH0 compromised at round 10, detected at round 11\")\n",
    "print(\"Testing 30 rounds total with early compromise\")\n",
    "print(\"Testing with EQUAL and DIRICHLET cluster splits\\n\")\n",
    "\n",
    "# Store transient results (global and per-cluster)\n",
    "test_results['transient_compromise'] = []\n",
    "test_results['transient_compromise_per_cluster_equal'] = {0: [], 1: [], 2: []}\n",
    "test_results['transient_compromise_per_cluster_dirichlet'] = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Create test data partitions (if not already created)\n",
    "if 'test_data_equal' not in globals():\n",
    "    test_data_equal = create_test_data_partitions(cluster_split='equal')\n",
    "if 'test_data_dirichlet' not in globals():\n",
    "    test_data_dirichlet = create_test_data_partitions(cluster_split='dirichlet')\n",
    "\n",
    "print(\"Testing rounds 1-9: Normal operation\")\n",
    "for round_num in range(1, 10):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'normal'\n",
    "    test_results['transient_compromise'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_eq['round'] = round_num\n",
    "        acc_eq['phase'] = 'normal'\n",
    "        test_results['transient_compromise_per_cluster_equal'][cluster_id].append(acc_eq)\n",
    "        \n",
    "        acc_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_dir['round'] = round_num\n",
    "        acc_dir['phase'] = 'normal'\n",
    "        test_results['transient_compromise_per_cluster_dirichlet'][cluster_id].append(acc_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] Normal - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRound 10: CH0 compromised (but not detected yet)\")\n",
    "model_path = f'trained_models/hierarchical_equal/model_round_10.pkl'\n",
    "with open(model_path, 'rb') as f:\n",
    "    saved = pickle.load(f)\n",
    "\n",
    "# Global + per-cluster\n",
    "accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "accuracies['round'] = 10\n",
    "accuracies['phase'] = 'compromised'\n",
    "test_results['transient_compromise'].append(accuracies)\n",
    "\n",
    "for cluster_id in range(CFG['n_clusters']):\n",
    "    acc_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "    acc_eq['round'] = 10\n",
    "    acc_eq['phase'] = 'compromised'\n",
    "    test_results['transient_compromise_per_cluster_equal'][cluster_id].append(acc_eq)\n",
    "    \n",
    "    acc_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "    acc_dir['round'] = 10\n",
    "    acc_dir['phase'] = 'compromised'\n",
    "    test_results['transient_compromise_per_cluster_dirichlet'][cluster_id].append(acc_dir)\n",
    "\n",
    "print(f\"[Round 10] Compromised - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRound 11: Compromise detected, D&R-E phase begins\")\n",
    "print(\"Rounds 11-17: D&R-E Phase (7 rounds) - CH0 offline, cluster excluded\")\n",
    "\n",
    "for round_num in range(11, 18):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'dre'\n",
    "    test_results['transient_compromise'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_eq['round'] = round_num\n",
    "        acc_eq['phase'] = 'dre'\n",
    "        test_results['transient_compromise_per_cluster_equal'][cluster_id].append(acc_eq)\n",
    "        \n",
    "        acc_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_dir['round'] = round_num\n",
    "        acc_dir['phase'] = 'dre'\n",
    "        test_results['transient_compromise_per_cluster_dirichlet'][cluster_id].append(acc_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] D&R-E - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRounds 18-20: Continuity Phase (3 rounds) - Gradual re-entry\")\n",
    "continuity_rates = {18: 0.30, 19: 0.70, 20: 1.00}\n",
    "\n",
    "for round_num in range(18, 21):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'continuity'\n",
    "    accuracies['participation_rate'] = continuity_rates[round_num]\n",
    "    test_results['transient_compromise'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_eq['round'] = round_num\n",
    "        acc_eq['phase'] = 'continuity'\n",
    "        acc_eq['participation_rate'] = continuity_rates[round_num]\n",
    "        test_results['transient_compromise_per_cluster_equal'][cluster_id].append(acc_eq)\n",
    "        \n",
    "        acc_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_dir['round'] = round_num\n",
    "        acc_dir['phase'] = 'continuity'\n",
    "        acc_dir['participation_rate'] = continuity_rates[round_num]\n",
    "        test_results['transient_compromise_per_cluster_dirichlet'][cluster_id].append(acc_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] Continuity ({int(continuity_rates[round_num]*100)}%) - \"\n",
    "          f\"Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nRounds 21-30: Re-stabilization Phase\")\n",
    "for round_num in range(21, 31):\n",
    "    model_path = f'trained_models/hierarchical_equal/model_round_{round_num}.pkl'\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved = pickle.load(f)\n",
    "    \n",
    "    # Global + per-cluster\n",
    "    accuracies = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='global')\n",
    "    accuracies['round'] = round_num\n",
    "    accuracies['phase'] = 'stabilization'\n",
    "    test_results['transient_compromise'].append(accuracies)\n",
    "    \n",
    "    for cluster_id in range(CFG['n_clusters']):\n",
    "        acc_eq = evaluate_model_on_test(saved['weights'], test_data_equal, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_eq['round'] = round_num\n",
    "        acc_eq['phase'] = 'stabilization'\n",
    "        test_results['transient_compromise_per_cluster_equal'][cluster_id].append(acc_eq)\n",
    "        \n",
    "        acc_dir = evaluate_model_on_test(saved['weights'], test_data_dirichlet, model_type='cluster', cluster_id=cluster_id)\n",
    "        acc_dir['round'] = round_num\n",
    "        acc_dir['phase'] = 'stabilization'\n",
    "        test_results['transient_compromise_per_cluster_dirichlet'][cluster_id].append(acc_dir)\n",
    "    \n",
    "    print(f\"[Round {round_num}] Stabilization - Traffic: {accuracies['traffic_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Transient Compromise complete: {len(test_results['transient_compromise'])} rounds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ ALL TESTING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll test results stored in 'test_results' dictionary:\")\n",
    "print(f\"  Single cluster: {len(test_results['single_cluster'])} rounds\")\n",
    "print(f\"  Hierarchical equal: {len(test_results['hierarchical_equal'])} rounds\")\n",
    "print(f\"  Hierarchical dirichlet: {len(test_results['hierarchical_dirichlet_global'])} rounds\")\n",
    "print(f\"  Per-cluster: {len(test_results['hierarchical_dirichlet_per_cluster'][0])} rounds each\")\n",
    "print(f\"  Compromise after convergence: {len(test_results['compromise_after_convergence'])} rounds\")\n",
    "print(f\"  Transient compromise: {len(test_results['transient_compromise'])} rounds\")\n",
    "print(f\"\\nReady for visualization!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. Communication Cost & Convergence Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMMUNICATION COST & CONVERGENCE ANALYSIS\")\n",
    "\n",
    "# - HierarchicalMTLFedAvgEnhanced instead of HierarchicalMTLFedAvg\n",
    "# - CompromisedHierarchicalStrategyEnhanced instead of CompromisedHierarchicalStrategy\n",
    "\n",
    "print(\"HOW TO ENABLE ENHANCED TRACKING\")\n",
    "print(\"\"\"\n",
    "To get communication cost and convergence tracking, modify your training calls:\n",
    "1. For normal training (like the baseline test):\n",
    " Replace: strategy = HierarchicalMTLFedAvg(...)\n",
    " With: strategy = HierarchicalMTLFedAvgEnhanced(...)\n",
    "\n",
    "2. For compromise tests:\n",
    " Replace: strategy = CompromisedHierarchicalStrategy(...)\n",
    " With: strategy = CompromisedHierarchicalStrategyEnhanced(...)\n",
    "\n",
    "3. After training completes, get the summary:\n",
    " comm_summary = strategy.get_comm_summary()\n",
    " print(f\"Model Size: {comm_summary['model_size_formatted']}\")\n",
    " print(f\"Total Communication: {comm_summary['total_cost_formatted']}\")\n",
    " print(f\"Convergence Round: {comm_summary['convergence_round']}\")\n",
    "\"\"\")\n",
    "\n",
    "print(\"EXPECTED METRICS (Based on Study)\")\n",
    "print(\"\"\"\n",
    "From the study slides:\n",
    "- Model Size: ~278.1 KB\n",
    "- Formula: W = 2T(N·ω)\n",
    " where T = rounds, N = clients per round (600 with 100% participation), ω = model size\n",
    "Example calculation for 100 rounds:\n",
    " W = 2 × 100 × 600 × 278.1 KB\n",
    " = 2 × 100 × 600 × 278,100 bytes\n",
    " = 33,372,000,000 bytes\n",
    " ≈ 33.37 GB\n",
    "\n",
    "Convergence (from slides):\n",
    "- Baseline: Converges around round 90\n",
    "- Variance: < 1% over 10-round window\n",
    "\"\"\")\n",
    "\n",
    "print(\"STUDY PARAMETERS TO UPDATE\")\n",
    "print(\"\"\"\n",
    "To match the study exactly, update the following in the test cells:\n",
    "Test 2 - CH Compromise After Convergence:\n",
    " Current: compromise_start_round=101, rounds=125, compromised_ch=1\n",
    " Update: compromise_start_round=111, rounds=120, compromised_ch=0\n",
    " \n",
    "Test 3 - Transient CH Compromise:\n",
    " Current: compromise_start_round=50, rounds=125, compromised_ch=0\n",
    " Update: compromise_start_round=11, rounds=30, compromised_ch=0\n",
    " \n",
    "Recovery Phases (from study slides):\n",
    " - Detection & Re-Election: 7 rounds (e.g., 111-117 or 11-17)\n",
    " - Continuity (Inter-Cluster Sync): 3 rounds (e.g., 118-120 or 18-20)\n",
    " - Stabilization: Gradual (30%, 70%, 100% participation)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. Enhanced Visualization with Convergence & Communication Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_convergence(\n",
    "    test_accuracies,\n",
    "    title,\n",
    "    convergence_round=None,\n",
    "    compromise_round=None,\n",
    "    compromise_end_round=None,\n",
    "    subplot_ax=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot accuracy with convergence and compromise markers.\n",
    "    \n",
    "    Args:\n",
    "        test_accuracies: List of dicts with task accuracies + round index\n",
    "        title: Plot title\n",
    "        convergence_round: Round where convergence was detected\n",
    "        compromise_round: Round where compromise started\n",
    "        compromise_end_round: Round where compromise ended\n",
    "        subplot_ax: Optional matplotlib axis for subplotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Safely extract fields\n",
    "    rounds = [entry.get(\"round\") for entry in test_accuracies]\n",
    "    traffic_acc = [entry.get(\"traffic_accuracy\", 0) for entry in test_accuracies]\n",
    "    duration_acc = [entry.get(\"duration_accuracy\", 0) for entry in test_accuracies]\n",
    "    bandwidth_acc = [entry.get(\"bandwidth_accuracy\", 0) for entry in test_accuracies]\n",
    "\n",
    "    # Create figure/axis if needed\n",
    "    if subplot_ax is None:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = plt.gca()\n",
    "    else:\n",
    "        ax = subplot_ax\n",
    "\n",
    "    # Plot accuracy curves\n",
    "    ax.plot(rounds, traffic_acc,   color='green',  label='Traffic',   linewidth=2, marker='o', markersize=3)\n",
    "    ax.plot(rounds, duration_acc,  color='blue',   label='Duration',  linewidth=2, marker='s', markersize=3)\n",
    "    ax.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth', linewidth=2, marker='^', markersize=3)\n",
    "\n",
    "    # Convergence marker\n",
    "    if convergence_round is not None and convergence_round in rounds:\n",
    "        idx = rounds.index(convergence_round)\n",
    "        avg_acc = (traffic_acc[idx] + duration_acc[idx] + bandwidth_acc[idx]) / 3\n",
    "\n",
    "        ax.axvline(\n",
    "            x=convergence_round,\n",
    "            color='green',\n",
    "            linestyle=':',\n",
    "            linewidth=2,\n",
    "            label=f'Convergence (Round {convergence_round})',\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            [convergence_round],\n",
    "            [avg_acc],\n",
    "            color='green',\n",
    "            s=200,\n",
    "            marker='*',\n",
    "            zorder=5,\n",
    "            edgecolors='black'\n",
    "        )\n",
    "\n",
    "    # Compromise indicator\n",
    "    if compromise_round is not None:\n",
    "        ax.axvline(\n",
    "            x=compromise_round,\n",
    "            color='red',\n",
    "            linestyle='--',\n",
    "            linewidth=2.5,\n",
    "            label=f'CH Compromised (Round {compromise_round})',\n",
    "            alpha=0.8\n",
    "        )\n",
    "\n",
    "        if compromise_end_round is not None:\n",
    "            ax.fill_between(\n",
    "                [compromise_round, compromise_end_round],\n",
    "                0, 1,\n",
    "                alpha=0.15,\n",
    "                color='red',\n",
    "                label='Compromise Period'\n",
    "            )\n",
    "\n",
    "    # Labels & grid\n",
    "    ax.set_xlabel(\"Rounds\", fontsize=12)\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    if subplot_ax is None:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENHANCED TRACKING IMPLEMENTATION\n",
    "\n",
    "### What Was Added:\n",
    "\n",
    "**1. Communication Cost Tracking** \n",
    "- Formula: `W = 2T(N·ω)` from the study\n",
    " - W = total communication cost (bytes)\n",
    " - T = number of rounds\n",
    " - N = participating clients per round\n",
    " - ω = model size in bytes\n",
    " - 2 = bidirectional (upload + download)\n",
    "- Real-time calculation per round\n",
    "- Cumulative cost tracking\n",
    "- Human-readable formatting (KB, MB, GB)\n",
    "\n",
    "**2. Convergence Detection** \n",
    "- Automatic detection when accuracy stabilizes\n",
    "- Window: 10 rounds\n",
    "- Threshold: < 1% variance\n",
    "- Reports convergence round\n",
    "- Tracks time to convergence\n",
    "\n",
    "**3. Enhanced Strategy Classes**\n",
    "- `HierarchicalMTLFedAvgEnhanced`: Base strategy with tracking\n",
    "- `CompromisedHierarchicalStrategyEnhanced`: Compromise strategy with tracking\n",
    "- Both inherit from original strategies and add:\n",
    " - Communication cost calculation\n",
    " - Convergence detection\n",
    " - Summary generation via `get_comm_summary()`\n",
    "\n",
    "**4. Visualization Enhancements**\n",
    "- `plot_with_convergence()`: Shows convergence markers\n",
    "- Green star at convergence point\n",
    "- Red shading for compromise periods\n",
    "- Vertical lines for key events\n",
    "\n",
    "**5. Study Alignment**\n",
    "- Updated compromise rounds to match study:\n",
    " - **Test 2**: Round 111 (after convergence)\n",
    " - **Test 3**: Round 11 (transient)\n",
    "- CH0 compromise (matches study diagrams)\n",
    "- Recovery phases documented:\n",
    " - Detection & Re-Election: 7 rounds\n",
    " - Continuity: 3 rounds\n",
    " - Stabilization: Gradual participation\n",
    "\n",
    "### How to Use:\n",
    "\n",
    "```python\n",
    "# Example with baseline training\n",
    "strategy = HierarchicalMTLFedAvgEnhanced(...) # Use enhanced version\n",
    "history = fl.simulation.start_simulation(...)\n",
    "\n",
    "# After training\n",
    "summary = strategy.get_comm_summary()\n",
    "print(f\"Model Size: {summary['model_size_formatted']}\")\n",
    "print(f\"Total Communication: {summary['total_cost_formatted']}\")\n",
    "print(f\"Convergence Round: {summary['convergence_round']}\")\n",
    "\n",
    "# Visualize with convergence markers\n",
    "plot_with_convergence(\n",
    " test_accuracies=strategy.test_accuracies,\n",
    " title=\"Baseline Training\",\n",
    " convergence_round=summary['convergence_round']\n",
    ")\n",
    "```\n",
    "\n",
    "### Expected Results (from study):\n",
    "- **Model Size**: ~278.1 KB\n",
    "- **100 rounds communication**: ~33.37 GB\n",
    "- **Convergence**: Around round 90\n",
    "- **Baseline performance**: \n",
    " - Traffic: ~70%\n",
    " - Duration: ~60%\n",
    " - Bandwidth: ~95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUICK REFERENCE: Implementation vs Study\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Feature | Study (Slides) | Our Implementation | Status |\n",
    "|---------|---------------|-------------------|---------|\n",
    "| **Architecture** | 3 clusters, CH1 global | 3 clusters, CH1 global | Match |\n",
    "| **Total Clients** | 600 (200 per cluster) | 600 (200 per cluster) | Match |\n",
    "| **Data Splits** | Dirichlet & Equal | Dirichlet & Equal | Match |\n",
    "| **Model Size** | 278.1 KB | Auto-calculated | Ready |\n",
    "| **Communication Formula** | W = 2T(N·ω) | W = 2T(N·ω) | Implemented |\n",
    "| **Convergence Detection** | Round ~90, variance < 1% | 10-round window, < 1% | Implemented |\n",
    "| **Baseline Rounds** | 100 | 100 | Match |\n",
    "| **Convergence Compromise** | Round 111, CH0 | Need to update to 111, CH0 | Update needed |\n",
    "| **Transient Compromise** | Round 11, CH0 | Need to update to 11, CH0 | Update needed |\n",
    "| **Recovery Phases** | D&R-E (7) + Continuity (3) | Documented | Implementation optional |\n",
    "\n",
    "## Test Parameters to Update\n",
    "\n",
    "### Test 1: Baseline \n",
    "- Already correct: 100 rounds, no compromise\n",
    "\n",
    "### Test 2: CH Compromise After Convergence \n",
    "**Current values:**\n",
    "```python\n",
    "rounds=125\n",
    "compromise_start_round=101\n",
    "compromised_ch=1\n",
    "```\n",
    "\n",
    "**Should be (to match study):**\n",
    "```python\n",
    "rounds=120 # 110 normal + 10 with compromise\n",
    "compromise_start_round=111 # Compromise at round 111\n",
    "compromised_ch=0 # CH0 compromised (not CH1)\n",
    "```\n",
    "\n",
    "### Test 3: Transient CH Compromise \n",
    "**Current values:**\n",
    "```python\n",
    "rounds=125\n",
    "compromise_start_round=50\n",
    "compromised_ch=0\n",
    "```\n",
    "\n",
    "**Should be (to match study):**\n",
    "```python\n",
    "rounds=30 # Shorter duration for transient test\n",
    "compromise_start_round=11 # Early compromise at round 11\n",
    "compromised_ch=0 # Keep CH0\n",
    "```\n",
    "\n",
    "## Communication Cost Calculation\n",
    "\n",
    "**Formula:** `W = 2 × T × N × ω`\n",
    "\n",
    "**Example for 100 rounds with 600 clients:**\n",
    "- Model size (ω): ~278.1 KB = 278,100 bytes\n",
    "- Rounds (T): 100\n",
    "- Clients per round (N): 600\n",
    "- Bidirectional factor: 2\n",
    "\n",
    "**Calculation:**\n",
    "```\n",
    "W = 2 × 100 × 600 × 278,100\n",
    " = 33,372,000,000 bytes\n",
    " = 33.37 GB\n",
    "```\n",
    "\n",
    "**Per round:**\n",
    "```\n",
    "W_round = 2 × 600 × 278,100\n",
    " = 333,720,000 bytes\n",
    " = 333.72 MB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. Visualization: All Tests Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Cluster Visualizations: Normal Testing (Equal & Dirichlet Splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Cluster Normal Testing: Equal and Dirichlet Splits (100 rounds)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with 2 rows x 3 columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Per-Cluster Performance: Normal Testing (100 Rounds)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Row 1: Equal Split\n",
    "for cluster_id in range(3):\n",
    "    ax = axes[0, cluster_id]\n",
    "    data = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][:100]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=11)\n",
    "    ax.set_ylabel('Training Accuracy', fontsize=11)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Equal Split', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Row 2: Dirichlet Split  \n",
    "for cluster_id in range(3):\n",
    "    ax = axes[1, cluster_id]\n",
    "    data = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][:100]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=11)\n",
    "    ax.set_ylabel('Training Accuracy', fontsize=11)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Dirichlet Split', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Per-cluster normal testing visualizations complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Cluster: CH Compromise After Convergence (Equal & Dirichlet Splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Cluster CH Compromise After Convergence (125 rounds)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with 2 rows x 3 columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "fig.suptitle('Per-Cluster: CH Compromise After Convergence (125 Rounds)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Row 1: Equal Split\n",
    "for cluster_id in range(3):\n",
    "    ax = axes[0, cluster_id]\n",
    "    data = test_results['compromise_after_convergence_per_cluster_equal'][cluster_id]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    # Phase markers\n",
    "    ax.axvline(x=90, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax.text(90, 0.95, 'round 90', rotation=90, va='top', fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax.axvspan(111, 118, alpha=0.15, color='pink', label='D&R-E\\n(111-117)')\n",
    "    ax.axvspan(118, 121, alpha=0.15, color='yellow', label='Continuity\\n(118-120)')\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=10)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Equal Split', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 125)\n",
    "\n",
    "# Row 2: Dirichlet Split\n",
    "for cluster_id in range(3):\n",
    "    ax = axes[1, cluster_id]\n",
    "    data = test_results['compromise_after_convergence_per_cluster_dirichlet'][cluster_id]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    # Phase markers\n",
    "    ax.axvline(x=90, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "    ax.text(90, 0.95, 'round 90', rotation=90, va='top', fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax.axvspan(111, 118, alpha=0.15, color='pink', label='D&R-E\\n(111-117)')\n",
    "    ax.axvspan(118, 121, alpha=0.15, color='yellow', label='Continuity\\n(118-120)')\n",
    "    \n",
    "    ax.set_xlabel('Rounds', fontsize=10)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Dirichlet Split', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 125)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Per-cluster CH compromise after convergence visualizations complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Cluster: Transient CH Compromise (Equal & Dirichlet Splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Cluster Transient CH Compromise (30 rounds)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with 2 rows x 3 columns\n",
    "fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "fig.suptitle('Per-Cluster: Transient CH Compromise (30 Rounds)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Row 1: Equal Split\n",
    "for cluster_id in range(3):\n",
    "    ax = axes[0, cluster_id]\n",
    "    data = test_results['transient_compromise_per_cluster_equal'][cluster_id]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    # Phase markers\n",
    "    ax.axvspan(10, 11, alpha=0.2, color='gray', label='Detection\\n(round 10)')\n",
    "    ax.axvspan(11, 18, alpha=0.15, color='pink', label='D&R-E\\n(11-17)')\n",
    "    ax.axvspan(18, 21, alpha=0.15, color='yellow', label='Continuity\\n(18-20)')\n",
    "    ax.axvspan(21, 30, alpha=0.10, color='lightgreen', label='Stabilization\\n(21-30)')\n",
    "    \n",
    "    ax.set_xlabel('Global Rounds', fontsize=10)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Equal Split', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 30)\n",
    "\n",
    "# Row 2: Dirichlet Split\n",
    "for cluster_id in range(3):\n",
    "    ax = axes[1, cluster_id]\n",
    "    data = test_results['transient_compromise_per_cluster_dirichlet'][cluster_id]\n",
    "    \n",
    "    traffic = [item['traffic_accuracy'] for item in data]\n",
    "    duration = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth = [item['bandwidth_accuracy'] for item in data]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    \n",
    "    ax.plot(rounds, bandwidth, 'orange', label='Bandwidth', linewidth=2)\n",
    "    ax.plot(rounds, duration, 'cyan', label='Duration', linewidth=2)\n",
    "    ax.plot(rounds, traffic, 'teal', label='Traffic', linewidth=2)\n",
    "    \n",
    "    # Phase markers  \n",
    "    ax.axvspan(10, 11, alpha=0.2, color='gray', label='Detection\\n(round 10)')\n",
    "    ax.axvspan(11, 18, alpha=0.15, color='pink', label='D&R-E\\n(11-17)')\n",
    "    ax.axvspan(18, 21, alpha=0.15, color='yellow', label='Continuity\\n(18-20)')\n",
    "    ax.axvspan(21, 30, alpha=0.10, color='lightgreen', label='Stabilization\\n(21-30)')\n",
    "    \n",
    "    ax.set_xlabel('Global Rounds', fontsize=10)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=10)\n",
    "    ax.set_title(f'Cluster {cluster_id} - Dirichlet Split', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Per-cluster transient compromise visualizations complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUMMARY STATISTICS - ALL TESTS\")\n",
    "\n",
    "print(\"\\n TEST 1: BASELINE (100 Rounds)\")\n",
    "print(f\"Final Accuracies (Round 100):\")\n",
    "print(f\" Traffic: {baseline_traffic[99]:.4f} ({baseline_traffic[99]*100:.2f}%)\")\n",
    "print(f\" Duration: {baseline_duration[99]:.4f} ({baseline_duration[99]*100:.2f}%)\")\n",
    "print(f\" Bandwidth: {baseline_bandwidth[99]:.4f} ({baseline_bandwidth[99]*100:.2f}%)\")\n",
    "avg_baseline = (baseline_traffic[99] + baseline_duration[99] + baseline_bandwidth[99]) / 3\n",
    "print(f\" Average: {avg_baseline:.4f} ({avg_baseline*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n TEST 2: CH COMPROMISE AFTER CONVERGENCE (CH0 at Round 111)\")\n",
    "print(f\"Before Compromise (Round 110):\")\n",
    "print(f\" Traffic: {compromise_after_traffic[109]:.4f}\")\n",
    "print(f\" Duration: {compromise_after_duration[109]:.4f}\")\n",
    "print(f\" Bandwidth: {compromise_after_bandwidth[109]:.4f}\")\n",
    "avg_before = (compromise_after_traffic[109] + compromise_after_duration[109] + compromise_after_bandwidth[109]) / 3\n",
    "print(f\" Average: {avg_before:.4f}\")\n",
    "\n",
    "print(f\"\\nAfter Compromise (Round 120):\")\n",
    "print(f\" Traffic: {compromise_after_traffic[-1]:.4f}\")\n",
    "print(f\" Duration: {compromise_after_duration[-1]:.4f}\")\n",
    "print(f\" Bandwidth: {compromise_after_bandwidth[-1]:.4f}\")\n",
    "avg_after = (compromise_after_traffic[-1] + compromise_after_duration[-1] + compromise_after_bandwidth[-1]) / 3\n",
    "print(f\" Average: {avg_after:.4f}\")\n",
    "\n",
    "print(f\"\\nImpact of Compromise:\")\n",
    "print(f\" Traffic: {(compromise_after_traffic[109] - compromise_after_traffic[-1])*100:.2f}% drop\")\n",
    "print(f\" Duration: {(compromise_after_duration[109] - compromise_after_duration[-1])*100:.2f}% drop\")\n",
    "print(f\" Bandwidth: {(compromise_after_bandwidth[109] - compromise_after_bandwidth[-1])*100:.2f}% drop\")\n",
    "print(f\" Average: {(avg_before - avg_after)*100:.2f}% drop\")\n",
    "\n",
    "print(\"\\n TEST 3: TRANSIENT CH COMPROMISE (CH0 from Round 11)\")\n",
    "print(f\"Before Compromise (Round 10):\")\n",
    "print(f\" Traffic: {transient_traffic[9]:.4f}\")\n",
    "print(f\" Duration: {transient_duration[9]:.4f}\")\n",
    "print(f\" Bandwidth: {transient_bandwidth[9]:.4f}\")\n",
    "avg_trans_before = (transient_traffic[9] + transient_duration[9] + transient_bandwidth[9]) / 3\n",
    "print(f\" Average: {avg_trans_before:.4f}\")\n",
    "\n",
    "print(f\"\\nAfter Compromise (Round 30):\")\n",
    "print(f\" Traffic: {transient_traffic[-1]:.4f}\")\n",
    "print(f\" Duration: {transient_duration[-1]:.4f}\")\n",
    "print(f\" Bandwidth: {transient_bandwidth[-1]:.4f}\")\n",
    "avg_trans_after = (transient_traffic[-1] + transient_duration[-1] + transient_bandwidth[-1]) / 3\n",
    "print(f\" Average: {avg_trans_after:.4f}\")\n",
    "\n",
    "print(f\"\\nImpact of Compromise:\")\n",
    "print(f\" Traffic: {(transient_traffic[9] - transient_traffic[-1])*100:.2f}% difference\")\n",
    "print(f\" Duration: {(transient_duration[9] - transient_duration[-1])*100:.2f}% difference\")\n",
    "print(f\" Bandwidth: {(transient_bandwidth[9] - transient_bandwidth[-1])*100:.2f}% difference\")\n",
    "print(f\" Average: {(avg_trans_before - avg_trans_after)*100:.2f}% difference\")\n",
    "\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"1. Baseline converges around round 100 with stable accuracy\")\n",
    "print(\"2. CH compromise after convergence shows immediate performance degradation\")\n",
    "print(\"3. Transient compromise during training affects learning trajectory\")\n",
    "print(\"4. Different CHs (CH0 vs CH1) may have different impact levels\")\n",
    "print(\"5. The hierarchical architecture shows resilience/vulnerability patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All CH Compromisation Tests Complete!\n",
    "\n",
    "### Test Summary (Updated to Match Study):\n",
    "- **Test 1**: Baseline (100 rounds) - Normal convergence without compromise\n",
    "- **Test 2**: CH Compromise After Convergence (120 rounds) - **CH0 compromised at round 111** (matches study slides 7-9)\n",
    "- **Test 3**: Transient CH Compromise (30 rounds) - **CH0 compromised at round 11** (matches study slide 12)\n",
    "\n",
    "### Visualization Layout:\n",
    "**Row 1 (First 100 Rounds):**\n",
    "- Graph 1: Traffic Classification (Baseline vs Pre-Compromise)\n",
    "- Graph 2: Duration Classification (Baseline vs Pre-Compromise)\n",
    "\n",
    "**Row 2 (Full 120 Rounds - After Convergence):**\n",
    "- Graph 3: Traffic with **CH0 compromise at round 111**\n",
    "- Graph 4: Duration & Bandwidth with **CH0 compromise at round 111**\n",
    "\n",
    "**Row 3 (Full 30 Rounds - Transient):**\n",
    "- Graph 5: Traffic with **CH0 compromise at round 11**\n",
    "- Graph 6: Duration & Bandwidth with **CH0 compromise at round 11**\n",
    "\n",
    "### Architecture Used:\n",
    "- **3 Clusters**: 600 clients total (200 per cluster)\n",
    "- **Cluster Heads**: CH0, CH1, CH2\n",
    "- **Global Aggregator**: CH1\n",
    "- **Two-Tier Hierarchy**:\n",
    " - Tier 1: Members → CH (local aggregation)\n",
    " - Tier 2: CH0, CH2 → CH1 (global) → CH0, CH2 → Members\n",
    "\n",
    "### Compromise Method:\n",
    "- **Type**: Label flipping (parameter sign inversion)\n",
    "- **Impact**: Malicious CH sends poisoned model parameters\n",
    "- **Detection**: Observable through accuracy degradation\n",
    "\n",
    "### Key Findings (Based on Study):\n",
    "1. Baseline achieves stable convergence around round 90 (detected automatically)\n",
    "2. Post-convergence compromise (round 111) shows immediate performance degradation\n",
    "3. Transient compromise (round 11) affects learning trajectory from early stages\n",
    "4. **CH0 compromise** studied (local cluster head impact)\n",
    "5. System shows vulnerability to CH-level attacks, requiring recovery mechanisms\n",
    "6. Study shows recovery phases: Detection & Re-Election (7 rounds) + Continuity (3 rounds)\n",
    "\n",
    "### Next Experiments:\n",
    "- Test different compromise types (random_noise, model_poison)\n",
    "- Compromise different CHs (CH2, multiple CHs simultaneously)\n",
    "- Implement defense mechanisms (anomaly detection, secure aggregation)\n",
    "- Compare impact under equal vs Dirichlet data splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check if test results are available\n",
    "if 'test_results' not in locals():\n",
    "    print(\"⚠️ Test results not available. Please run the testing phase first.\")\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PLOTTING PER-TASK TRAINING ACCURACY GRAPHS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define experiments to plot\n",
    "    experiments = {\n",
    "        'single_cluster': 'Single Cluster',\n",
    "        'hierarchical_equal': 'Hierarchical (Equal Split)'\n",
    "    }\n",
    "    \n",
    "    # Create figure with 3 subplots (one per task)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    tasks = [\n",
    "        ('traffic_accuracy', 'Traffic Classification', 'green'),\n",
    "        ('duration_accuracy', 'Flow Duration Classification', 'blue'),\n",
    "        ('bandwidth_accuracy', 'Bandwidth Classification', 'orange')\n",
    "    ]\n",
    "    \n",
    "    for idx, (task_metric, task_title, color) in enumerate(tasks):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot each experiment\n",
    "        for exp_key, exp_label in experiments.items():\n",
    "            if exp_key in test_results and test_results[exp_key]:\n",
    "                data = test_results[exp_key]\n",
    "                rounds = [item['round'] for item in data]\n",
    "                accuracies = [item[task_metric] for item in data]\n",
    "                \n",
    "                linestyle = '-' if exp_key == 'single_cluster' else '--'\n",
    "                marker = 'o' if exp_key == 'single_cluster' else 's'\n",
    "                \n",
    "                ax.plot(rounds, accuracies, color=color, label=exp_label,\n",
    "                       linewidth=2, marker=marker, markersize=4, \n",
    "                       linestyle=linestyle, markevery=10)\n",
    "        \n",
    "        # Format subplot\n",
    "        ax.set_xlabel('Rounds', fontsize=11)\n",
    "        ax.set_ylabel('Accuracy', fontsize=11)\n",
    "        ax.set_title(task_title, fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='lower right', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Set y-axis limits with padding\n",
    "        if exp_key in test_results and test_results[exp_key]:\n",
    "            all_acc = [item[task_metric] for item in test_results['single_cluster']]\n",
    "            if 'hierarchical_equal' in test_results:\n",
    "                all_acc += [item[task_metric] for item in test_results['hierarchical_equal']]\n",
    "            y_min = max(0.0, min(all_acc) - 0.05)\n",
    "            y_max = min(1.0, max(all_acc) + 0.05)\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    plt.suptitle('Federated Multi-Task Learning - Per-Task Training Accuracy', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics for each task\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PER-TASK SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for task_metric, task_title, _ in tasks:\n",
    "        print(f\"\\n{task_title}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for exp_key, exp_label in experiments.items():\n",
    "            if exp_key in test_results and test_results[exp_key]:\n",
    "                data = test_results[exp_key]\n",
    "                accuracies = [item[task_metric] for item in data]\n",
    "                rounds = [item['round'] for item in data]\n",
    "                \n",
    "                print(f\"\\n  {exp_label}:\")\n",
    "                print(f\"    First round:  {accuracies[0]:.4f} ({accuracies[0]*100:.2f}%)\")\n",
    "                print(f\"    Last round:   {accuracies[-1]:.4f} ({accuracies[-1]*100:.2f}%)\")\n",
    "                print(f\"    Best:         {max(accuracies):.4f} ({max(accuracies)*100:.2f}%) at Round {rounds[np.argmax(accuracies)]}\")\n",
    "                print(f\"    Improvement:  +{accuracies[-1] - accuracies[0]:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract data from single cluster training history\n",
    "if 'history_single' in locals():\n",
    "    # Debug: Print available attributes\n",
    "    print(\"Available history_single attributes:\")\n",
    "    print([attr for attr in dir(history_single) if not attr.startswith('_')])\n",
    "    print()\n",
    "    \n",
    "    # Check both metrics_distributed_fit (training) and metrics_distributed (evaluation)\n",
    "    metrics_fit = getattr(history_single, 'metrics_distributed_fit', {})\n",
    "    metrics_eval = getattr(history_single, 'metrics_distributed', {})\n",
    "    \n",
    "    print(\"Available metrics in metrics_distributed_fit:\", list(metrics_fit.keys()) if metrics_fit else \"None\")\n",
    "    print(\"Available metrics in metrics_distributed:\", list(metrics_eval.keys()) if metrics_eval else \"None\")\n",
    "    print()\n",
    "    \n",
    "    # Try to get rounds from either source\n",
    "    rounds = []\n",
    "    traffic_acc = []\n",
    "    duration_acc = []\n",
    "    bandwidth_acc = []\n",
    "    \n",
    "    # First, try to get per-task accuracies from evaluation metrics\n",
    "    if metrics_eval:\n",
    "        if 'traffic_accuracy' in metrics_eval:\n",
    "            rounds = [r for r, _ in metrics_eval['traffic_accuracy']]\n",
    "            traffic_acc = [float(v) for _, v in metrics_eval['traffic_accuracy']]\n",
    "            duration_acc = [float(v) for _, v in metrics_eval['duration_accuracy']]\n",
    "            bandwidth_acc = [float(v) for _, v in metrics_eval['bandwidth_accuracy']]\n",
    "        elif 'accuracy' in metrics_eval:\n",
    "            rounds = [r for r, _ in metrics_eval['accuracy']]\n",
    "    \n",
    "    # If not found, try training metrics\n",
    "    if not rounds and metrics_fit:\n",
    "        if 'traffic_accuracy' in metrics_fit:\n",
    "            rounds = [r for r, _ in metrics_fit['traffic_accuracy']]\n",
    "            traffic_acc = [float(v) for _, v in metrics_fit['traffic_accuracy']]\n",
    "            duration_acc = [float(v) for _, v in metrics_fit['duration_accuracy']]\n",
    "            bandwidth_acc = [float(v) for _, v in metrics_fit['bandwidth_accuracy']]\n",
    "        elif 'accuracy' in metrics_fit:\n",
    "            rounds = [r for r, _ in metrics_fit['accuracy']]\n",
    "    \n",
    "    # Fallback: Use test_results if available\n",
    "    if not rounds and 'test_results' in locals() and 'single_cluster' in test_results:\n",
    "        print(\"Using test_results for plotting...\")\n",
    "        data = test_results['single_cluster']\n",
    "        rounds = [item['round'] for item in data]\n",
    "        traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "        duration_acc = [item['duration_accuracy'] for item in data]\n",
    "        bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    if not rounds:\n",
    "        print(\"⚠️ No training metrics found in history_single or test_results\")\n",
    "        print(\"Please ensure training has been completed.\")\n",
    "    else:\n",
    "        print(f\"✅ Single cluster training: {len(rounds)} rounds\")\n",
    "        \n",
    "        # Create styled plot matching your reference\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot per-task accuracies if available\n",
    "        if traffic_acc and duration_acc and bandwidth_acc:\n",
    "            # Calculate y-axis bounds\n",
    "            all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "            y_min = max(0.0, min(all_acc) - 0.05)\n",
    "            y_max = min(1.0, max(all_acc) + 0.05)\n",
    "            \n",
    "            plt.plot(rounds, traffic_acc, color='green', label='Traffic Classification', \n",
    "                    linewidth=2, marker='o', markersize=4)\n",
    "            plt.plot(rounds, duration_acc, color='blue', label='Flow Duration Classification', \n",
    "                    linewidth=2, marker='s', markersize=4)\n",
    "            plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "                    linewidth=2, marker='^', markersize=4)\n",
    "            \n",
    "            plt.ylim(y_min, y_max)\n",
    "        else:\n",
    "            # Fallback: plot overall accuracy if available\n",
    "            if 'accuracy' in metrics_eval or 'accuracy' in metrics_fit:\n",
    "                acc_metrics = metrics_eval.get('accuracy', metrics_fit.get('accuracy', []))\n",
    "                if acc_metrics:\n",
    "                    acc_values = [float(v) for _, v in acc_metrics]\n",
    "                    plt.plot(rounds, acc_values, color='blue', label='Overall Accuracy', \n",
    "                            linewidth=2, marker='o', markersize=4)\n",
    "                    plt.ylim(0, 1.05)\n",
    "            else:\n",
    "                print(\"⚠️ No accuracy metrics available to plot\")\n",
    "        \n",
    "        plt.xlabel('Rounds', fontsize=12)\n",
    "        plt.ylabel('Accuracy', fontsize=12)\n",
    "        plt.title('Single Cluster Training - Federated Multi-Task Learning', fontsize=14, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=11)\n",
    "        plt.grid(True, alpha=0.3, linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"SINGLE CLUSTER TRAINING SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total Rounds: {len(rounds)}\")\n",
    "        if traffic_acc and duration_acc and bandwidth_acc:\n",
    "            print(f\"\\nFinal Accuracies:\")\n",
    "            print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "            print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "            print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "        print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Training history not available yet - run training cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if 'test_results' in locals() and 'single_cluster' in test_results:\n",
    "    data = test_results['single_cluster']\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    # Calculate y-axis bounds\n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='green', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(rounds, duration_acc, color='blue', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=4)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=4)\n",
    "    \n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Single Cluster Testing - Baseline Performance', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SINGLE CLUSTER TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nFinal Accuracies (Round {rounds[-1]}):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"\\nBest Accuracies:\")\n",
    "    print(f\"  Traffic:   {max(traffic_acc):.4f} at Round {rounds[np.argmax(traffic_acc)]}\")\n",
    "    print(f\"  Duration:  {max(duration_acc):.4f} at Round {rounds[np.argmax(duration_acc)]}\")\n",
    "    print(f\"  Bandwidth: {max(bandwidth_acc):.4f} at Round {rounds[np.argmax(bandwidth_acc)]}\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run testing cells first to generate test_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 0\n",
    "if 'test_results' in locals() and 'hierarchical_dirichlet_per_cluster' in test_results:\n",
    "    data = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][:100]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=4)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=4)\n",
    "    \n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Training Accuracy', fontsize=12)\n",
    "    plt.title(f'Cluster {cluster_id} - Equal Split (100 Rounds)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CLUSTER {cluster_id} - EQUAL SPLIT TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nFinal Accuracies (Round {rounds[-1]}):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run per-cluster testing cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 2\n",
    "if 'test_results' in locals() and 'hierarchical_dirichlet_per_cluster' in test_results:\n",
    "    data = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][:100]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=4)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=4)\n",
    "    \n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Training Accuracy', fontsize=12)\n",
    "    plt.title(f'Cluster {cluster_id} - Equal Split (100 Rounds)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CLUSTER {cluster_id} - EQUAL SPLIT TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nFinal Accuracies (Round {rounds[-1]}):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run per-cluster testing cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 1\n",
    "if 'test_results' in locals() and 'hierarchical_dirichlet_per_cluster' in test_results:\n",
    "    data = test_results['hierarchical_dirichlet_per_cluster'][cluster_id][:100]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=4)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=4)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=4)\n",
    "    \n",
    "    plt.xlabel('Rounds', fontsize=12)\n",
    "    plt.ylabel('Training Accuracy', fontsize=12)\n",
    "    plt.title(f'Cluster {cluster_id} - Equal Split (100 Rounds)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=11)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CLUSTER {cluster_id} - EQUAL SPLIT TEST RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nFinal Accuracies (Round {rounds[-1]}):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run per-cluster testing cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 0\n",
    "if 'test_results' in locals() and 'compromise_after_convergence_per_cluster_equal' in test_results:\n",
    "    data = test_results['compromise_after_convergence_per_cluster_equal'][cluster_id]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=3)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=3)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=3)\n",
    "    \n",
    "    # Phase markers\n",
    "    plt.axvline(x=90, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Round 90 (Convergence)')\n",
    "    plt.axvspan(111, 118, alpha=0.15, color='pink', label='D&R-E Phase (111-117)')\n",
    "    plt.axvspan(118, 121, alpha=0.15, color='yellow', label='Continuity (118-120)')\n",
    "    plt.axvspan(121, 125, alpha=0.10, color='lightgreen', label='Stabilization (121-125)')\n",
    "    \n",
    "    plt.xlabel('Global Rounds', fontsize=12)\n",
    "    plt.ylabel('Test Accuracy', fontsize=12)\n",
    "    plt.title(f'CH Compromise After Convergence - Cluster {cluster_id} (Equal Split)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10, ncol=2)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlim(0, 125)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CH COMPROMISE - CLUSTER {cluster_id} (EQUAL SPLIT)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nBefore Compromise (Round 110):\")\n",
    "    if len(traffic_acc) > 109:\n",
    "        print(f\"  Traffic:   {traffic_acc[109]:.4f} ({traffic_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Duration:  {duration_acc[109]:.4f} ({duration_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Bandwidth: {bandwidth_acc[109]:.4f} ({bandwidth_acc[109]*100:.2f}%)\")\n",
    "    print(f\"\\nAfter Recovery (Round 125):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run CH compromise testing cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 1\n",
    "if 'test_results' in locals() and 'compromise_after_convergence_per_cluster_equal' in test_results:\n",
    "    data = test_results['compromise_after_convergence_per_cluster_equal'][cluster_id]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=3)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=3)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=3)\n",
    "    \n",
    "    # Phase markers\n",
    "    plt.axvline(x=90, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Round 90 (Convergence)')\n",
    "    plt.axvspan(111, 118, alpha=0.15, color='pink', label='D&R-E Phase (111-117)')\n",
    "    plt.axvspan(118, 121, alpha=0.15, color='yellow', label='Continuity (118-120)')\n",
    "    plt.axvspan(121, 125, alpha=0.10, color='lightgreen', label='Stabilization (121-125)')\n",
    "    \n",
    "    plt.xlabel('Global Rounds', fontsize=12)\n",
    "    plt.ylabel('Test Accuracy', fontsize=12)\n",
    "    plt.title(f'CH Compromise After Convergence - Cluster {cluster_id} (Equal Split)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10, ncol=2)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlim(0, 125)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CH COMPROMISE - CLUSTER {cluster_id} (EQUAL SPLIT)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nBefore Compromise (Round 110):\")\n",
    "    if len(traffic_acc) > 109:\n",
    "        print(f\"  Traffic:   {traffic_acc[109]:.4f} ({traffic_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Duration:  {duration_acc[109]:.4f} ({duration_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Bandwidth: {bandwidth_acc[109]:.4f} ({bandwidth_acc[109]*100:.2f}%)\")\n",
    "    print(f\"\\nAfter Recovery (Round 125):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run CH compromise testing cell first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cluster_id = 2\n",
    "if 'test_results' in locals() and 'compromise_after_convergence_per_cluster_equal' in test_results:\n",
    "    data = test_results['compromise_after_convergence_per_cluster_equal'][cluster_id]\n",
    "    rounds = [item['round'] for item in data]\n",
    "    traffic_acc = [item['traffic_accuracy'] for item in data]\n",
    "    duration_acc = [item['duration_accuracy'] for item in data]\n",
    "    bandwidth_acc = [item['bandwidth_accuracy'] for item in data]\n",
    "    \n",
    "    all_acc = traffic_acc + duration_acc + bandwidth_acc\n",
    "    y_min = max(0.0, min(all_acc) - 0.05)\n",
    "    y_max = min(1.0, max(all_acc) + 0.05)\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.plot(rounds, traffic_acc, color='teal', label='Traffic Classification', \n",
    "            linewidth=2, marker='o', markersize=3)\n",
    "    plt.plot(rounds, duration_acc, color='cyan', label='Flow Duration Classification', \n",
    "            linewidth=2, marker='s', markersize=3)\n",
    "    plt.plot(rounds, bandwidth_acc, color='orange', label='Bandwidth Classification', \n",
    "            linewidth=2, marker='^', markersize=3)\n",
    "    \n",
    "    # Phase markers\n",
    "    plt.axvline(x=90, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Round 90 (Convergence)')\n",
    "    plt.axvspan(111, 118, alpha=0.15, color='pink', label='D&R-E Phase (111-117)')\n",
    "    plt.axvspan(118, 121, alpha=0.15, color='yellow', label='Continuity (118-120)')\n",
    "    plt.axvspan(121, 125, alpha=0.10, color='lightgreen', label='Stabilization (121-125)')\n",
    "    \n",
    "    plt.xlabel('Global Rounds', fontsize=12)\n",
    "    plt.ylabel('Test Accuracy', fontsize=12)\n",
    "    plt.title(f'CH Compromise After Convergence - Cluster {cluster_id} (Equal Split)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=10, ncol=2)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlim(0, 125)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CH COMPROMISE - CLUSTER {cluster_id} (EQUAL SPLIT)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Rounds: {len(rounds)}\")\n",
    "    print(f\"\\nBefore Compromise (Round 110):\")\n",
    "    if len(traffic_acc) > 109:\n",
    "        print(f\"  Traffic:   {traffic_acc[109]:.4f} ({traffic_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Duration:  {duration_acc[109]:.4f} ({duration_acc[109]*100:.2f}%)\")\n",
    "        print(f\"  Bandwidth: {bandwidth_acc[109]:.4f} ({bandwidth_acc[109]*100:.2f}%)\")\n",
    "    print(f\"\\nAfter Recovery (Round 125):\")\n",
    "    print(f\"  Traffic:   {traffic_acc[-1]:.4f} ({traffic_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Duration:  {duration_acc[-1]:.4f} ({duration_acc[-1]*100:.2f}%)\")\n",
    "    print(f\"  Bandwidth: {bandwidth_acc[-1]:.4f} ({bandwidth_acc[-1]*100:.2f}%)\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"⚠️ Run CH compromise testing cell first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cesnet",
   "language": "python",
   "name": "cesnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
